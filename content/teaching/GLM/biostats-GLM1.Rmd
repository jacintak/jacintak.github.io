---
title: 'Statistical modelling 1: Simple linear models'
editor_options:
  chunk_output_type: console
date: "`r format(Sys.Date(), '%d-%b-%Y')`"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = F, warning = FALSE, message = FALSE, comment = NA, fig.height = 3, fig.width = 3, dpi = 100, fig.align = "center", fig.show = "hold")
library(tidyverse)
theme_set(theme_classic())
library(MASS)
```
***

# Overall lecture aims

* Identify the structure of general linear models
* Describe how linear models are parametrized
* Understand how to use linear models
* Understand how to interpret and evaluate linear models

Recommended reading:
Chapter 5.5 Getting Started with R


## Other resources
 
* Seeing Theory. Chapter 6. https://seeing-theory.brown.edu/regression-analysis/index.html
* Statistics: an introduction using R. Michael J. Crawley. Wiley Press. Chapters 7 - 11.
* Experimental Design and Data Analysis for Biologists. Gerry P. Quinn & Michael J. Keough. Cambridge Press. Chapters 5, 6, 8, 12.
* R for Data Science (For more advanced R). https://r4ds.had.co.nz/index.html
* https://learningstatisticswithr.com/book/index.html
* [An interactive visual explanation of hierarchical Models](http://mfviz.com/hierarchical-models/)

## Using datasets

Datasets used here are from the package `MASS` which comes with R. To use any data in `MASS`, `library(MASS)` needs to be called first.

## By the end of this lecture you should:
 
* Describe the structure of a general linear model
* Understand how a simple linear model is parametrised
* Understand what the parameters of a linear model represent
* Parametrise a simple linear model from given information
* Interpret a simple linear model in a graphical format

In R:

* Use built-in R datasets and R libraries
* Conduct a simple linear model

***

# Height and girth of trees

Imagine we measured the height and girth of trees to test the hypothesis that thicker trees are taller.  

```{r linear-model}
tree_plot <- ggplot(trees, aes(Girth, Height)) + geom_point()
tree_plot
```

How should test our hypothesis? We could assign trees to categories like "thick" or "thin" trees and do a t-test. What does our t-test say?

```{r t-test, fig.show='asis'}
trees %>% 
  mutate(Thickness = ifelse(Girth <= median(Girth), "Thin", "Thick")) %>% 
  ggplot(aes(Thickness, Height)) + 
  stat_summary(fun.y = mean, geom = "bar", fill = 'grey') +
  stat_summary(fun.data = "mean_se", geom = "errorbar", lwd = 1, width = 0.3)
t.test(Height ~ Thickness, trees %>% 
  mutate(Thickness = ifelse(Girth <= median(Girth), "Thin", "Thick")))
```

## **But** this isn't the best way
* How did we define "thick" or "thin"?
* We lost information using groups - tree girth is not categorical data, it's continuous
* This increases unexplained variance = *bad!*
* When might you want to group data?

***

# Linear regression

> models the linear relationship between 2 continuous variables

* Continuous response variable (Y)
* Continuous predictor variable (X)

```{r regression-model, fig.cap= "Fit a line", fig.width = 4}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2, aes(colour = "Linear model")) +
  scale_colour_manual(name=NULL, values=c("#3366FF"))
```

## What does linear mean?

* By definition, all predictor parameters should not be divided by each other
* No parameter is an exponent
* No parameter is multiplied or divided by another

Not a linear model:  
$H_a = \frac{a \times H \times T}{1 + a \times H \times T}$ e.g. Holling’s Type II predation equation  
$y = \frac{\beta_1 x}{\beta_2 x}$ e.g. enzyme rate of reactions

## Why do we use a linear model?

* Quantify a relationship
  - E.g. relationship between drug concentration and time
* Predict what will happen
  - E.g. growth at different time points
* Explain as much about the response variable as possible
* Partition variation
  - E.g. Phenotypic variation = genetic variation + environmental variation

***

# Structure of a general linear model

> $$Y = intercept + slope \times X$$

```{r general-linear-model}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2) +
  xlab("Predictor variable, X") +
  ylab("Response variable, Y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank()) 
```

Intercepts and slopes can be mathematically represented by any symbol or letter but conventionally the equation is written as:

$$y_i = \beta_0 + \beta_1 \times x_i + \varepsilon_i$$

Where $i$ refers to individual data points.

## Variables of a linear model

Variables are $y_i$ and $x_i$  
$y_i$ = response variable, what you are interested in, e.g. height  
$x_i$ = predictor variable, what you are manipulating, e.g. girth


## Parameters of a linear model

Parameters are what we don't know and we need to work out (*parametrise*). They are $\beta_0$ and $\beta_1$.   
$\beta_0$ = population intercept, value of y when x is 0, constant  
$\beta_1$ = population slope, unit change in y with a unit change in x  
$\varepsilon_i$ = the residual of the model (how much the line gets wrong for each data point or what is left over), also called **random error**  

***

# Ordinary least squares regression

Ordinary least squares regression (OLS) is the technique used to parametrise the linear model by finding the 'best fitting' line.

> The aim is to make the sum of the squared residuals, $\sum{\varepsilon_i^2}$, as small as possible 

Meaning calculating the difference between each point and the line, squaring that difference and adding it all up (Figure \@ref(fig:residuals)).  
Residuals are squared to make all values positive - remember points can fall above and below the best fit line.

```{r residuals, fig.cap = "Residuals (blue lines) are the difference between the data point and the predicted line (black line)",fig.width= 5, fig.height=5,}
plot(Height ~ Girth, trees)
tree_lm <- lm(Height ~ Girth, trees)
abline(tree_lm, lwd=3)
for(i in seq_along(trees$Height)) {
	  yval <- coef(tree_lm)[1]+coef(tree_lm)[2]*trees$Height[i]
	  segments(x0 = trees$Girth[i], y0 = trees$Height[i], 
	           x1 = trees$Girth[i], y1= coef(tree_lm)[1]+coef(tree_lm)[2]*trees$Girth[i], col = "blue")
	}
```


## Finding the intercept

* Why not extrapolate to 0?
* We know $\bar{y}$ and $\bar{x}$ - the means of x and y
* We can rearrange the linear equation to $\beta_0 = \bar{y} - \beta_1 \bar{x}$ where $\beta_1$ is the estimated slope
* But `R` will do all this for you!

***

# Halfway there! Revision time!
Test your understanding so far by answering the questions below. Click `Code` to see the answer.

What types of variables are used in simple linear regression?
```{r, class.source = 'fold-hide', eval = FALSE, echo = TRUE}
Both the response and predictor variable should be continuous. (Categorical predictor variables are also allowed as we will see in later lectures)
```

What is the method for parameterising linear models called?
```{r, class.source = 'fold-hide', eval = FALSE, echo = TRUE}
Ordinary least squares regression. The aim is to minimise the sum of squared residuals.
```

What is random error in linear regression?
```{r, class.source = 'fold-hide', eval = FALSE, echo = TRUE}
The variation in data that cannot be explained by the linear model. i.e. the difference between the predicted value of the model and the observations: the residuals. Denoted ε.
```

***

# What does a linear model tell us?

> 1. What will Y be, given a new value of X?
> 2. Does the population slope $\beta_1$ differ to 0?

## Back to trees

The function `lm()` in `R` stands for linear model. It will do the OLS calculations for us. The function takes data in the form `lm(Y ~ X, data)`. Use `summary()` on your `lm` to get more information.

```{r trees-lm}
tree_lm
```

Here the `(Intercept)` is the value of the intercept, $\beta_0$ and `Girth` is the slope of the model, $\beta_1$.  
We then put these numbers in our linear model equation to get the **parametrised linear model**

$Y = \beta_0 + \beta_1 \times X$  
becomes $Height =$ `r round(coef(tree_lm)[1],3)`  + `r round(coef(tree_lm)[2],3)` $\times Girth$

```{r tree-plot-2, fig.cap = "Does the above equation match this line?"}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2, fullrange = TRUE) +
  scale_x_continuous(limits = c(0,21), expand = c(0, 0)) +
  scale_y_continuous(limits = c(60,90), expand = c(0, 0)) 
```

## Calculating a new value of Y

We can use the parametrised equation to work out the height of a tree for any value of girth. If a tree is 10cm thick, what is its predicted height?

$Height =$ `r round(coef(tree_lm)[1],3)`  + `r round(coef(tree_lm)[2],3)` $\times Girth$
$Height =$ `r round(coef(tree_lm)[1],3)`  + `r round(coef(tree_lm)[2],3)` $\times 10$  
$Height =$ `r round(coef(tree_lm)[1],3) + 10 * round(coef(tree_lm)[2],3)` cm

## Does the population slope $\beta_1$ differ to 0?

Typically we are more interested in the value of the slope than the intercept. We want to know if the slope of the fitted line is statistically different to 0 because that represents our hypotheses:

> Null hypothesis (in orange, Figure \@ref(fig:tree-slopes)):
> $$H_0: \beta_1 = 0$$
> Alternative hypothesis (Figure \@ref(fig:regression-slopes)):
> $$H_1: \beta_1 \neq 0$$

```{r tree-slopes, fig.cap = "Hypotheses of linear models"}
tree_plot +
#  geom_smooth(method = "lm", se = F, lwd = 2) +
  geom_hline(yintercept = mean(trees$Height), colour = "orange", lwd = 2)
```

```{r regression-slopes, fig.cap = "Regression lines can have positive (blue) or negative (red) slopes, either are H1"}
tree_plot +
  geom_smooth(method = "lm", se = F, lwd = 2) +
  geom_abline(slope = -1.054, intercept = 90, lwd = 2, colour = "red")
```

### Testing linear model parameters in R

R will conduct a statistical test on the model parameters for us. You can see it using `summary(lm())`

```{r model-output}
summary(tree_lm)
```

* The P values above test whether our slope is significantly different to 0 
* Tested like a single parameter t-test (why?)
  - What are the degrees of freedom?
  
**Based on the R output above, does our estimated slope for Girth differ to 0 and what can we conclude about our hypothesis?**


# Probability distribution functions (PDF)

> Linear models need a descriptor of how response variables (or errors) should be distributed

**Probability distributions**

* E.g. Gaussian/normal distribution
* Grouped by "family"
* Other types exist but not covered here – same principles apply, called generalised linear models (GLM)


```{r normal, fig.cap= "a normal distribution with mean = 0 and sd  = 1"}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab("") +
  scale_y_continuous(breaks = NULL)
```


## Linear models in the Gaussian family

> Same principles, different names

* Linear regression
  * continuous response
  * one or more continuous predictors
* Analysis of Variance (ANOVA)
  * continuous response
  * categorical predictors
* Analysis of Covariance (ANCOVA)
  * continuous response
  * A mix of continuous and categorical predictors
  * Special case with special assumptions

### You have seen these statistical tests in previous lectures

```{r tests}
t.test(Height ~ Sex, survey)
kable(anova(lm(Height ~ Sex, survey)), digits = 32)
```

A one-way ANOVA is mathematically identical to a t-test


> In this module, don’t focus on memorising what kind of test fits what kinds of data

They are all variants on general linear models anyway...

* We are interested in **slopes** and (to a lesser extent) intercepts
* Understand how these models relate to your *hypotheses & experimental design*

***

# Revision time!
Test your understanding so far by answering the questions below. Click `Code` to see the answer.

What is the function in R to conduct a linear regression?
```{r, class.source = 'fold-hide', eval = FALSE, echo = TRUE}
The function is lm().
```

How do you find a predicted value of a response for a given predictor value using a parameterised linear model?
```{r, class.source = 'fold-hide', eval = FALSE, echo = TRUE}
Substitute the value of the predictor in the parameterised linear model and solve it.

if Y = 10 * X + 6 and X = 2:
   Y = 10 * 2 + 6 = 20 + 6
   Y = 26 
```

What is the probability distribution function of linear regression?
```{r, class.source = 'fold-hide', eval = FALSE, echo = TRUE}
The normal distribution. Also called Gaussian.
```
