[{"authors":["admin"],"categories":null,"content":"I want to know where animals live and how they persist in their environment. My interests lie at the intersection between ecology, evolutionary biology and experimental biology.\nMy research centers on characterising diversity in traits and understanding the consequences of this diversity for abundances and distributions of species. I answer these questions through an integrative combination of field and natural history observations, manipulative laboratory experiments, and quantitative analysis and modelling, grounded within conceptual frameworks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"26f7143ab6a67e9866e2f9d6bd18a253","permalink":"https://jacintak.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I want to know where animals live and how they persist in their environment. My interests lie at the intersection between ecology, evolutionary biology and experimental biology.\nMy research centers on characterising diversity in traits and understanding the consequences of this diversity for abundances and distributions of species. I answer these questions through an integrative combination of field and natural history observations, manipulative laboratory experiments, and quantitative analysis and modelling, grounded within conceptual frameworks.","tags":null,"title":"Jacinta Kong","type":"authors"},{"authors":["Jacinta Kong","N. C. Wu"],"categories":null,"content":"Thermal tolerances, such as critical temperatures, are important indices for understanding an organism’s vulnerability to changing environmental temperature. Differences in thermal tolerance over ontogeny may generate a ‘thermal bottleneck’ that sets the climate vulnerability for organisms with complex life cycles. However, a species’ microhabitat preference and methodological differences among studies can generate confounding variation in thermal tolerance that may mask trends in large-scale comparative studies and may hinder our ability to assess climate change vulnerability within and among species. Here, we evaluated two approaches to resolving ontogenetic and environmental drivers of thermal tolerance and methodological variation: mathematical standardisation of thermal tolerance and classifying microhabitat preferences. Using phylogenetically informed, multi-level models with a global dataset of upper critical temperatures from 438 Anuran species, we found ontogenetic trends in thermal tolerance were similar across microhabitat preferences and standardising critical temperatures against common methodological variation had little impact on our conclusions. Our results suggested thermal bottlenecks are not strongly present in Anurans but instead, implied strong developmental or genetic conservatism of thermal tolerance within families and ecotypes. We discussed considerations for resolving confounding variation to interpret thermal tolerance at a macrophysiological scale.\n","date":1671469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671469200,"objectID":"e78c0cf5d77134ca231e683164c872b1","permalink":"https://jacintak.github.io/talk/BES2022/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/talk/BES2022/","section":"talk","summary":"Can we improve our ability to identify climate vulnerability in ectotherm life cycles?","tags":["postdoc","conference","ectotherms","temperature","comparative analysis"],"title":"BES 2022","type":"talk"},{"authors":["Jacinta Kong","J.-F. Arnoldi","A. L. Jackson","A. E. Bates","S. A. Morley","J. A. Smith","N. L. Payne"],"categories":null,"content":"Abstract The capacity of ectotherms to adjust their thermal tolerance limits through evolution or acclimation seems relatively modest and highly variable, and we lack satisfying explanations for both findings given a limited understanding of what ultimately determines an organism’s thermal tolerance. Here, we test if the amount of heating an ectotherm tolerates throughout a heating event until organismal failure scales with temperature’s non-linear influence on biological rates. To account for the non-linear influence of temperature on biological rates on heating tolerance, we rescaled the duration of heating events of 316 ectothermic taxa acclimated to different temperatures and describe the biological rate-corrected heating duration. This rescaling reveals that the capacity of an organism to resist a heating event is in fact remarkably constant across any acclimation temperature, enabling high-precision estimates of how organismal thermal tolerance limits vary under different thermal regimes. We also find that faster heating consistently reduces biological rate-corrected heating durations, which helps further explain why thermal tolerance limits seem so variable on absolute temperature scales. Existing paradigms are that heating tolerances and thermal tolerance limits reflect incomplete metabolic compensatory responses, are constrained by evolutionary conservatism, or index failure of systems such as membrane function; our data provide a different perspective and show that an organism’s thermal tolerance emerges from the interaction between the non-linear thermal dependence of biological rates and heating durations, which is an approximately-fixed property of a species.\n","date":1670284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670284800,"objectID":"c46224d6f32815a903cfd3157c9d5a0c","permalink":"https://jacintak.github.io/publication/2022-biorxiv/","publishdate":"2022-12-06T00:00:00Z","relpermalink":"/publication/2022-biorxiv/","section":"publication","summary":"Abstract The capacity of ectotherms to adjust their thermal tolerance limits through evolution or acclimation seems relatively modest and highly variable, and we lack satisfying explanations for both findings given a limited understanding of what ultimately determines an organism’s thermal tolerance. Here, we test if the amount of heating an ectotherm tolerates throughout a heating event until organismal failure scales with temperature’s non-linear influence on biological rates. To account for the non-linear influence of temperature on biological rates on heating tolerance, we rescaled the duration of heating events of 316 ectothermic taxa acclimated to different temperatures and describe the biological rate-corrected heating duration.","tags":["ectotherms","temperature","postdoc","comparative analysis"],"title":"Heating tolerance of ectotherms is explained by temperature’s non-linear influence on biological rates","type":"publication"},{"authors":null,"categories":null,"content":"\rIntroduction\rThis is a static version of the learnr tutorials for the Statistical Modelling practicals.\nWelcome! In two practicals and in three lectures we will be looking at statistical modelling.\nWe have progressed beyond the realm of doing stats by hand and basic statistical tests like t-tests are not appropriate for the types of data we will look at or the hypotheses we test. R will be the tool we will use.\nIn these sessions, we will focus on the practical aspects of applying statistics (implementation \u0026amp; interpretation) with real data – not on the theory or mathematical proofs – in fact, nearly all the relevant theory has been introduced to you in previous lectures.\nThe practicals and lectures will feed off each other and these will also build upon the previous lectures and practicals. All concepts in this practical are assessable and relevant to the lectures. I expect you to be up to date with the module material.\nThe demonstrators are available if you have questions about the concepts or have technical issues with running the practical. Please talk to them, they get lonely. They will be checking in and making sure you are keeping to time.\nWhat is statistical modelling?\rRecall from the lectures:\nStatistical models are logical mathematical or statistical descriptions of what we believe to be important in a biological system.\nWe use models to:\nMake sense of a complex and messy real world and its data\rFormulate new theories\rTest our understanding of a biological system\rFormulate hypotheses\rMake predictions\rMake evidence-based decisions\rRun simulations – “what if…?”, what happens if we change something?\rIf our model doesn’t match what we observe from empirical data, that’s OK! We can refine our model (our hypothesis) to better match what we observe. That’s the Scientific Method.\nRemember!\rScience isn’t about “proving” a hypothesis or being “right” – we find “support” for a hypothesis and it’s when we “fail” or are wrong that progress happens.\rTo paraphrase: all models are wrong but some can be useful\nTypes of models\rModels can be characterised in several ways depending on what they describe:\nTheoretic models describe processes – explored here\rEmpirical models describe data – explored in lectures and in the next practical\rSome biological systems can be described both ways.\nModels can be placed on a spectrum of simple to complex, and specific to general.\nModels can also be described based on their precision, realism and generality. But these trade-off – No one model can perfectly capture all these qualities.\nAll types of models are based on assumptions or require information about the biological system. Models describe a biological system with some limitations.\nThe best model to use depends on the intended question.\nVariables vs parameters\rDistinguishing between variables and parameters is important in statistics, particularly for modelling.\nVariables\nThese are quantities that change with each iteration of a statistical model. E.g. the predictor (independent) variable and the response (dependent) variable.\nParameters\nThese are quantities that do not change with each iteration of a statistical model. They are often a constant and often represent the assumptions about our biological system we make to model it. If we change the fundamental assumptions of the model, then the value of the parameter may change. Often the value of the parameter is unknown to us and need to be parametrised from empirical data.\nA variable could switch to being a parameter or vice versa depending on the experimental design and hypothesis tested, which in turn determines the statistical model. These decisions should be made when planning an experiment, not during or afterwards.\nWe will go through the process of parametrising in the next practical and in the lectures.\nPractical information\rIn these practicals we will look at a theoretic model of a predator-prey interaction.\nThe aim is to understand how statistical models can be applied to biological data.\nThe majority of this practical will focus on the practicalities of designing experiments and collecting data, which we covered earlier in the module.\nI recommend taking your time with these practical and the concepts because they are fundamental to biological statistics you are likely to encounter again (plus are highly relevant to your final report). There is no need to rush.\nLearning objectives\rThis practical is split into three parts with distinct learning objectives (recommended time to spend):\nPart A: Building a theoretic model (20 mins)\nKnow how a biological process can be described by a statistical model\rUnderstand how statistical models are applied to answer real world biological questions\rPart B: Designing an experiment (20 mins)\nApply your knowledge about experimental design and hypothesis formulation to a biological problem in practice\rDon’t spend too long on Parts A and B, move through them and the CA quickly. Part C is the most important part and will take the most time.\nPart C: Collecting data (2 hours)\nThis is the main activity of the practical.\nKnow how to organise a spreadsheet and fill in a spreadsheet with data following the principles of tidy data\rYou will be uploading your data to Blackboard. In the next practical we will be analysing the aggregated \u0026amp; anonymised class data set and using that to answer CA questions.\nContinuous Assessment\rI recommend finishing the CA before Part C. The CA is worth 10% and is due at the end of the prac. There is a total of 10 marks.\nThe CA aims to assess your understanding of the concepts in this practical and your ability to apply the concepts in practice.\rPart A: Theoretic models\rIn the previous practical you looked at a killer T cell digesting a pathogen. This infection response is a biological process and is analogous to a predator (e.g. a lion) hunting a prey (e.g. zebra), or a robot (a “predator”) cleaning up an oil spill (its “prey”).\nYou started to compose a Scratch model. Although the Scratch model comprises of pictures and code blocks, under the hood these blocks represent computer code, and more abstractly a mathematical processes. Thus, the Scratch model is an implementation of a statistical model of a biological system – the infection response.\nThis model was simple – too simple to be realistic. The killer T cell captured any and all pathogens touching it whereas in reality, a killer T cell may only target a few pathogens at a time or sequentially. This is also true for animal predators who hunt. Most predators need time to catch and process their food before their next meal.\nWe can conceptualise a more realistic representation of an infection response, or more abstractly a predator-prey interaction. By breaking down what we observe to be important about the infection response into variables and assumptions, we can build our own theoretic model (describing a process) of an predator-prey interaction from scratch.\nWhat is a predator-prey interaction?\rIn a predator-prey interaction we have two variables:\nPredator\rPrey\rIf we think about what we consider to be important in a predator-prey interaction, searching for and handing prey two mutually exclusive aspects. We can then make the following statements, or assumptions, about the predator-prey interaction:\nA predator randomly searches for prey\rA predator can only “search” a fixed area per unit time (search rate)\rA predator can only eat one prey at a time – it must “process” (i.e. digest) the prey before it can begin searching for the next one (handling time)\rHandling time and searching activity are mutually exclusive\rPrey randomly move around independently of the predator (e.g. it does not slow down or speed up when the prey is near)\rBoth the predator and the prey move at the same speed and at a constant speed\rBoth the predator and the prey only move within a fixed area (e.g. their habitat), they cannot leave.\rThe numbers of predators and prey are fixed at the start of the experiment (i.e. they do not replicate while the model is running, prey numbers can only decrease, predator numbers stay the same)\rEach predator-prey interaction lasts a fixed amount of time (total time)\rThese same statements can be applied to an infection response or to any analogous scenario – merely replace “predator” and “prey” with the relevant terms.\nDiscussion\rWould you agree with the above statements? Or are they too simplistic of a complex biological system?\rIf you thought our model is too simplistic, you are probably right but in modelling philosophy some degree of simplification or abstraction is perfectly acceptable because models are only representations of reality, they are not meant to copy the real world right down to the minutiae.\nThus, one application of models is to consider them as hypotheses of biological systems – what we think are important components of the system.\rFunctional responses\rInfection responses, predator-prey interactions… they can be generally classified as functional responses. Luckily for us, there are already well known analytical (mathematical) models of functional response that we can use.\nThere are multiple types of functional responses (labelled with Roman numerals: I — IV). Each of these models represents a different hypothesis about functional responses and mathematically describes a different relationship between our two variables (e.g. the killer T cell and the pathogen, or a predator and its prey). You can read more about functional responses by running vignette(\"functional_responses\").\nThe assumptions we make above are based on a Type II functional response model, which is common in biology. You can read about the full derivation by running vignette(\"TypeII_models\").\nThe Type II functional response model is a good example of a general model – it has been used to describe animal predators eating prey (e.g. Holling’s disc equations) or enzymes catalysing reactions (e.g. Michaelis-Menten equation). We can also use it to describe the immune response of the previous practical.\nIt doesn’t matter what the mathematical symbols represent biologically, maths is a universal language that describes the underlying biological process. Here, we are applying it to an general predator-prey scenario involving students and plastic counters.\nA Type II functional response\r(#fig:funct_resp)Type II functional response of a predator-prey interaction\rDiscussion\rHow would you describe the Type II relationship in words in a results section of a lab report?\rMaybe something like:\n“The number of prey eaten rapidly increases at low prey densities and gradually plateaus to a maximum number of prey eaten at higher prey densities.”\nThe mathematical model of the figure above is:\n\\[H_a=\\ \\frac{a\\times H\\times T_{total}}{1+a\\times H\\times T_h}\\]\nYou should read the full derivation in the documentation file to see how we’ve turned our earlier assumptions into mathematical expressions – run vignette(\"TypeIImodels\") in Console or via the Packages tab (click StatsModels -\u0026gt; User guides).\nLet’s go through what the letters and numbers mean and how they link to our model assumptions.\n\\(H\\) is the number of prey within a fixed area (prey density, number per area). This is our predictor variable. We decide what values to use before our experiment\r\\(a\\) is the search rate or attack rate. It is the search area per unit time of a prey. This is a parameter that we do not know and that we calculate from our data.\r\\(H_a\\) is the number of prey captured. We record this at the end of our experiment as our response variable.\r\\(T_{total}\\) is the total time a predator spent hunting prey (time). This is a constant parameter that we decide before we start the experiment based on our assumptions.\r\\(T_h\\) is the time a predator spends digesting a single prey (time per prey). This is a parameter that we do not know and that we calculate from our data. In functional response models, this is called handling time.\rYou’ll see that we know the value of some of these parameters already and some we need to calculate from our data.\nDiscussion\rVariables or parameters? Which is which in our model? What information do we know already and what do we need to find out? And how would you find it out?\rFinding our unknown parameters\rHow do we estimate these unknown parameters from our data (e.g from the figure above)?\nActually, the hyperbolic nature of the Type II makes it challenging to extract this information – and certainly beyond the expectations of this module. But we can use mathemagics to turn this hyperbolic relationship into a straight line by a process called linearising. And straight lines are easier to manipulate or interpret – something we expect you to be able to do in this module.\nThe linearised Type II equation is:\r\\[\\frac{1}{H_a}=\\ \\frac{1}{a}\\times\\frac{1}{H\\times T_{total}}+\\frac{T_h}{T_{total}}\\]\rThe full derivation is accessible in vignette(\"TypeIImodels\"), we won’t go though how this is derived here, but you do need to understand this equation:\n\\(\\frac{1}{H_a}\\) is the inverse of our response variable – the number of prey eaten\r\\(\\frac{1}{a}\\) is the inverse of our unknown search rate parameter\r\\(\\frac{1}{H \\times T_{total}}\\) is the inverse of our predictor variable (prey density) and the total predator-prey interaction time\r\\(\\frac{T_h}{T_{total}}\\) is our unknown handling time divided by the total predator-prey interaction time\rDiscussion\rDoes the overall structure of the equation look familiar to you? (maybe from high school maths)\rThe graph looks like this:\rFigure 1: Linearised type II functional response of a predator-prey interaction\rThe linearised function has the general form \\(y = \\beta_1 x + \\beta_0\\) which is a straight line. \\(y\\) is the response variable, \\(x\\) is the predictor variable, \\(\\beta_1\\) is the slope or gradient of the line, and \\(\\beta_0\\) is the intercept of the line. We will see this notation again in the lectures.\nThis linear form allows us to estimate the coefficients for the linear equation where y = \\(\\frac{1}{H_a}\\) and x = \\(\\frac{1}{H\\times T_{total}}\\).\nFrom the linearised Type II equation we can see that:\n\\[Slope=\\ \\frac{1}{a}\\]\n\\[Intercept=\\ \\frac{T_h}{T_{total}}\\]\nWe can calculate the slope and intercept from regression and then do some algebra to find \\(a\\) and \\(T_h\\):\n\\[a=\\frac{1}{slope}\\]\rand\r\\[T_h = T_{total} \\times intercept\\]\nNow we have our model, we need to parameterise it and find the values of our unknown parameters.\nFor the rest of the practical we will look at the question:\rWhat are the values of our unknown parameters, handling time and search rate?\rUnlike the previous practical, we are going to investigate this question by conducting a real experiment but it is possible to build an equivalent simulation in Scratch.\nThat’s the end of Part A. Check in with your demonstrator. Take a break. Stand up. Shake your limbs. Breathe.\nYou’re doing great!\nPart B: Designing an experiment\rThe predator-prey experiment\rYou have been provided some plastic counters, a stopwatch, a piece of paper and a jar with a lid. You are the predator and the counters are your prey. The piece of paper is your arena and the jar is for collecting prey that you have captured.\nYou will be working in pairs – one of you will be the predator, the other will be the observer (keeping track of the time, recording the data). You will need a computer between you for entering and uploading data.\nThe protocol is simple:\nSpread out the desired number of counters on the A4 sheet randomly. Do not move counters around once landed\rThe predator will close their eyes and use their index finger to randomly tap around the arena at a constant search rate called out by the observer. The observer has a stop-watch and tells the predator when to tap.\rIf the predator touches a counter, they need pick it up and “processes” it. The prey caught must be placed in the provided jar.\rThe predator cannot hold the jar while searching for your prey because prey handling and prey search are mutually exclusive processes.\rThe predator must pick up the jar, unscrew the lid, drop in the counter, close the lid fully and place the jar back down on the table – all with their eyes closed\rYou can replicate using a different “predator” (student) to take turns but you should use the same predator across all prey densities.\rThe goal is to pick up as many counters as possible within a fixed time period.\nDiscussion\rWhy should we not touch the counters after we have placed them on the paper?\nWhy can’t we hold the jar while looking for prey?\nIdentifying treatments\rExperiments consist of variables that are controlled or manipulated. The manipulated variables are usually predictor variables. Controlled variables are other influential variables that may affect the response variable in ways that might mask or accentuate the effect of the predictor variable on the response variable. We control as many possible variables as possible to make sure what we observe is the true effect of the predictor variable of interest.\nIf we cannot control a variable, the least we can do is record it as a co-variate and apply more complex stats to account for the additional variable.\nIf we don’t keep a comprehensive record, then we risk the co-varying variable confounding the true effect of the predictor variable, adding uncontrolled variation (decreasing the chance of getting a significant P value) and increasing the risk of Type I or II error.\rWe change the prey densities with each run of the experiment because it is the predictor variable. The different values of prey density is our treatment.\nHow many prey densities should you use?\nGenerally, the more treatments, the better your data may capture some true biological relationship.\nBut more treatments means more work, time and effort! You need at least two treatments (two observations) to fit a straight line to data but there is no hard or fast rule for deciding these things.\nFor the purpose of this practical, we will decide on 5 treatments of prey density. You are free to choose any 5 numbers from a range of 1 to 100.\nWe will collate the entire class’ data so you don’t have to choose the same numbers. We aim for consistency and balance when designing experiments, so your treatments should be equally spaced out. For example, with equal increments.\nReplication, replication, replication\rReplication is really important to increase the accuracy and precision of our data and make sure our results are not due to random chance.\nGenerally, the more replication the better!\nAgain, the trade-off is more work, time, effort or computing power. It also reduces the chances of making a type 1 or type 2 error.\nFor the purpose of this practical, we will use 3 replicates of prey density.\nThree is usually a minimum number of replications. In a real-world scientific study, you may see higher replications.\nA replication of 3 means that we need to repeat our experiment three times for every value of the treatment. Each replication should be independent of the others. Otherwise you risk pseudoreplication – not replicating when you think you are.\nIdentifying constant parameters\rIn our experiment we have 1 predator tapping at a constant rate.\nOur tapping rate is not the same as our unknown parameter search rate (also called area of discovery or attack rate).\nTotal foraging time is another pre-determined constant.\nWe will use a tapping rate of 1 tap per second. Your assistant will need to call out when to tap for the predator.\nWe will use a value of 1 minute for total foraging time because we can divide by 1 and it makes our maths easier.\nThe number of predators, tapping rate and total time are all parameters and they are constant in a single experiment. These are constants that does not change – doing so would violate the assumptions of our model, change the experimental design and hypotheses tested, nd turn these parameters into variables.\nChanging these but keeping our hypotheses the same would be introducing additional variation into our data and increase the chance of making a type 2 error. Thus, sometimes parameters become variables; it all depends on the aim of the study.\nAlthough these constant parameters are unknown to us, we want to know their values to use the analytical model but we need empirical data to fit to our analytical model. We will estimate these values using statistical models next practical to complete the equation:\n\\[H_a=\\ \\frac{a\\times H\\times T_{total}}{1+a\\times H\\times T_h}\\]\nRemember!\rKeep it simple! A more complex statistical analysis cannot fix issues arising from a badly designed experiment.\rHypotheses\rThe final but most important thing before we can start collecting data is to formulate our hypotheses.\nAnother use of statistical models is to explore different scenarios that represent different hypotheses.\nWhat happens if our predator was more efficient? What would you change about the experiment to achieve this? How would this aim affect \\(H_a\\) and the estimates of \\(a\\) and \\(T_h\\)?\nFor example, vaccines provide a chance for the immune system to “learn” how to eat prey (e.g. viruses). After this experience, immune systems become more efficient at destroying prey.\nWe will simulate a different predator behaviour by repeating our counter and jar experiment but this time, the jar does not have a lid so the predator does not need to open and close the lid when they are handing the counters. We will call this new variable foraging strategy.\nThese two experiments are independent. If we want to compare the foraging properties between the two predators, we need to run the experiment again keeping everything the same but changing only the variable of interest.\nForaging strategy becomes a second predictor variable that has two possibilities:\nJar with a lid; the original protocol simulating a less experienced predator. We will call this yes_lid\rJar without a lid; the second experiment simulating a more experienced predator. We will call this no_lid\rThe null hypothesis (H0)\rRemember our research questions is: What are the values of our unknown parameters, handling time and search rate?\nWe know from our model that we expect a positive linear relationship between the inverse of prey captured and the inverse of prey density multiplied by total foraging time.\nWe have a second question with the addition of foraging strategy: Does foraging strategy affect handling time and search rate?\nWe can write a null hypothesis (H0) about the effect of foraging strategy on our model parameters:\nH0: There is no difference in the number of prey eaten with prey density, and in handling time and search rate between foraging strategies\nRemember!\rThe alternative hypothesis is the outcome we would expect if there was an effect of the predictor on the response variable. The null hypothesis is what we expect when there is no effect (hence, null). Most of the time we want there to be an effect (what we hope is correct about a biological system).\rThe alternative hypothesis (H1)\rHere’s the most basic alternative hypothesis we can formulate\nH1: There is a difference in the number of prey eaten with prey density, and in handling time and search rate between foraging strategies\nNotice, it’s just the opposite of the null hypothesis.\nIt’s often more interesting to formulate a more specific alternative hypothesis than just the opposite of the null hypothesis if we expect there’s a direction to the effect of the new predictor variable on the response variable. Which we do in this practical.\nDiscussion\rFormulate a more specific hypothesis we can use as an alternative hypothesis.\nHint: What are we aiming to do with this experiment? What sort of detail should be included?\nIt’s better to be specific in your hypothesis if it is plausible in your biological system. For example, if we are testing whether a vaccine induces an immune response, the resulting immune response can only be positive – a negative immune response (i.e. becoming more susceptible to disease) is implausible (and unfortunate).\nThus, a simplistic hypothesis would not provide an informative or helpful conclusion if we were testing a new vaccine. If the vaccine made people more susceptible to disease, we would still accept our alternative hypothesis and reject the null hypothesis even if the direction of the effect was the opposite to our idealised outcome (improved immunity).\nIf you are less certain about your predicted outcomes you could be less specific if you wanted to (e.g. “There is a linear relationship between prey density and number of prey eaten” means the hypothesised relationship could be positive or negative).\nChoose wisely!\rThe wording of your hypothesis will dictate the best experimental design and statistical analysis to test your hypothesis! Think carefully about the details.\nKeep your hypothesis as simple as possible to investigate what you are interested in, in your biological system. A more complex hypothesis means a more complex statistical analysis is needed.\nThere are no limits to how many hypotheses you have. You could have three hypotheses, a null and two alternatives describing different possible outcomes.\nRemember!\rAs scientists we never try to “prove” or “disprove” hypotheses. We aim to gather evidence and see whether the evidence supports our hypothesis.\rConducting a simple statistical analysis on a complex hypothesis and experimental design means that there is variation in your data that is not accounted for and could be affecting your response variable – it increases the chances of making a type II error (the meaningful biological relationship is hidden under statistical noise).\nWhen in doubt, keep it simple.\nDetails matter!\rThinking ahead, you should also be careful how you describe the relationship between multiple predictor variables in your hypothesis. Predictor variables can be independent or they can affect each other’s influence on the response variable.\nConsider these sentences with two predictor variables that describe the same relationship with Y, but different relationships between X1 and X2:\nY decreases with X1 and X2 but the relationship does not differ between X1 and X2\rY decreases with X1 and X2 and the relationship differs between X1 and X2\rThese could be a set of hypotheses. How would you expect the graphs of these relationships to look like?\rDiscussion\rWhich of the statistical tests you’ve already learnt would you use to analyse the data if foraging strategy was the only predictor variable? (no prey density)\nWhat about if prey density was the only predictor variable? (no foraging strategy)\rThat’s the end of Part B. We have designed an experiment and identified our hypotheses.\nTake a break. Check in with your demonstrator. Stand up. Shake your limbs. Breathe.\rKeep it up!\nPart C: Collecting data\rPlanning about collecting data before collecting any data will save you a lot of headache if you realise halfway through that you’ve made a mistake and have to start over, or that there is a better, more efficient way of organising your data. Planning is everything.\nScientists often work in teams, which means that several people might collect data. This means that every person must collect data in a consistent way so that all the data can be aggregated. It also means that what information to collect and how it is stored or shared needs to be agreed on before anyone starts. One might also need to consider any laws or regulations for how data is collected (e.g. research ethics, privacy laws).\nHaving a standardised data sheet and way of data entry is key to maintain consistency and minimise data loss.\nTidy data\rTidy data is set of principles for organising data sheets and filling in data. It is designed in a way to make data analysis easier and reduces the amount of work required to prepare your data for analysis.\nIn short:\nEvery column is a variable\rEvery row is an observation\rNo empty cells\rYou can read more about tidy data here.\nTo paraphrase another quote:\nAll data has a place and all data is in its place\nLong vs Wide data\rThere are two general ways of presenting data in a spreadsheet (or as tabbed data):\nLong – Variables are presented in one column, the corresponding value is presented in a second column. This format can be hard to interpret\rWide – Each variable is a separate column (many columns) and each row is a single observation. This is the way we are collecting data in class (see above criteria)\rAs a general rule, aim to add rows (observations) not columns and columns should be independent of each other.\nCertain types of data analysis require a wide or long format. Sometimes we need to convert the data structure from wide to long or vice versa. We won’t be needing to do this in this module because we are taking care with designing our data sheet to make it easier to analyse our data with minimal post-collection data manipulation.\nFilling in a data sheet\rTo help you, there is a data sheet template on blackboard you can fill in.\nHere are the rules we want you to apply here (these are not global standards and may vary elsewhere):\nCaSe coNsistEncy in all text\rAll words including column headers should be in lower case, no spaces\rColumn headers should be informative\rpathogen_density or total_time is good (even the mathematical notation like H or Th because it is standardised)\rcol1 or pd is bad – not easy to understand (e.g. acronyms not everyone uses)\rThe column headers in the template are pre-filled. You do not need to change them.\rNo empty cells\rAll cells should have a value. Put NA if there is no value\rEvery cell is a single value\rDo not mix alphanumeric characters (e.g. l3773r5 \u0026amp; numb3r5)\rNumeric columns should only contain numbers (0-9), no letters. Character strings should only contain letters (a-z), no numbers\rAvoid special characters (e.g. (), ~, \u0026amp;, / etc.)\rNo spaces (called whitespace in computing)\rNo spaces between words. Use _ instead. E.g. no_spaces\rNo spaces before or after words\rThis rule doesn’t apply to generic text fields for taking notes, comments or observations. It applies to treatments of a variable if they are described by strings, e.g. no_fertiliser, yes_fertiliser\rData is stored in rows and columns in a tidy format\rAll data is presented together\rNot in multiple sheets – use a separate file if you must.\rData is raw\rMeaning it is not manipulated or processed after being recorded. We will do any data processing in R later\rThis is also important to maintain transparency in our workflow\rAll data is presented, not summarised. E.g. as means\rData is presented in a portable csv format\rNot everyone has access to Excel. What if someone uses Google Sheets? “.xlsx” files are for Excel.\r“.csv” comma separated files are portable across platforms, use less memory and are easy for a computer to read – save your data as csv\rAre all the necessary columns/information included and in the right order?\rDo not change the headers of the template\rInfo!\rThese guidelines are how R reads data from a spreadsheet. Thus, following these best practices will prevent errors when importing data into R.\nWe want you to follow these rules exactly because we will collate your datasheets across the class.\nDatabase lingo\rWhen data is imported into a computer, the computer will classify it according to the type of data. Here are some common data types:\nnumeric – fields or cells that only contain numbers\rstring – strings of characters, i.e. text. Can be letters and numbers. Called character in R\rdate – for dates\rlogical – for logical statements, e.g. TRUE/FALSE\rThese are the main data classes characterised by R. There may be other types in other programs (e.g. Access, SQL). In computer science, variables (i.e. columns) in R are called fields, so “predator density” and “total time” are fields.\nDo your experiment \u0026amp; collect your data\rSee the table below for an example of column headers with no spaces and all cells with a single value. These are the information you need to provide in your data sheet and the type of data they are:\ncolumn_information\rinformation_type\rdescription\rstudent_number\rnumeric\ryour student number\rtotal_time\rnumeric\rin minutes\rprey_density\rnumeric\rNA\rreplicate\rnumeric\rNA\rprey_eaten\rnumeric\rNA\rforaging_strategy\rcharacter\rno_lid for no lid or yes_lid for a jar with a lid\rThese headers are pre-filled for you in the correct order on the template. You do not need to change anything. The order of columns has to exactly match the order in the table.\nWarning!\rR is a computer program so you need to be specific about how you present information to it. For example, you may think having your treatments in individual tables separated by empty rows is sensible and logical but those empty cells mean missing data to a computer – so the computer thinks you haven’t filled in the data properly!\nA space counts as a character so \"cat\", \"cat \" and \" cat\" would be read as three distinct inputs. This kind of human error can mess up your analysis but is very common.\rSave your data regularly!\nData cleaning\rDanger!\rInformation that isn’t structured properly will have errors when imported into R. This is why we must always check that data imported properly every time we load data.\nThis data will need to be cleaned before it can be analysed. So putting in the effort and attention to detail now will save future you a lot of avoidable stress!\rIn data science, data usually needs to be processed before we can use it. The data needs to be checked and validated. Are all the fields entered correctly?\nAttention to detail is important in case it introduces mistakes in our data that may bias our analyses! How much do you trust your fellow students to have followed the previous instructions to the letter?\nChecking and correcting data is called cleaning. We take raw data and clean it up. Sometimes data cannot be fixed (e.g. errors of unknown origin), then the conservative approach is to exclude the observation which is a shame if lots of time and effort (or money) went in to collecting it. Cleaning often takes longer than the actual analysis! It is a real pain! \u0026gt;:(\nIn the real world, human-collected data is almost never clean because humans are not robots. In previous years, as high as 95% of submissions for this practical failed the data entry guidelines above. Can you do better?\nUploading your dataset to Blackboard\rSave your data sheet as a csv file (click “Save As” and select csv file type). No other file types are accepted. Your file name should be each of your student numbers and practical session separated by _. e.g. 54321_12345_AM.csv for the morning session.\nUpload your csv file to the assignment link provided on Blackboard. Only one person in the group needs to upload a file.\nFinal checklist\rUse the following check list to make sure you have done everything you need for this practical.\nDoes your data sheet fully conform to the guidelines above?\rUploaded your dataset to Blackboard?\rCorrect file type?\rCorrect file name?\rDone the CA questions?\rThat’s a wrap! Well done for making it to the end. Check in with your demonstrator.\nTake a break. Stand up. Dance. Breathe.\rTo sum up, we:\nComposed a theoretic model of a predator-prey interaction with known and unknown parameters. The model describes a set of hypotheses based on assumptions we have made about the predator-prey interaction\rIdentified a biological question and hypothesis\rDesigned an experiment with adequate treatments and replication\rConducted an experiment and recorded data\rFilled in a spreadsheet with data following the principles of tidy data\rAll of these steps are part of the scientific method and are relevant to your final assessment.\nIf everyone follows the instructions, then all your datasets will be comparable and we can combine everyone’s data into one giant (anonymised) dataset. Meaning more data to play with than if you had done it by yourself. The idea is that with a large enough sample size (an entire module of students) we can get a good coverage of numbers between 1 and 100 for prey density values.\nIn the next practical we will be analysing the aggregated \u0026amp; anonymised class data set. We will use another type of statistical models (a linear regression) to statistically describe the data we collect and test the hypotheses of our functional response model.\n","date":1667088000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667088000,"objectID":"757f49caae264e58336a061f3c96db69","permalink":"https://jacintak.github.io/teaching/StatsModelsPrac1/","publishdate":"2022-10-30T00:00:00Z","relpermalink":"/teaching/StatsModelsPrac1/","section":"teaching","summary":"Static handout of Part 1 of Statistical Modelling practical","tags":["teaching","R stats","code"],"title":"Statistical modelling Part 1","type":"teaching"},{"authors":null,"categories":null,"content":"\r1. Why programming and computing?\rBasic computing terms\rDrives, Folders \u0026amp; Files\rDirectories\rWhy R?\rWhy RStudio?\r2. Installing R and RStudio\rWindows\rMac\rLinux\rChromebook\r3. Opening RStudio for the first time\r4. R structure and terminology\rInstalling more packages\rThe packages folder (your library)\r5. My first script\rLoading packages\rCommenting\r6. Working directories in R\rChanging working directories\rRubber ducks\rFinding help\rFinal checklist\rHave a look at this checklist:\nFundamental skills checklist\rDo you know how to navigate computer directories and get addresses of files or folders?\rDo you have R and RStudio installed and up to date on your computer?\rDo you know what an R package is and how to install them?\rDo you know how to set up working directories and use scripts in RStudio?\rIf you’ve answered no to any of the above questions, then continue on. Some of you may already have R and RStudio installed from previous modules - that’s great! Make sure everything is up to date. You can refer back to this document in the future.\n1. Why programming and computing?\rWe care about teaching you programming and computing skills because they are important skills in the current workforce and are not to be taken for granted. Programming is not just code, it’s a way of thinking and requires problem solving skills that are applicable to other scenarios. You can apply these skills to a range of problems or examples beyond what we will cover in this module.\nBest Practice\rLearning programming is like learning a language - there is grammar and syntax. It takes time and effort to learn and to practice. Don’t expect to pick it up immediately - take it in small steps and practice as much as you can.\rBasic computing terms\rWe don’t always formally learn computing terminology even if we’ve always been using computers. Here are some general computing concepts we will be using and that we expect you to be familiar with:\nDrives, Folders \u0026amp; Files\rIn computer science, most commercial operating software are organised in to drives, directories, folders \u0026amp; files.\nUsing Windows as example (Macs are similar):\rC:/ is a drive (a hard drive). Within the drive, information is sorted in folders (e.g. Documents). Within folders are lists of files which contain information (e.g. my_document.doc). Here, “.doc” is the file extension that tells the computer what kind of file it is and what program to use to read the information (a Word document). Folders within folders are sometimes called sub-folders.\nBasic commercial computers tend to have only one drive but you can have as many drives, folders and files as you want. For example:\nC: (the hard drive)\r-\u0026gt; my documents (within drive C)\r-\u0026gt; file A (within my documents)\r-\u0026gt; folder 1 (within my documents)\r-\u0026gt; folder 2 (within folder 1)\r-\u0026gt; file B (within folder 2)\rDirectories\rDirectories are the cataloguing system describing where files and folders are stored; also called addresses. Directories take the form of an address like “C:/documents/folder/file” which tells a program to look in this location.\nThe concept of folders and files comes from the days before computers where information was written on paper and stored in filing cabinets. Directories are not case sensitive. Case sensitivity means a computer reads capitalised letters (ABC) differently to lowercase letters (abc).\nBest Practice\rWe recommend that you organise your files in a structured way. This will make importing data into R easier. For example, have a folder for the module and sub-folders for each of the practicals:\nDocuments (folder) -\u0026gt; Biostats (folder) -\u0026gt; Practical 1 (folder) -\u0026gt; Script (file)\nWe don’t recommend using a automatically selected “downloads” folder or your “desktop” because these are not permanent file locations and it can be hard to find files in a messy folder.\rNotice that the components of the address is separated using a forward slash /. R can understand / but does not understand back slash \\ because back slashes have a specific meaning in programming. When typing addresses, make sure you use the correct slash.\nDanger!\rComputers are only as smart as the humans that use them.\nIf your code is not working there is most likely a spelling mistake or a typographic error. These human errors are easy to miss but equally easy to fix!\nThere is a difference between \"straight\" or “curly”, ‘single’ or “double” quotation marks! R does not recognise curly quotation marks so beware when pasting code from elsewhere.\nWe navigate through our computer’s directories using Explorer in Windows or Finder in MacOS. You can see the address of a directory in the address bar. We will learn how to use directories in RStudio.\nExpectations\rWe expect you to be able to manage your own computer organisation. E.g. know how to open a file, save files in a specific file type and know where you saved files!\nWe will ask you to load data from files using addresses so knowing where you saved the file is essential.\nWhy R?\rR is just one of many high-level programming languages used professionally (e.g. C++, Java, Python) but R is specially designed for doing statistics and handling data. Hence R is widely taught in statistic classes.\nKnowing R is not the same skill as knowing Excel.\nR is far more powerful for doing data science.\nR is an open source software and it is transparent, meaning you can see how your data is being manipulated. Transparency allows us to check whether the statistics is done correctly and is easier to see how statistical theory is being applied. This is not always possible to see in commercial statistical software or Excel.\nR is also reproducible because R allows you to document your code in scripts that you can give another person to replicate your analysis.\nThe traditional option to learning statistics is to do these calculations by hand with pre-calculated statistical reference tables but doing calculations by hand is an arduous task for realistic biological problems.\nBest Practice\rOne way to practice R is to use it preferentially whenever you can, even in other modules. For example, use R to make graphs for your assignments instead of Excel. Or to calculate simple statistics.\rBeyond what we will cover in this module, R can be used to write documents (like this one), interactive apps, websites, or presentations. The possibilities are endless.\nWhy RStudio?\rR and RStudio are different software. R is a computer programming language and statistical environment. RStudio is a user interface which has some useful features that makes using R easier. There are other user interfaces for R but RStudio has lots of support.\nRStudio acts as a mediator between you and R:\nuser -\u0026gt; input -\u0026gt; RStudio -\u0026gt; R -\u0026gt; RStudio -\u0026gt; output -\u0026gt; user\nIt is possible to use R by itself (you may prefer it) but RStudio makes everything a lot easier by providing some organisation and allows you to write scripts.\n2. Installing R and RStudio\rDanger!\rYou need to install R then RStudio in that order! RStudio cannot work without R but R can work without RStudio.\nAn analogy: R is the engine of a car and RStudio is the steering wheel - you control the wheel but the engine is what makes the car go forward.\nInstall R from CRAN. https://cran.r-project.org/\nIf you prefer instructional videos, here’s a video about installing R https://vimeo.com/203516510 and RStudio https://vimeo.com/203516968\nHere are the instructions for various operating software:\nWindows\rFor R:\nGo to https://cran.r-project.org/bin/windows/base/\rClick “Download R” in the blue box for the latest version.\rSave the file, open it and follow the instructions. You can leave everything as the default option. Make sure you’ve installed the program somewhere sensible like the Programs folder in the C:/ drive.\rOpen it and check it installed properly\rFor RStudio:\nGo to https://rstudio.com/products/rstudio/download/\rClick download for RStudio Desktop Open Source Licence. The FREE option.\rInstall the program somewhere sensible\rOpen it and check it installed properly\rMac\rFor R:\nGo to https://cran.r-project.org/\rClick “Download R for (Mac) OS X”\rSave the latest release file (e.g. R-4.0.2.pkg), open it and follow the instructions. You can leave everything as the default option. Make sure you’ve installed the program somewhere sensible.\rOpen it and check it installed properly\rFor RStudio:\nGo to https://rstudio.com/products/rstudio/download/\rClick download for RStudio Desktop Open Source Licence. The FREE option.\rInstall the program somewhere sensible\rOpen it and check it installed properly\rLinux\rFor R:\nGo to https://cran.r-project.org/\rClick “Download R for Linux”\rClick your version of Linux\rCopy and paste the relevant installation code\rOpen R and check it installed properly\rFor RStudio:\nGo to https://rstudio.com/products/rstudio/download/\rClick download for RStudio Desktop Open Source Licence. The FREE option.\rRun the relevant code\rOpen it and check it installed properly\rSee https://linuxconfig.org/rstudio-on-ubuntu-18-04-bionic-beaver-linux for a guide\nChromebook\rThere are a few options:\nThe easiest option is to run Linux on your computer, then you can install R and RStudio. Try the instructions on https://blog.sellorm.com/2018/12/20/installing-r-and-rstudio-on-a-chromebook/ or https://github.com/jennybc/operation-chromebook#links-re-r-and-rstudio\rUse RStudio Server\rUse RStudio Cloud https://rstudio.cloud/ (in beta so it may not work)\rSorry chromebook users, if your chromebook version is very old then it may not be possible to install R.\n3. Opening RStudio for the first time\rIf you open RStudio you’ll see several windows that organise how information is passed to R and how output from R is presented:\nThe default RStudio layout can be customised.\nLeft: The big window is the Console. This is the interface with R and is the same as using R on its own. (don’t worry about “Terminal”)\rTop right: This window has three tabs: “Environment” - shows you what information is stored in R’s memory, “History” - shows your code history \u0026amp; “Connections” - don’t worry about this one, it’s for connection to database servers.\rBottom right: This has several tabs. The most important being “Files” - showing you where RStudio is looking at on your computer \u0026amp; “Plots” - shows you any plots you make in R.\rWhen you open a Script, it will appear as a panel in the top left.\nExpectations\rWe expect you to be familiar with the RStudio layout and using R within it.\nYou can customise the layout, colour scheme, font and font size of RStudio in Options.\nShortcuts and hotkeys will make your life easier. You can see a list of them under Help. There are only minor differences between Windows and Mac shortcuts (e.g. using Cmd instead of Ctrl).\n4. R structure and terminology\rThere are a number of terms you’ll come across when using R. Here are some basic ones:\nR is a statistical environment that consist of packages. Packages are sets of functions that do something to input depending on the instructions described in the function. All your packages are stored in your library (a folder on your computer). When you download R it comes with a basic set of packages as default (base packages). Some of these base packages are loaded every time you open R.\nR is CaSe SenSitiVe. Meaning that library() is different to Library().\nBest Practice\rKeep your R, RStudio and packages up to date. These things are continuously updated by the community. You can update your packages by clicking the Update button under the Packages tab.\rInstalling more packages\rWe can customise and expand the functionality of R by installing more packages, which are made by the community and distributed freely. The function to download packages from the Internet is:\ninstall.packages(\u0026quot;\u0026lt;insert name of package\u0026gt;\u0026quot;) # installs a package\rFor example, install.packages(\"learnr\"). Note how non-curly quotation marks are used. Some packages have more functions to do more advanced computing, some contain datasets you can practice with.\nThe packages folder (your library)\rAdditional packages are saved to a folder on your computer. We should check that your packages folder is set up properly - especially if you use Windows 10 and sync with OneDrive.\nThis is not a problem for Mac users or Windows users that are not logged in to a Microsoft account (check it anyway, in case).\nPackages should ideally be downloaded to your local computer and not saved on the cloud (e.g. through OneDrive).\nThe constant syncing slows down communication and creates issues between the cloud and R and RStudio. You can see where your packages are saved using .libPaths() in your console - meaning Library Paths, the address of your package libraries.\nThere should be two addresses:\nOne in your Program Files or wherever your computer installed R. E.g. \"C:/Program Files/R/R-4.1.1/library\" - this is where all the base R packages are installed (the default packages that come with R)\rOne somewhere else on your computer - this is your personal library. When you install new packages from the Internet, they will be saved here.\rIf the second address is on your local computer (e.g. Documents) and not in the cloud then you don’t have to do anything. E.g. \"C:/Users/XXXX/Documents/R/win-library/4.1\" is fine\rIf the address has OneDrive in the address, then we need to fix that - follow the steps below. E.g. \"C:/Users/XXXX/OneDrive/Documents/R/win-library/4.1\" is problematic\rTo change the address of your personal library in Windows 10\nCreate an empty folder in your Documents folder called R. Make sure it is not syncing with the cloud. E.g. with the address C:\\Users\\XXXX\\Documents\\R\rClick Start (Windows icon). Type in “environmental variables” [without quotations], you should see “Edit environment variables for your account” in the search results - click that\rUnder User variables (the top window) click New\rIn the Variable name field type R_LIBS_USER - exactly like that in all caps\rIn the Variable value field enter the address of the package folder (where you made it in step 1) - make sure to use backslash \\ not forward slash /. E.g. C:\\Users\\XXXX\\Documents\\R\rClick OK twice to exit\rType .libPaths() in R to check that your personal library folder is now listed in R\rChanging default settings in the Control Panel is pretty advanced computing - we don’t expect you to know this. Don’t be afraid to ask for help.\n5. My first script\rThe greatest advantage of RStudio is that it allows you to write scripts. These are files ending in .R that are created and opened by RStudio. R itself cannot open, read or create scripts.\nScripts are text documents of code that you can save on your computer and open later. Scripts are instructions to give to R, but can also serve as a record of what you’ve done (transparency \u0026amp; reproducibility).\nBefore RStudio, we had to save our code in notepad or similar then copy and paste it into R (believe me it was a pain). Now we can do the same but in one click.\nBest Practice\rWe strongly recommend running code from your script rather than directly from the console because a script will be easier to proof-read and troubleshoot.\nScripts provide a guide to what you want to enter to R and saves you from having to type out code over and over again.\rYou can create a new script under File -\u0026gt; New file -\u0026gt; Script (Ctrl+Shift+N) or click the white square with a green and white plus sign in the top left corner.\nRun your code from scripts, rather than the console. It is easier to spot and fix mistakes!\nActivity\rOpen your first script. Save it with a meaningful name.\nThe numbers along the left hand side of the script refer to line numbers. It helps when referring to a specific function or code snippet to others.\nLet’s add your first functions to your R script!\nLoading packages\rOne of the first things you may need in an R script is to load the packages you will use.\rSome base R packages are automatically loaded.\nTo use a package in R, you need to call it from R’s library. The function to load a package is:\nlibrary(\u0026lt;insert name of package\u0026gt;) # loads a package\rA package we will see later in the module is MASS which contains datasets you can use at home to practice the statistical tests covered in the lectures. MASS is a base R package but is not automatically loaded.\nRunning data() or data(package = .packages(all.available = TRUE)) will show the list of available built-in datasets. Some of these will be relevant to biological sciences, others are more general.\nOnce a package is loaded, entering the name of a dataset will display the entire dataset.\nActivity\rType library(MASS) into your script.\nPut your cursor on the line you want to run then press Ctrl+Enter or click Run\rYou can run multiple lines by highlighting the relevant lines\rYou do not need to highlight a single line to run it\rYou can run the entire script from beginning to end using the shortcut Ctrl+Alt+R\rShortcuts will make your life easier.\nRun the library(MASS) code. You have just run your first line of code.\nR will load the package MASS in the background. You can check this in RStudio under the Packages tab where there will now be a tick in the box next to MASS.\nTry calling one of the MASS datasets, like Rabbit, to view the entire dataset. To call a dataset, type in the name and run the code.\nYou can see the descriptions of each dataset by calling help(\u0026lt;dataset name\u0026gt;), e.g. help(Rabbit) will tell you it describes the blood pressure of rabbits before and after a drug treatment.\nActivity\rWe will use the package learnr later in the module.\nThis code will install the package but there is one mistake - fix the mistake and install the package: install.packages(learnr)\nInstall any other packages you are asked to. If it worked you should see something under the Tutorial tab (next to Files, Plots etc.) in RStudio.\nWhen you’ve installed the package, modify your code to install the package remotes. You now should have installed two packages.\nCommenting\rThe # (hash/pound) sign indicates comments. Anything after this symbol is not run in R. Commenting is for writing notes or telling the user what is going on.\n# this is a comment\rBest Practice\rComment often and in detail. Someone should be able to understand what you did and why. Including your future self.\rActivity\rAdd at least one comment to your R script describing what your script is doing.\n6. Working directories in R\rRemember directories? Typing out whole addresses starting from the hard drive is annoying. There is a short cut if we use working directories.\nThe working directory in R is the default directory R will look in first. We can then use directory addresses that are relative to this default address to call files.\nFor example, following this directory structure:\nC:\r-\u0026gt; documents\r-\u0026gt; file A -\u0026gt; folder 1 -\u0026gt; folder 2 -\u0026gt; file B\rIf the default directory is C:/, then the address for file B is C:/documents/folder1/folder2/fileB.\nBut if we set folder 1 as the working directory C:/documents/folder1/, then we can use the relative address for the file: folder2/fileB. This way we don’t have to type C:/documents/folder1/ every time.\nNavigating through directories using addresses can be confusing.\nAnother useful command is .. which tells the address to go up a directory. For example, if folder 2 was the working directory C:/documents/folder1/folder2 and we wanted to access file A, then we need to tell the computer to go up two directories. The relative address for the file is: ../../fileA which means that the computer is now looking in the documents folder. In contrast, the relative address for file B is even shorter, fileB, since folder 2 is already the working directory.\nRStudio has a default working directory.\nYou can see which working directory is the default directory in RStudio on the “Files” tab. The code to see the working directory in R is:\ngetwd() # GET Working Directory. There is nothing in the brackets\rDanger!\rWhen you run code directly from the Console, it will use the RStudio working directory. The working directory of a script by default is the directory the script is saved in - this may be different to the default working directory in the Files tab.\nThis may be one reason RStudio cannot find a file even with a “correct” relative address and why it’s important to know which working directory your computer is using.\nCarefully check that the file address is correct! And that you know where the file is on your computer!\nYou may need to change the working directory.\nChanging working directories\rYou can change the default RStudio directory under Tools -\u0026gt; Global Options but on a daily basis, changing the directory temporarily under Session -\u0026gt; Set working directory is more useful.\nSetting the working directory using the drop down menu.\nThe R function to change your working directory is:\nsetwd(\u0026quot;\u0026lt;insert directory address here\u0026gt;\u0026quot;) # SET Working Directory\rFor example, setwd(\"~/\") will change your working directory to your default directory (called Root in computing terms). It is good practice to keep similar files in the same folder. Otherwise you will need to specify the full address when you call a file that exists in another folder and it can get confusing if your files are all over the place.\nSetting the working directory using R code is better for writing scripts.\nExpectations\rWe expect you to know how to change working directories and tell R where to find files through relative addresses because that is how we import and export data to and from R.\nThink of directories and addresses like postal addresses - if your address is incorrect the postman (R) wouldn’t know where to go to pick up your parcel (file).\nIf R cannot find a file, check the address or the working directory is correct.\nRubber ducks\rProblem solving is an important skill. If your code is not working then it’s likely that you’ve made an error somewhere - and that’s OK! It’s all part of learning how to program and there’s no shame in not getting it right the first time. The first thing is to retrace your steps and identify whether you’ve missed a step or misspelled something.\nLearning to problem solve independently is not something you learn by reading or something you can be taught. It is a skill you have to learn by doing, which means having a go yourself before seeking help from others. Make a habit of asking for help as your last option because for the assessment you will need to use R and troubleshoot independently.\nIf you can’t identify the problem, then you need to be able to describe your problem to another person. The other person needs enough information and context to understand what you hope to achieve and what you’ve done so far to suggest a solution. Sometimes the solution isn’t immediately obvious so it may require some trial and error - and that’s all part of the process too! It’s really hard to fix someone’s coding problem without context.\nTalking through your problem is called rubber ducking in computer science - talking through your thought process to a rubber duck may help you realise the solution.\nExpectations\rWe expect you to be able to troubleshoot common programming problems on your own. Working things out yourself is also an essential skill, generally.\n99% of all errors are incorrect addresses (working directory not set properly), missing characters, wrong quotation marks, or missing a step in the instructions. Typographic errors and spelling mistakes will be the first things we will check.\nWe don’t want you to treat R and coding as a black box (i.e. something you don’t understand how it works). Don’t expect to be given code that works, that defeats the point of learning programming – We want you to be able to write your own code and do data analysis in R independently by the end of this module for any scenario.\nFinding help\rAll R code comes with help files. You can access them from the “Help” tab in RStudio. If you want help on a specific function, then you can type in help(\u0026lt;insert function name here\u0026gt;). If you don’t know your exact query, you can search using ??\u0026lt;insert term here\u0026gt; - e.g. ??mean.\nThe Internet is also great for R help. Websites like StackExchange are help forums for programming. Most likely your question has already been answered on StackExchange. The trick is knowing what to type into Google - meaning you need to know what outcome you want.\nFinal checklist\rYou should now have R and RStudio installed on your computer, installed some packages and updated your R settings if needed.\nYou are now ready for the rest of the module.\nDo you know how to navigate computer directories and addresses to find files or folders?\rDo you know how to set up scripts and directories in RStudio?\rWe’ve introduced some basic computing concepts: scripts, directories and commenting.\nWe’ve also introduced some fundamental R functions. You will need these in the future:\ninstall.packages\rlibrary\rgetwd\rsetwd\r","date":1660608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660608000,"objectID":"726db3bf28a75be1a7b405a79c489e25","permalink":"https://jacintak.github.io/teaching/introR/","publishdate":"2022-08-16T00:00:00Z","relpermalink":"/teaching/introR/","section":"teaching","summary":"Download and set up R and RStudio","tags":["teaching","R stats","code"],"title":"Setup R","type":"teaching"},{"authors":null,"categories":null,"content":"\rThis is a static version of the learnr tutorials for the Statistical Modelling practicals.\nPractical outline\rWelcome to Part 2! We will continue where we left off in Part 1 and progress through a workflow of conducting an experiment and analysing data.\nWe will explore two types of statistical modelling – using linear regression (empiric model) to parameterise our function response model (theoretic model).\nLearning objectives:\nKnow how to parameterised a linear regression in R with biological data\rKnow how to use linear models to predict new values\rKnow how to interpret linear regression output to test hypotheses\rKnow what to include in a results paragraph\rKnow how to make a graph in R with appropriate labels and caption\rWe have covered all of these concepts in the lectures or previous practicals. This prac will reinforce and revise these concepts and allows you to practice them under supervision of the demonstrators.\nI recommend taking your time and making sure you understand the content and are up to date with the module. This practical synthesises and integrates everything up to this point. There is no new material here.\rThere are two activities:\nA step by step recap of the lecture content and the practicalities of linear regression in R using an example dataset. Work through the coding exercises and answer the questions in order.\nTesting your understanding of the content by applying what you’ve learnt from the example to the data you collected last practical. This is the assessment.\nDon’t forget to answer the CA questions in a separate link. There are prompts that will tell you when to answer the questions. The demonstrators aren’t allowed to help you answer the CA directly but they can help you understand the theory parts so that you can answer the CA.\nHow to problem solve\rWe don’t expect you to be expert programmers but we do expect you to be able and willing to figure things out yourself (problem solving).\nIn programming, it’s really easy to make mistakes that breaks code. That is no means a reflection on you or your ability to code/learn.\nIf your code is not working take a moment to breathe. Then check for common, minor errors such as:\nSpelling mistakes\rWrong dataset name\rWrong variable (column) name\rMissing or wrong quotation mark\rMissing bracket\rInconsistent cases (e.g. Uppercase)\rMissed a step\rInvalid syntax (e.g. spaces)\rDuplicates of the same function with multiple errors – keep your scripts tidy!\rThese reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.\nYou can and should easily fix the above mistakes yourself!\nWe want you to know how to work problems out independently.\nBe clear about your problem\rProgramming requires a clear idea of what you want to achieve.\nYou need to do the thinking for the computer.\nOne of the hardest thing about troubleshooting someone else’s programming is that we cannot read your mind. When describing your problem, be comprehensive \u0026amp; give context. The more information you provide, the more information and context people have to help you.\nIf something isn’t working, can you articulate what you aim to do? What you expect to see if it worked? What you did leading up to this point? And what you think is happening to cause the error?\nThis is called the rubber duck method – like you are explaining your problems to a rubber duck. Notice, it’s exactly like the Scientific Method.\nIf there’s an error message, read it. Error messages are the computer telling you what’s wrong. Try googling the entire error message to see what other people have said. The entire error message is necessary for context, not just the last bit.\nSome general advice\rProgramming is trial and error. You shouldn’t expect to get it on the first try. And that’s OK – it’s part of the process.\nYou’ll often find it’s because you haven’t proof-read your code carefully enough for the mistakes above.\nPractice the basics before you progress to harder stuff or answer the assessment. Take good notes.\nLearning is about the process and about your growth, not the end result or your grade. Apply your knowledge to new scenarios.\nDon’t over-think it.\nTwo pitfalls to avoid\rDanger!\rDo not take the code here for granted.\nThe point of learning computation and programming is to not treat software like a black box where you cannot see inside and you don’t know where the output came from or what they mean.\nEvery dataset is different, so code you copy from someone else is not guaranteed to work with your dataset first time. For example, you may be using different column names.\nThink about what the code means and what it’s doing so you can make sure it’s appropriate and correct.\nDo not skip questions or exercises to answer your assessment on the fly.\nAll the activities and quiz questions will give you the knowledge to do the assessment – do these first. Don’t do the assessment questions at the same time as the practical exercises.\nDon’t stress yourself by attempting the assessment unprepared. Demonstrators are not allowed to help you answer the assessment questions.\nFunctional responses recap\rRemember, in the previous practical we asked the question “How does prey density and foraging strategy affect the number of prey captured in a predator-prey scenario?”.\nWe have three variables of interest in our experimental design:\nprey_captured: The response variable, continuous numeric variable\rprey_density: The first predictor variable, continuous numeric variable\rforaging_strategy: The second predictor variable, categorical variable with two sub-groups: no_lid and yes_lid for whether the jar had a lid or not.\rWe derived a mathematical expression of a predator-prey interaction (Type II model) called a functional response by turning our assumptions about a predator prey interaction into mathematical equations. You can refresh your memory from vignette(\"functional_responses\").\nThe Type II model looks like this:\n(#fig:funct_resp)Type II functional response of an predator-prey response\rThe equation of the model is:\n\\[H_a=\\ \\frac{a\\times H\\times T_{total}}{1+a\\times H\\times T_h}\\]\nThe model is not linear.\nThe aim of the practical was to find the unknown values of search rate, \\(a\\) and handling time, \\(T_h\\) and we hypothesised that these values will change depending on the type of jar used (foraging strategy).\nThe process of estimating the value of unknown parameters in a statistical model is called parameterising.\nNon-linear methods of parameterising these equations are beyond the scope of this module. But we can use algebra to make this equation linear and therefore we can use linear regression to get an estimate for the unknown parameters (see vignette(\"TypeII_models\") to refresh your memory).\nWe then get a model:\n\\[\\frac{1}{H_a}=\\ \\frac{1}{a}\\times\\frac{1}{H\\times T_{total}}+\\frac{T_h}{T_{total}}\\]\rThis model has the form \\(Y = \\beta_0 + \\beta_1 X\\) of a general linear model.\nThe graph looks like this:\rFigure 1: Linearised type II functional response of an predator-prey response\rNow we can fit a linear model and find the values of search rate (\\(a\\)) and handling time (\\(T_h\\)) from the slope and intercept of the linear regression.\n\\[ \\beta_1 = \\frac{1}{a}\\]\rand\r\\[ \\beta_0 = \\frac{T_h}{T_{total}}\\]\nWhich linear model?\rSimple or multiple regression?\rIn the lectures, we saw that the structure of the appropriate linear model to fit depends on the characteristics of the data, the experimental design and the hypothesis. We looked at cases with one predictor variable (simple regression) and with two predictor variables (multiple regression).\nIf we did a simple linear regression with one predictor variable, we would have ignored foraging strategy meaning we have pooled observations for foraging strategy.\nBy pooling foraging strategy, any variation in the data generated by foraging strategy is unaccounted for. Unaccounted variation may increase our chance of making a Type I or II error because foraging strategy confounds the effect of prey density.\nPhrased differently, the effect of prey density on numbers of prey captured is masked by the effect of foraging strategy. Thus, in this case, using a simple linear regression on a dataset with two or more predictor variables is not the best course of action for explaining as much variation in the data as we can.\nIf we want to see whether using a lid has an effect on our predator-prey interaction response, we need to include the variable in our linear model as well as our original predictor variable, prey density. We need a multiple linear regression model.\nPlan ahead!\rConfounding variation from doing a test that is too simple for the experimental design can drastically change the conclusion of the analysis! See Simpson’s paradox.\nAdditive or multiplicative model?\rThere are also two types of multiple regression: additive and interactive/multiplicative.\nWe could fit an interactive model to our data if we were more interested in finding a mathematical description of our data that explains the most variation in prey captured using the simplest model possible (model parsimony). So using multiplicative models are instead a statistical representation of the data, rather than a theoretic model.\nBoth are types of statistical models and valid in their own right but in this case an interaction doesn’t match the goals of constructing a theoretic model of predator-prey interactions from fundamental observations.\nForaging strategy determines the amount of prey captured but does not affect the number of prey present. And prey density determines the number of prey captured but does not influence the foraging strategy. This is based on our assumptions about the predator-prey interaction – the predator does not change foraging behaviour.\nIf the predator changed their foraging behaviour then there would be an interaction between the two predictor variables. This could be realistic. For example, animal predators might use different foraging strategies under different prey densities or they may switch to another strategy when prey density reaches a certain threshold. In fact, this is a Type III functional response.\nHypotheses\rInfo!\rThe wording of the hypothesis will dictate the appropriate linear model, statistical test, and our expectations about the graph.\nFollowing the Type II model, the number of prey captured will increase with prey density until there are too many prey for a predator to handle and the number of prey captured will plateau.\nFollowing the linearised Type II model, we expect a positive relationship between the two predictors.\nWe also had predictions about how foraging strategy affected the parameters of the Type II model – handling time and search rate (or attack rate).\nOur hypotheses are:\nH0: The number of prey captured increases with prey density but does not differ between foraging strategies\nH1: The number of prey captured increases with prey density and is higher with the more efficient foraging strategies (shorter handling time) but the search rate does not vary\nDiscussion\rHow would your data look like to support these hypotheses?\nAdditive linear models\rAs we saw in the lectures, the theory behind all linear regressions is the same no matter how many predictor variables or interactions you have.\nVariance in \\(Y\\) is partitioned sequentially and in alphabetical order (unless otherwise asked to) in R\nTo practice fitting linear regression in R, we will use the dataset crabs – see help(crabs) for more information. You can also check the data using str(crabs). This dataset is provided in R in the package MASS. I have already loaded the data for you within the tutorial so you don’t need to do it but you will need to load it via library(MASS) if you want to try code in a script. What happens in the interactive tutorial is independent of R’s Environment.\nThere are three variables we will look at (carapace is the zoological term for shell):\nCL: Carapace length (mm) – our response variable\rCW: Carapace width (mm) – our predictor variable\rsp: Colour morph (B or O for blue or orange crabs)\rInfo!\rThe code we use here can be applied to any dataset. You can try it in your own time. For example, airquality has environmental data or the Penguins dataset about penguins can be downloaded as a package.\rWe can ask the question “Does the relationship between shell length and width differ between colour morphs?”. We can also phrase this as “Does the relationship between shell length and width depend on the colour of the crab?”.\nCompared to a simple linear regression with one predictor variable (e.g. CW), this is a more complex question needing a more complex experimental design (two predictor variables) and thus a more complex statistical analysis.\nInfo!\rMultiple regression allows us to ask “Does including information about our predictor variables improve our ability to detect/understand trends in our response variable?”\r“Bigger” crabs are expected to be larger in width and length so we expect a positive relationship between these variables. Whether this relationship is also dependent on the colour of the crab is what we can find out!\nWe don’t hypothesise colour morphs to influence carapace width, i.e. crabs don’t change colour as they get bigger, so an additive model fits with our understanding of the biological system (assuming the growth rate of both colours are the same).\nIt always helps to see a graph of the data:\rThe model\rMathematically, an additive model is:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\]\nWhere \\(X_1\\) is our first predictor variable and \\(X_2\\) is our second predictor variable.\n\\(\\beta\\) are the regression coefficients (also called terms) describing the variation in \\(Y\\) attributed to each source of variation. The additive model has three terms that are unknown to us: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\). R’s job is to find the values of these coefficients from empirical data.\n\\(\\varepsilon\\) is the random error (residual error) not accounted for by the model – it’s always part of the equation but it’s often ignored, much like \\(c\\) in integration.\nIn additive models, the effect of one predictor on the response variable is additive or independent of the other.\nThere is no interaction between the predictor variables colour or shell width (i.e. \\(\\beta_3 = 0\\)). Thus the two fitted lines have the same slope (\\(\\beta_1\\)) – hence they are sometimes called fixed slopes models. The model is described as a “reduced” model because it does not contains all possible terms – all predictor variables and all possible combinations of their interactions.\nInteractive models are considered “full” models if they contain all interactions in a fully crossed design.\nTo estimate the \\(\\beta\\) coefficients, the ordinary least squares regression splits the dataset into its predictor variables and fits a model to each component – this is called partial regression.\nThe goal is to assign as much variation in the data as possible to each predictor variable. This process requires a baseline variable that sets the contrast for assigning variation (i.e. what \\(\\beta_0\\) represents).\nDummy variables are used with a categorical predictor to set the baseline for the ordinary least squares regression. Since Blue is alphabetically before Orange, Blue is the baseline contrast used in the partial regression process and assigned the dummy variable 0 in crabs.\nThus for a blue crab, the dummy variable is 0:\n\\[CL = \\beta_0 + \\beta_1 CW + \\beta_2 sp \\times 0 + \\varepsilon\\]\rsimplifies to \\(CL = \\beta_0 + \\beta_1 CW\\)\nAn orange crab gets a dummy variable of 1, thus:\r\\[CL = \\beta_0 + \\beta_1 CW + \\beta_2 sp \\times 1 + \\varepsilon\\]\rbecomes \\(CL = (\\beta_0 + \\beta_2) + \\beta_1 CW\\)\nIn effect, we are fitting two lines to this data – one for each sub-category of colour. The technical term for each of these lines is a partial regression line. We can generalise these regression lines as:\n\\[ shell \\space length = \\beta_{0_{colour}} + \\beta_{1} shell \\space width + \\varepsilon\\]\rNow the intercept parameter (\\(\\beta_{0_{colour}}\\)) specifies that it is dependent on the colour of the crab as we showed above:\r\\[\\beta_{0_{Blue}} = \\beta_0\\]\r\\[\\beta_{0_{Orange}} = \\beta_0 + \\beta_2\\]\nCharacters or factors?\rCategorical data is are called factors and the sub-groups are called levels in R\nBecause R needs to attribute variation to every sub-group, the order of the subgroups is relevant. R shows you the difference between levels of a factor (\\(\\beta\\)). The default order is in alphabetical order of the levels.\nThis is relevant to the experimental design too. It may be helpful to think of the first level of a treatment and the first intercept (\\(\\beta_0\\)) and slope estimate (\\(\\beta_1\\)) as the control of your experiment because R uses these coefficients as the baseline for hypothesis tests.\nFor example, the jar without a lid treatment is coded as no_lid and the jar with a lid treatment is coded as yes_lid. We can also think of jar with a lid as our experimental control (a less efficient predator) and the jar without a lid as our experimental treatment (a more efficient predator). Our “control” group (yes_lid) is alphabetically after no_lid, so we have the reverse scenario.\nRemember!\rThe order will affect how R parameterises our model. And thus will change the interpretation of the output! But not always the conclusion.\nBy default, foraging strategy is classified as a character vector because it is a field with strings (letters). Characters do not identify sub-groups thus there is no structure to this variable – they are simply a vector of strings.\nInfo!\rBoth characters and factors are ways R stores categorical data, the difference being that factors have a specific order.\rTo fit a linear regression, R converts the foraging_strategy variable from a character vector to a factor and the order of levels is assigned alphabetically. R will use no_lid as the baseline contrast for \\(\\beta_0\\) and \\(\\beta_1\\). We just need to keep the order in mind when parameterising.\nWe will not do anything about this for now – in this case, it doesn’t change our conclusions.\nInfo!\rIf you needed to change the order of the sub-groups you will need to change the variable to a factor (as.factor) and define the order of levels using levels:\ndata$var \u0026lt;- as.factor(data$var, levels = c(\"level1\", \"level2\").\nIf we did this, then the regression uses yes_lid as the baseline (\\(\\beta_0\\) and \\(\\beta_1\\)) and the dummy variables are reversed but the parameterised model is the same.\rFitting the model in R\rAn additive linear regression in R with two predictor variables follows the general formula:\nlm(Y ~ X1 + X2, data)\rWhere:\nX1 \u0026amp; X2 are the two predictor variables\r+ indicates the relationship between the two predictors: a plus sign for an additive relationship\rlm stands for linear model\rY is our response variable\rdata is the name of our dataset\r~ indicates a relationship between our response and predictor variables\rIn crabs, our response variable is CL and our predictor variables are CW and sp.\nDid you get some output when you ran the model?\nIt should tell us two things:\nCall is the formula used. It should be the same as the linear model code\rCoefficients are the estimated coefficients of the model. There should be three coefficients called (Intercept), CW \u0026amp; spO. From left to right they are: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\).\rWe can already substitute the estimated \\(\\beta\\) coefficients into the full expression of the linear model \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) for the crabs dataset as:\n\\[CL = -0.67 + 0.88 CW + 1.09 sp + \\varepsilon\\]\nBut let’s look at the coefficients in more detail starting with the first two coefficients. The interpretation for these coefficients is the same as simple linear regression. There’s the intercept (Intercept) (\\(\\beta_0\\)) and there’s the slope CW (\\(\\beta_1\\)).\nInfo!\rCoefficients are named based on what variation in \\(Y\\) they contribute to. So the slope coefficient is called CW because it how much shell length changes with shell width.\rRemember we are expecting two lines in our model – one for blue crabs and one for orange crabs.\nParameterising our partial regression model\rBecause the first two coefficients are for blue crabs, we already have our equation for blue crabs with \\(\\beta_0\\) and \\(\\beta_1\\).\nParametrised equation for blue crabs:\nblue crab shell length = -0.7 + 0.9 shell width\nHalfway there! Now for the orange crabs!\nSince we expect the slope of the regression for orange crabs is the same as blue crabs, we already know the value of \\(\\beta_{1}\\) is 0.9. But we need to manually calculate the intercept for orange crabs.\nThe estimate for spO is \\(\\beta_2\\), which is the difference in the intercept between orange crabs and blue crabs. Or the additional variation in shell length attributed to colour, while pooling for shell width.\nTo calculate the intercept for orange crabs we need to add the estimated coefficient of the intercept for blue crabs (\\(\\beta_0\\)) with the difference (\\(\\beta_2\\)): \\(\\beta_{0_{Orange}} = \\beta_0 + \\beta_2\\).\nNow that you know how to parameterise the linear regression for orange crabs:\n(Intercept) CW spO -0.7 0.9 1.1 You should be able to parameterise the linear model for orange crabs.\nInfo!\rR shows the difference between parameter estimates so you need to calculate the correct values. They are the partial regression coefficients that show the change in the response variable with one predictor variable while holding all other predictor variables constant.\nFor example, using the mean value of orange crabs to estimate coefficients of blue crabs and vice versa, because the mean value of each of these groups represents the null hypothesis.\nIn other words, if we were to accept the null hypothesis that there is no relationship between shell width and colour on shell length, then the slope of the line should be 0 and the intercept of the line should be the sample mean of shell length (ignoring shell width and colours).\rPredicting new values\rOne application of statistical models is to make predictions about outcomes under new conditions\nWe can calculate the value of the response variable from any given value of the predictor variable.\nFor example, we can use the parametrised equation of our model \\(CL = -0.7 + 0.9 CW + 1.1 sp\\) and the dummy variables 0 for Blue crabs or 1 for Orange crabs to work out the length of a crab for any value of shell width.\nIf a blue crab is 10 mm wide, what is its predicted shell length?\nWe are told the value of shell width (10 mm)\rWe know the parameterised linear model:\r\\[CL = -0.7 + 0.9 CW + 1.1 sp\\]\nWe can substitute the value of 10 for CW:\r\\[CL = -0.7 + 0.9 \\times 10 + 1.1 sp\\]\nWe know the dummy variable for blue crabs is 0 because it is the baseline level (assigned alphabetically) so we can substitute that into sp:\r\\[CL = -0.7 + 9 + 1.1 \\times 0\\]\nand solve for length:\r\\[CL = -0.7 + 9\\]\n\\[CL = 8.3 mm\\]\nOr we can use our two partial regression models to predict the shell length of blue or orange crabs.\nBlue crab shell length = -0.7 + 0.9 shell width\nOrange crab shell length = 0.4 + 0.9 shell width\nYou should now be able to:\nUse the model to predict the carapace length of a blue or orange crab for a given carapace width.\rUse the model to calculate the difference in carapace length between blue and orange crabs for a given carapace width.\rEvaluating hypotheses\rRemember that statistical models may represent hypotheses\nHere are the hypotheses:\nH0: Carapace length increases with carapace width but does not differ between colour morphs\nH1: Carapace length increases with carapace width and differs between colour morphs but the rate of size increases does not vary between colours\nWe can test these hypotheses using the linear regression. It’s important to understand these hypotheses graphically. Look at the following graphs:\nQuestion\nWhich of the graphs above represents the null hypothesis about crab size?\rWhich of the graphs above represents the alternative hypothesis about crab size?\rNotice how the slopes of all the lines are the same. We expected this based on the additive model maths.\nWe want to know if the lines have different intercepts to each other. Phrased differently, we need to test whether the difference in the intercept is different to 0.\nWe could have more hypotheses that also test whether there is a positive relationship in the data or not (i.e. is the slope = 0?) but the specific wording of our hypotheses above do not do so.\nWe don’t just need to know the difference, we need to statistically test this; whether an estimated/observed value of a sample is significantly different to a known population value. You’ve learnt about this kind of test already.\nQuestion\nCan you remember which coefficient in the additive model will test this hypothesis?\rWhich of the statistical tests you’ve already learnt in this module would test whether our observed slope is significantly different to 0?\rRemember!\rStatistical significance is not the same thing as biological significance.\nA relationship between two purely randomly generated numbers can be statistically significant but have no biological meaning. A biologically meaningful relationship can be statistically non-significant.\rR automatically tests the following hypotheses as part of lm:\nH0: The difference is equal to 0\nH1: The difference is not equal to 0\nTo see more information about our linear regression we need to ask to see the summary of our linear regression by placing our lm function within summary(lm()).\nWhen you run summary you get a lot of information. Let’s break it down from top to bottom:\nCall is the formula used to do the regression\rResiduals are the residuals of the ordinary least squares regression\rCoefficients are the estimated coefficients we saw earlier plus the standard error of these estimates, a t-value from a one sample t-test testing whether the estimated coefficient is significantly different to 0 and the P value of this t-test\rSome additional information about the regression at the bottom which we can ignore for now\rFrom the output you should be able to:\nIdentify the correct t-statistic\rMake an inference about the one-sample t-test\rAccept or reject your null hypothesis\rInfo!\rThe one sample t-test is done on each coefficient. The hypotheses are the same. The test can also be phrased as “Does the coefficient explain a significant amount of additional variation in \\(Y\\)?”.\nFor example, if we ran a multiplicative model with a \\(\\beta_3\\) term, then the t-test will determine whether there is an interaction or not. If the t-test was not significant, then the interaction does not explain more variation and an additive model without an interaction is more appropriate. This is an example of model selection.\rPutting results into sentences\rStatistical analyses need to be interpreted into full sentences, within a results section\nAlways place your statistical analysis into the wider context of your hypotheses and aims. Demonstrate your understanding by justifying your choices and interpreting the output.\nANOVA tables should be nicely formatted in a proper table with the correct column names. Don’t copy directly from R as is.\nWhen reporting summary statistics, describe an average with a measure of spread. You need to give a sense of the distribution of points so means on their own are not that informative. Include the range or standard error or standard deviation. E.g. “Mean carapace length was \\(32.1 \\pm 0.5\\) (mean \\(\\pm\\) standard error)”.\nBe comprehensive!\rR output should not be presented as a figure or screenshots. Do not copy and paste directly into a report – this output is meaningless to a reader because they do not know your data or your workflow like you do.\nP values should never be reported on their own – they are also meaningless without the test statistic and degrees of freedom used to calculate them. P value of what?\nA results sentence needs at a minimum:\nThe main result\rThe name of the statistical test\rThe test statistic and degrees of freedom\rDf can be written as a subscript to the test statistic (e.g. \\(t_{14}\\)) or reported as df = 14\rF statistic need the degrees of freedom for the within \u0026amp; among error. E.g. \\(F_{1,25}\\)\rThe P value\rReally small or large P values can be summarised. E.g. P \u0026lt; 0.001. Don’t write out many decimal places\rReference to any relevant figures or tables\rAll the above information is given to you in summary or the ANOVA table. The degrees of freedom of a linear regression are found at the bottom of summary (Residual standard error).\nA sentence does not need to include significant in the wording. Statistical significance or not is already implied by the wording of the sentence and the inclusion of the P value. The term can also be ambiguous in meaning.\nDanger!\rThere are many ways that P values can be biased or manipulated to give small or large values.\nPeople’s obsession over getting significant P values has driven a rise of unethical statistical practices (called P hacking or P fishing) which we do not want you as scientists in training to fall into the habit of.\nThe point of statistical testing is to understand trends in data. Not to get a significant P value. Null or non-significant results are valid results.\rCheck any published scientific paper to see how they’ve reported their results. Or any guide to scientific writing.\nPractice: Exploring a Type II functional response\rRemember we had some unknown parameters in our predator-prey functional response model? The search rate, \\(a\\), and the handling time, \\(T_h\\). We need to find the value of those parameters from the slope and the intercept of our linearised Type II dataset – exactly like we did for the crabs example. We can use the lm function on our predator-prey model data to parameterise our model.\nBefore we can do our lm in R we need to import the data and linearise the data.\nStep 1: Importing data\nFirst, we need to import the data in to R. This should be familiar to you from before.\nWarning!\rDon’t use the code chunks in this tutorial to import data, it won’t work. Make your own script (File -\u0026gt; New Script, or Ctrl(Cmd) + Shift + N).\nDon’t write your code directly in the console either – you won’t have a good record of what you’ve done and it is harder to troubleshoot. You may want to use the code here to help you with your final report or future projects.\nIf you copy the code in this tutorial, make sure to modify it as appropriate for your data.\rThe examples here use mathematical notation as the variable names here but the class dataset uses the names from Part 1. You should know which notation matches with what variable description.\nThe class dataset is provided as a comma separated values file (.csv). The function to import a csv file is read.csv:\nclass_data \u0026lt;- read.csv(\u0026quot;directory/folder/class_data.csv\u0026quot;)\rThis imports the spreadsheet into an R object called class_data but you can use whatever name you want. You need to replace the file address within the quotation marks with where ever you saved your file on your computer. File housekeeping is important!\nRemember!\rMake sure you adapt code to match your dataset. Don’t take code for granted!\nStep 2: Linearising data\nWe need to make our data linear to fit a linear model to it! Let’s first take the inverse of our response variable \\(H_a\\) to get \\(\\frac{1}{H_a}\\)\nclass_data$Ha.1 \u0026lt;- 1/class_data$Ha\rThis line of code calculates the inverse of Ha, that is, 1 divided (/) by the number of prey captured (Ha), then saves that number to a new column in our dataset called Ha.1. Note the use of no spaces.\nYour column names can be whatever is meaningful to you. E.g. you don’t have to call your new column Ha.1 but remember what you called it.\nWhen manipulating data like this, it’s best practice to add new columns to the data, rather than overwrite the original column. That way, if you make a mistake it’s easier to see what went wrong and you won’t have to start from the beginning!\nRemember!\rdata$column is the general structure to select a column in R. The dataset name and the column names in your code must exactly match your data name and columns. R is case sensitive.\nMake sure you’re not making basic errors like using the wrong column name. Remember, problem solving is fundamental to coding.\nNow let’s linearise the predictor variable (H) to get \\(\\frac{1}{H\\times T_{total}}\\).\nclass_data$HT.1 \u0026lt;- 1/(class_data$H * class_data$T_total)\rBecause we use a value of 1 minute for \\(T_{total}\\), we are essentially dividing 1 by only prey density (\\(H\\)). Told you a value of 1 would make our maths easier!\nWatch out!\rSince we have done some division, it’s a good time to check for any undefined values in case we divided by 0.\nIn R undefined values are denoted as infinities (Inf). You can check how many infinities there are using table(is.infinite(class_data$Ha.1)) – this will check whether each cell has undefined values (is.infinite) and give you logical TRUE/FALSE output, then tabulate the logic statements to count the number of TRUE/FALSE occurrences in the column Ha.1.\nWe can replace the infinities with zeroes using class_data$Ha.1 \u0026lt;- ifelse(class_data$Ha.1 == Inf, 0, class_data$Ha.1)\nYou might recognise the if else statement and understand what’s happening from previous lectures: if there is an undefined value, then replace that value with 0, else leave the value as is. We don’t discard these observations because a value of 0 has biological meaning.\rWe now have all the correct columns to parameterise our functional response.\nStep 3: Construct an additive linear model for your data\nIf lm is the function to do a linear regression, Ha.1 is the name of our response variable, HT.1 is the name of the first predictor variable, foraging_strategy is the name of the second predictor variable, and the name of our dataset is class_data, you should be able to write the code for the linearised type II functional response (or replacing the respective components with the column name and dataset names you are using).\nHint\rThe workflow is exactly as we did for crabs – so you’ve practised it already! Remember that R partitions variation sequentially and in alphabetical order for factors.\nIf you didn’t understand the previous material or you got some questions incorrect or you skipped bits, I recommend finishing them before progressing. Demonstrators cannot help you answer the assessment.\nInfo!\rLinear regressions are always done on the entire data, not on averages. i.e. you wouldn’t use the average of your replicates for each treatment for the underlying data. Data must be raw (unprocessed).\nTo fit a line to data, ordinary least squares regression depends on quantifying variation of observations around the mean (think back to how sampled data from a population is distributed). Averaging data removes that variation and thus there is less information for R to use (fewer degrees of freedom).\rWe won’t plot our regression lines just yet but you should be able to interpret the R output as a graph by looking at the coefficient values. We will construct a graph later!\nStep 4: Parameterising \\(a\\) and \\(T_h\\)\nThe final step is to get our unknown values of \\(a\\) and \\(T_h\\) for each type of foraging strategy from our linear regression. Since we know our partial regression takes the form \\(Y = \\beta_{0_{foraging \\space strategy}} + \\beta_1 X_1\\) with separate intercepts for each sub-group of foraging_strategy and the same slope for both groups, and we know the linearised type II model is \\(\\frac{1}{H_a}=\\ \\frac{1}{a}\\times\\frac{1}{H\\times T_{total}}+\\frac{T_h}{T_{total}}\\), then you should have all the information to parameterise the type II model and derive values for \\(a\\) and \\(T_h\\) from the linear regression output for both foraging strategies.\nIt’s time to answer the CA!\rYou have all the information you need to answer the CA.\nMake sure you’ve completed all the exercise up to this point.\nRemember to save and submit your answers.\nThat’s the end of the assessment but not the end of the practical.\nKeep going. You’re doing great!\rVisualising data\rVisualising data is very important for understanding our data and for communicating our data to others. For example, in a written report.\nplot is the general plot function. Box-plots (boxplot) and bar plots (barplot) have their own plotting commands. You can add error bars using arrows.\nTheory\rThe general formula to plot a scatter graph is plot(response ~ predictor, data)\nIf our data is categorised (e.g. sp in crabs has B or O), then we need to plot our points for each group separately. We can do this by calling a plot with no points using plot(response ~ predictor, data, type = \"n\") where \"n\" tells R not to plot anything. Then we add points manually with points(response ~ predictor, data[data$group == \"subsetA\",]) for each sub-category.\nWe need to subset our data (like we did in previous pracs) so that R only plots the sub-categories of data.\nUsing the crabs dataset, plot crab shell length (response) against crab shell width (predictor) for only blue crabs.\nIf you are correct, your graph should match the one below:\rNow add the orange crabs to plot all the data – You have to start the code from the beginning.\nUh oh! We cannot distinguish between the colours of crabs! Time to learn more R to add more features to the graph.\nColours\rColour is important when presenting data. Are the colours meaningful? Are they necessary? Can they be clearly distinguished? Are they appropriate for screens, for printing, or accessible for colour-blind people?\nInfo!\rRed-Green colour blindness is the most common form of colour blindness. A simple guideline is to avoid using red and green together where ever possible. Blue and orange are better contrasting colours (that’s why they are common colour schemes for movie posters).\rColour in R is defined by col. So in a graph if I wanted to change the colour of the points from black (default) to red then I can either call col = \"red\" or col = 2 as an argument within the points() function, because red is the second colour in the default R colour palette (black is 1). There are lots of colours to choose from (Google it for a full list). R also accepts hexidecimal RGB colour codes for custom colours (e.g. black is #000000).\nChange the colour of the points to their relevant colour (blue or orange)\nShapes\rShapes are important because it makes the points stand out. It can also be used to distinguish between groups of data on the same graph. Sometimes it’s necessary to change colour and shape to make it easier to distinguish between groups – redundancy is acceptable and encouraged in graphical design.\nThe shape of the points are coded pch = \u0026lt;number\u0026gt; as an argument within plot() or points(). There are lots of options designated a number from 1 to 25. (Google “R pch” for the full list). 1 is the default open circle. A filled circle is 16.\nChange the shape of the points from open to filled circles.\nYou can combine all the code above to change the colour, shape and axes labels.\nThis graph is fine but if we were to put this in a professional scientific paper or report there are a few missing elements and we may want to customise the aesthetics for a prettier graph.\nAxes labels\rThe R code to change the labels on a graph are xlab and ylab for the x axis and y axis, respectively. These are called within the plot() function like:\nplot(response ~ predictor, data, xlab = \u0026quot;label\u0026quot;, ylab = \u0026quot;label\u0026quot;)\rChange the x axis label of our crabs plot to “Width” instead.\nIn practice, we want our figures to be informative and be understandable independently of the main text. This means having informative labels and including units.\nThere are two mistakes in the following code to change the axis labels to “Width” or “Length” AND include units (mm) in both the x and y axis. Fix it and run the code\nRegression lines\rOnce we’ve done the hard work to do a linear regression, it’s nice to add it to our graph so we can see how it fits to the data. A linear regression is always a linear line (straight line) so we can use the code to plot a straight line.\nThe formula to plot a straight line is abline(intercept, slope) because it plots a line from a to b. The intercept is the first value, the slope is the second value. You need to plot the data first before adding additional lines.\nHere are the coefficients for a simple linear model for the crabs showing the intercept and slope respectively:\n(Intercept) CW -0.7 0.9 Complete the abline() formula to plot our regression line then press run.\nThe same code is used to plot either regression line for a multiplicative regression – you just need to use the slope and intercept values you want!\nYou can also change the colour, type and width of the line as arguments within abline().\nlwd = \u0026lt;number\u0026gt; is line width. The default is 1. Values greater than 1 are a thicker line.\rlty = \u0026lt;number\u0026gt; is line type. The default solid line is 1. Different types of dashed or dotted lines are numbers 2 to 6.\rA final graphing test\rLet’s integrate everything we’ve learnt today and plot the graph of our additive linear model with the correct regression lines.\nFigure 2: A complete graph for a scientific report\rHere are the coefficients for the additive linear model:\n(Intercept) CW spO -0.7 0.9 1.1 Change the basic crabs graph to match the graph above – this graph is suitable for a scientific report\nOther variables in plot\rBorders\nA border around the plot is plotted by default. This is dictated by the default bty = \"o\". You can remove the border entirely with bty = \"n\" and you can plot only the bottom and right border (axes) with bty = \"l\" – that’s a lowercase L, not a number 1. Borders also apply to legends.\nLegends\nFigure legends can be added using legend. Check out the help file for details because you need to set:\nWhere to put the legend\rThe text of the legend\rThe colours\rThe lines or points used\rTitle\nFigure titles can be set using main = \"\u0026lt;title\u0026gt;\". There are other ways of captioning figures too, like subtitles.\nLines\ntype tells R what kind of plot you want. R plots points (type = \"p\") by default but can also plot lines (type = \"l\"), [lowercase L]. You can plot a combination with type = \"b.\nlines is the equivalent to points for plotting individual lines.\nCaptions\rThe last thing every scientific graph needs is a caption. The caption needs to describe the graph and explain every aspect of the graph to the reader independently of the main text.\nWhat is the overall relationship?\rWhat is on the x axis? What is on the y axis?\rWhat is the sample size?\rWhat do the colours, lines or points represent?\rOften a single sentence saying “Figure 1. A graph of the response against the predictor” is not enough information in a professional report.\nSaving your graph\rOne way to save your graph is to click “Export” in the Plots window.\nThe other way is to use code:\npng(\u0026quot;figure.png\u0026quot;)\rplot(Y~X, data)\rdev.off()\rThere are three steps:\npng tells R to prepare a png file where you want the graph to be saved. jpeg and pdf are also accepted formats.\rPlot the graph\rdev.off tells R you are finished plotting the graph and R will write the file to disk\rPractice: Graphing results with a caption\rDiscussion\rUse your new found knowledge of making graphs in R to make a plot of our results from our predator-prey interaction response including appropriate axes labels with units. The aesthetics are up to you. Write a caption as well.\rYou can show your figure to your demonstrator for feedback.\nEnd\rThat’s the end. Take a break. Stand up. Shake your limbs. Breathe.\nWe’ve covered a lot of practical skills that you can use in your final report, in the future and in your other modules. Just as you should build upon what you’ve learnt in previous modules in this one.\nHere’s a summary of what we’ve done:\nImported data into R\rManipulated data in R, including cleaning\rParameterised a linear regression in R with biological data\rMade predictions with linear models\rInterpreted linear regression output to test hypotheses\rWrote a results paragraph\rMade a graph in R with appropriate labels and caption\rSuccess!\rOver the course of the last practical and this one, we have done a crash course in the Scientific Method from formulating a hypothesis from a biological system (infection or predator/prey response) to evaluating hypotheses and every thing in between.\nWe have followed a workflow of data handling analysis that you can apply to any data.\r","date":1660608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660608000,"objectID":"93928065936d01289fe72ca04a86e940","permalink":"https://jacintak.github.io/teaching/StatsModelsPrac2/","publishdate":"2022-08-16T00:00:00Z","relpermalink":"/teaching/StatsModelsPrac2/","section":"teaching","summary":"Static handout of Part 2 of Statistical Modelling practical","tags":["teaching","R stats","code"],"title":"Statistical modelling Part 2","type":"teaching"},{"authors":null,"categories":null,"content":"\rIntroduction\rStatsModels is an R package of learnr tutorials comprising two practicals on statistical modelling made for BUY22S01 Statistics and Computation at Trinity College Dublin. The package is distributed via GitHub. Each practical is for a 3 hour slot with an estimated completion time of 2.5 hours.\nThe aim of the practicals is to introduce statistical modelling. It uses functional responses (i.e. Type II models, Holling’s disc equation) as background information.\nThere are two parts to the practical:\nIntroduction to statistical modelling\rConstructing a Type II functional response\rDesigning an experiment\rCollecting data\rData analysis using statistical models in R\rImporting data\rManipulating data\rConducting multiple linear regression (additive)\rVisualising data\rPart 1 requires students collect data by replicating Holling’s disc experiment with two predictor variables: prey density and jar type (lid/ no lid). This practical focuses on Type II models. Part 2 analyses the data using additive multiple linear regression. Simple linear regressions (with one predictor variable) or multiplicative regression are not covered because of time constraints.\nRemote version\rIt is possible to adapt this in-person practical to a remote version by getting students to collect data using a Scratch simulation of a Type II functional response. I have a Scratch model of one.\nSet up\rWe need to set up our computer to download the practical in R.\nFollow this checklist in order to make sure you are set up:\nHave you installed R version 4.0 or above? - if not, install/update R\rYou can check your R package version using R.Version()$version.string\rHave you installed RStudio version 1.0.136 or above? - if not install/update RStudio\rYou can check your RStudio version using RStudio.Version()$version\rHave you installed the following packages? - if not use install.packages(\"\u0026lt;name of package\u0026gt;\") to do so\rlearnr - needed to run the tutorials\rremotes - needed to install the tutorials\rIf everything works then you should see a Tutorials tab in one of your RStudio windows. There may already be tutorials listed there.\nInstalling the tutorials\rNow we need to install the tutorial. The tutorials are stored in a Package available on GitHub. You will only have to install the package once at the beginning.\nUse the following code:\nremotes::install_github(\u0026quot;jacintak/biostats\u0026quot;, dependencies = TRUE, build_vignettes = TRUE)\rIf you are asked to install any other packages, choose yes.\nIf you are asked to update any packages, press 1 for updating all of them.\nIf you are asked to install packages from source (i.e. in a pop-up window), press no.\nIf the package installed properly, you should automatically see the tutorials in the Tutorial tab.\nRunning a tutorial\rYou should be able to run a tutorial from the Tutorial tab when you open RStudio without needing to do anything.\nMake sure it’s a tutorial from the StatsModels package.\nIf you click run tutorial, the tutorial will show up in the tab. You can click the “Show in new window” icon to open it in another window. Press the “Home” icon to return to the Tutorials tab.\nIf that doesn’t work use this code and the tutorial will open in another window or your browser:\nlearnr::run_tutorial(\u0026quot;\u0026lt;insert name of the tutorial to run\u0026gt;\u0026quot;, package = \u0026quot;StatsModels\u0026quot;)\rQuit a tutorial by pressing the “Stop” icon.\n","date":1660608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660608000,"objectID":"17eecd051902cb6c28176e1e35bffd2a","permalink":"https://jacintak.github.io/project/statistical-modelling/","publishdate":"2022-08-16T00:00:00Z","relpermalink":"/project/statistical-modelling/","section":"project","summary":"Learning statitical modelling with functional responses","tags":["teaching","R stats","code"],"title":"Statistical Modelling Practicals","type":"project"},{"authors":[],"categories":["code"],"content":"\rInteractive functions don’t work with knitr\rR has a neat interactive feature. You can write a script or a function that asks the user for input, like a number or a string, that can be used as a variable. I wrote about how this feature works in an interactive function last year. Interactive functions are neat for demonstrating how variables work in functions but aren’t practical for most uses.\nOne practical use for an interactive function I had recently was in a function to import multiple data files downloaded from a data logger. In this case, I wrote a generic function for importing the data because the structure of the data from the data logger (temperature, time, etc.) was consistent. Pretty standard stuff but the function asks the user where the files were located on the local drive. All files in the directory would be cleaned up and imported as a named list.\nNow you could say that the interactive aspect is unnecessary and you would be right. But where would the fun in that be?\nThe importing function was saved as an R script (.R) and called in a Rmarkdown file using source() but there is a problem with this workflow. You can run an interactive session within an Rmarkdown file in a regular R session but you cannot knit it. By default, Rmarkdown does not permit an interactive R session while knitting. And that defeats the point of using Rmarkdown.\nFortunately, there is a workaround to get knitr to ask for the directory when knitting. We need to make some modifications to the YAML, setup chunk, and the importing function.\nYAML\rYou might have seen the option to “Knit with Parameters” in the Knit menu in RStudio. Parameters are additional variables that are called when knitting. We can use parameters to tell knitr where to look for the files we are importing.\nHere’s a generic YAML with a parameter (params) called folder and the directory of the files we want (data/subfolder). Note, no quotation marks in the address. The parameter folder is used like a regular variable in R when knitting.\n---\rtitle: \u0026quot;Title\u0026quot;\routput:\rhtml_document:\rdf_print: paged\rparams:\rfolder: data/subfolder\r---\rIn this example, the files we want are located in a folder called data and a sub-folder called subfolder within our RStudio directory. The address can be where ever you want and it could be a full address or a relative address. I’m using addresses relative to the working directory, specifically the project directory because I’m working within a project. You’ll see why this is important below.\nSetting up directories\rI like to use subdirectories within an RStudio project. For example, I will have a separate folder for scripts, files, figures and any other outputs within my RStudio project folder. However, this is not the default behaviour of knitr and causes some directory issues because knitr uses the source file directory (i.e. “/project/scripts/” folder because that’s where my Rmd file is saved) rather than the project directory (i.e. “/project/”).\nSo if I had the importing function (import_data.R) in the scripts sub-folder, then:\n# Does not work when knitting, works in session\rsource(\u0026quot;scripts/import_data.R\u0026quot;)\r# Works when knitting, does not work in session\rsource(\u0026quot;import_data.R\u0026quot;)\rI would rather have the first option only because it makes more sense given my directory structure but this is a personal choice.\nYou can tell knitr to use the project directory when knitting in the setup chunk via:\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\ropts_knit sets the options for knitting and find_rstudio_root_file() is a helper function to get the root directory of the RStudio project (provided you are in one). This is instead of setwd().\nThe interactive function\rHere’s a generic interactive function that will import all CSV files within a user-defined folder as a list called imported_files.\nimport_files \u0026lt;- function(){\r# Ask the user for the folder address\rfolder_address \u0026lt;- ifelse(interactive() == TRUE,\rreadline(\u0026quot;Enter relative folder address to working directory without quotation marks: \u0026quot;),\rparams$folder)\r# Complete relative address\rfolder_address \u0026lt;- paste(getwd(), folder_address, sep = \u0026quot;/\u0026quot;)\r# Get file names\radd.files \u0026lt;- list.files(folder_address, pattern=\u0026quot;.csv\u0026quot;, recursive = FALSE, full.names = TRUE)\r# Check the user has entered address properly\rif(identical(add.files, character(0))){ message(paste(\u0026quot;Address\u0026quot;, folder_address, \u0026quot;has no files. Please try again.\u0026quot;)) return(import_files()) # Return to the beginning of the function and start again\r}\r# Import file\rget.files \u0026lt;- lapply(add.files, read.csv)\rreturn(get.files)\r}\r# Return as list\rimported_files \u0026lt;- import_files()\rThe important feature of the interactive function that makes it play nicely with knitr is the ifelse statement when asking for the folder our CSVs are saved in (folder_address).\nIn a regular R session that is interactive, the function will ask for the address (via readline) but when knitting (thus when interactive() is FALSE) the folder address is the address defined in the folder parameter (called via params$folder).\nThis is where the knitting parameters we defined earlier comes in. So when knitting the input is data/subfolder.\nThe main reason I use relative addresses is so that I don’t have to type out the full address. I recreate the full address from the working directory so that there is no ambiguity in the address.\nThen, I have an if statement for checking the address and user input. An error message will appear if the address does not have any CSV files (checked using identical). It will print the address so you can check for typos.\nFinally, the lapply function will load the CSVs as a list.\nKnitting\rYou need to use the “Knit with Parameters” option rather than the default Knit button (or a manual render). I’m focussing on HTML here. When you knit, a window will pop up asking you what to input for each parameter you’ve set in the YAML. Here, it’s asking what’s the input for folder. The window will say what you’ve set for folder by default (data/subfolder) or you can change it in the popup.\nIs this really useful? Probably not and it wouldn’t be as reproducible, but we can do it because we can!\nSetting knitr options globally\rYou can set opts_knit within your .Rprofile as a global option using options(knitr.package.root.dir = \u0026lt;address\u0026gt;) so that the root directory of your project is where your .Rproj file is by default:\n# Always use project directory as root directory\rif(class(try(rprojroot::find_rstudio_root_file(), silent = TRUE)) != \u0026quot;try-error\u0026quot;){\roptions(knitr.package.root.dir = rprojroot::find_rstudio_root_file())}\rThis if statement in your .Rprofile file will check if there is an Rproject file (.Rproj) using rprojroot::find_rstudio_root_file().\nThe function try is used in debugging to catch any error messages. silent = TRUE will suppress showing these error messages.\nIf you are not working in a project, then find_rstudio_root_file() will generate an error message. So, we can check if we have generated an error message (class should be a \"try-error\"). If there is an error message, we are not working in a project and we do not change any options (options(knitr.package.root.dir should be NULL).\nIf there is no error message, then find_rstudio_root_file() has found an .Rproj file and will change the root directory to that location.\nAgain, setting .Rprofile defaults creates dependencies in your code which may be convenient for you but not reproducible to others.\nOther points\rYou could also have a project specific .Rprofile to your project root directory.\nParameters can be called anything and you can have any number of parameters. They only work when knitting. They don’t work in a regular session.\nYou can remove the last line of the importing function (imported_files \u0026lt;- import_files()) if you’d rather load the function into your Global Environment.\n","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"782bb9fe5d0e42f0904b888b12f14024","permalink":"https://jacintak.github.io/post/knitting-an-interactive-document/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/post/knitting-an-interactive-document/","section":"post","summary":"Using interactive functions with an Rmarkdown document","tags":["R stats","code"],"title":"Knitting an interactive document","type":"post"},{"authors":["M. R. Kearney","M. E. Jasper","V. L. White","I. J. Aitkenhead","M. J. Blacket","Jacinta Kong","S. L. Chown","A. A. Hoffmann"],"categories":null,"content":"Abstract The rarity of parthenogenetic species is typically attributed to the reduced genetic variability that accompanies the absence of sex, yet natural parthenogens can be surprisingly successful. Ecological success is often proposed to derive from hybridization through enhanced genetic diversity from repetitive origins or enhanced phenotypic breadth from heterosis. Here, we tested and rejected both hypotheses in a classic parthenogen, the diploid grasshopper Warramaba virgo. Genetic data revealed a single hybrid mating origin at least 0.25 million years ago, and comparative analyses of 14 physiological and life history traits showed no evidence for altered fitness relative to its sexual progenitors. Our findings imply that the rarity of parthenogenesis is due to constraints on origin rather than to rapid extinction.\nMedia articles University of Melbourne Pursuit The Conversation ","date":1654128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654128000,"objectID":"a6977a7ca057abb57b2c5b2cd8f2b87b","permalink":"https://jacintak.github.io/publication/2022-Science/","publishdate":"2022-06-02T00:00:00Z","relpermalink":"/publication/2022-Science/","section":"publication","summary":"Abstract The rarity of parthenogenetic species is typically attributed to the reduced genetic variability that accompanies the absence of sex, yet natural parthenogens can be surprisingly successful. Ecological success is often proposed to derive from hybridization through enhanced genetic diversity from repetitive origins or enhanced phenotypic breadth from heterosis. Here, we tested and rejected both hypotheses in a classic parthenogen, the diploid grasshopper Warramaba virgo. Genetic data revealed a single hybrid mating origin at least 0.","tags":["PhD","ectotherms","thermal adaptation","thermal response","parthenogenesis"],"title":"Parthenogenesis without costs in a grasshopper with hybrid origins","type":"publication"},{"authors":["L. E. Schwanz","A. Gunderson","M. Iglesias-Carrasco","M. A. Johnson","Jacinta Kong","J. Riley","N. C. Wu"],"categories":null,"content":"Abstract Comparative analyses have a long history of macro-ecological and -evolutionary approaches to understand structure, function, mechanism, and constraint. As the pace of science accelerates, there is ever-increasing access to diverse types of data and open-access databases that are enabling and inspiring new research. Whether conducting a species-level trait-based analysis or a formal meta-analysis of study effect sizes, comparative approaches share a common reliance on reliable, carefully-curated databases. Unlike many scientific endeavors, building a database is a process that many researchers undertake infrequently and in which we are not formally trained. This commentary provides an introduction to building databases for comparative analyses and highlights challenges and solutions that the authors of the commentary have faced in their own experiences. We focus on four major tips: 1) carefully strategizing the literature search; 2) structuring databases for multiple use; 3) establishing version control within (and beyond) your study; and 4) the importance of making databases accessible. We highlight how one’s approach to these tasks often depends on the goal of the study and the nature of the data. Finally, we assert that the curation of single-question databases has several disadvantages: it limits the possibility of using databases for multiple purposes and decreases efficiency owing to independent researchers repeatedly sifting through large volumes of raw information. We argue that curating databases that are broader than one research question can provide a large return on investment, and that research fields could increase efficiency if community curation of databases was established.\n","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646784000,"objectID":"a2e69a84f5c8dc42c6cd205ee41d1907","permalink":"https://jacintak.github.io/publication/2022-JEB/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/publication/2022-JEB/","section":"publication","summary":"Abstract Comparative analyses have a long history of macro-ecological and -evolutionary approaches to understand structure, function, mechanism, and constraint. As the pace of science accelerates, there is ever-increasing access to diverse types of data and open-access databases that are enabling and inspiring new research. Whether conducting a species-level trait-based analysis or a formal meta-analysis of study effect sizes, comparative approaches share a common reliance on reliable, carefully-curated databases. Unlike many scientific endeavors, building a database is a process that many researchers undertake infrequently and in which we are not formally trained.","tags":["metaanalysis","comparative analysis","data"],"title":"Best practices for building and curating databases for comparative analyses","type":"publication"},{"authors":null,"categories":null,"content":"\rEnd of the year!\rThe work-from-home go-in-to-work and back again switching meant I didn’t have time to write the post I originally intended. But this month I’ve made some updates around the site plus some general end of year housekeeping here on GitHub.\nResearch\rIt’s been a busy year including many firsts or personal records. I “attended” three conferences, wrote my first independent collaborative paper, submitted many papers, got many rejections, applied for jobs and reviewed many papers. I was pleasantly surprised to see my research mentioned in people’s conference presentations. Thank you for reading!\nTeaching\rI’ve updated the teaching materials on the site.\nOver the course of the year, I’ve turned various tutorials I ran for NERD club into blog posts on this site and collated them on a Project page with their original presentations. I’m glad that some people have found them useful.\nAt the moment I’m not sure what I want to do with the GLM course notes on the site. With an update, they may be useful to some, but I revamped my lectures this year, so these notes don’t follow the lectures. I’m still not sure what the best balance of teaching stats is. Too many additional resources seem overwhelming to students especially with remote learning so this year I went for the less is more approach. At the very least, creating the resource was a good opportunity to try out bookdown and figure out how to integrate it within a blogdown site.\nI’ve updated an Introduction to R with how to change the default library address because I noticed that the user library folder created in Documents was syncing to OneDrive for many students using Windows 10. This is a feature of using a Microsoft account but it means there are then issues between R accessing the user library and OneDrive syncing the folder. This becomes a prominent issue if you are running a session that depends on many packages.\nThis feature is frustrating because I don’t expect students to need to delve into Control Panel and change their computer settings. I don’t expect them to know how to do it and it only adds to their stress when they get package related errors during class. Does the younger generation even know what the Control Panel is and how to modify it? It defeats the point of Windows becoming more user friendly. In terms of user friendliness, it would be great if changing the default package folder was an option through the RStudio GUI.\nThese issues came about because I wanted to try delivering practicals through learnr. On paper, interactive tutorials are great and they are well integrated within the RStudio environment through the Tutorial pane. In practice, it complicates the process for the students because now they have to install all the dependencies - and there are a lot of them! Which is an additional learning curve of computer housekeeping. This is also not the kind of debugging I expect the demonstrators to know. The remote delivery makes troubleshooting difficult.\nThough learnr has been around for a few years, it’s still in active development and there are some issues or features that cause headaches during class. Unfortunately, debugging these error messages is not straightforward and the general fix is to reinstall the package again. Two issues I’ve come across that are known, open issues on GitHub:\nTrailing garbage error. Some students cannot open a tutorial a second time, or even a first time. The error is inconsistent across students which makes it hard to infer why it’s happening.\rSkipping coding exercises by pressing run code (even with no input). This defeats the point of not allowing skipping.\rThere is a great temptation among students to rush through and skip exercises to get to the “important bits” (it’s all important) or to jump to any in-class assessment and work in reverse (i.e. reading the assessment question then skim reading the relevant section without working through the theory first). Maybe this is a consequence of remote teaching and open book assessment that this cohort is now used to. Forcing students to slow down and engage with the material in order is one solution.\rI’ve made the learnr practicals available on GitHub and given it a project page. It’s a set of practicals about learning statistical modelling through the context of functional responses (predator-prey interactions). Students collect data in the first practical and analyse the data in the second. I made a Type II functional response model in Scratch for remote students to participate in data collection.\nI moved the original learnr tutorials about R to their own GitHub repository and project page. I’m not sure what to do with this too. My original intention was to provide it as an additional resource to students. Currently, I have revamped the tutorials based on material I’ve presented to NERD club.\nOverall, I’m satisfied with how learnr worked in a large classroom. The loading error is annoying but mostly because of the remote delivery and it only affects a minor proportion of students.\nHere’s to hoping for more adventures in the new year.\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"6d2b292eebf4112c230a23253c2105df","permalink":"https://jacintak.github.io/post/2021-12-01-december-update/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/post/2021-12-01-december-update/","section":"post","summary":"Made it to the end of the year.","tags":["teaching","R stats","code","conference","github"],"title":"December update","type":"post"},{"authors":[],"categories":["code"],"content":"\rI love gifs.\nGifs are great for showing how data changes over time or just for putting something engaging in a presentation. When I was working on modelling insect phenology I wanted to create a gif of how insects hatch throughout the year across Australia for my presentations. Making gifs in R has improved a little since then so here’s a quick guide to making gifs. I won’t cover interactive plots (e.g. plotly).\nLooping through images\rBasically this involves making lots of png images then looping through them. The package is animation which depends on ImageMagick so you will need to install ImageMagick first. I’ve found it handy for sequentially showings layers of a raster (e.g. where each layer is data over time).\nMake sure to install legacy functions when installing ImageMagick (e.g. convert). You may need to tell R where to find ImageMagick using ani.options(convert = 'C:/ImageMagick-7.0.9-Q16/convert.exe') for where ever ImageMagick was installed.\nLet’s pretend we have a raster of soil temperature for one day where each layer (n = 24) is an hour of a day (soil_temp), like a raster from microclimOz.\nWe need to create our loop, then we can use animation::saveGIF to save our image.\nlibrary(animation)\rlibrary(raster)\rpal \u0026lt;- terrain.colors(10, rev = TRUE)\rbre \u0026lt;- round(seq(min(minValue(soil_temp))-1,max(maxValue(soil_temp))+1,length.out=10))\rsaveGIF({\rfor(i in 1:24){plot(soil_temp[[i]], main = i, col = pal, breaks = bre)}\r}, movie.name = \u0026quot;soil_temp.gif\u0026quot;, clean=T, convert = \u0026quot;convert\u0026quot;)\rTo make sure the colour scale is consistent throughout the loop:\nI’ve defined a fixed colour scale (pal) using the base palette terrain.colours and reversed the scale so that higher temperatures are green. Note that the terrain colour palette isn’t great for colour blindness.\rI’ve created my own colour scale (bre) by manually defining the breaks in the colour scale based on the minimum and maximum temperatures in the entire raster dataset. Plus some wiggle room on either side.\rIf I didn’t manually define the colour scale, then each image in the gif will use its own automatically generated scale and the colours will be inconsistent.\nNote the use of curly brackets to call an independent line of R code within code (the loop within saveGIF).\nThe benefit of animation is that it works with any type of image in R and is basically a wrapper for ImageMagick, unlike gganimate. You could also use ImageMagick in the command line.\nGradually showing data\rThis is easily done using ggplot2 and gganimate. ImageMagick isn’t needed (a different engine is used) but this method is limited to ggplot objects.\nI have some heart rate data demonstrating the mammalian diving reflex that I will use as an example. I imported the data from my Apple Watch into R that you can read about at the link. Then, I made a gif for my lectures using gganimate:\nlibrary(gganimate) # loading just gganimate will also load ggplot2 for you\rlibrary(tidyverse)\rhr_plot \u0026lt;- heart_rate %\u0026gt;%\rfilter(time \u0026gt; \u0026quot;2020-10-17 11:00:34\u0026quot; \u0026amp; time \u0026lt; \u0026quot;2020-10-17 11:13:00\u0026quot;) %\u0026gt;% ggplot(aes(time, value)) +\r# Dive 1\rannotate(\u0026quot;rect\u0026quot;, fill = \u0026quot;lightgrey\u0026quot;, alpha = 0.7, xmin = as.POSIXct(\u0026quot;2020-10-17 11:05:00\u0026quot;), xmax = as.POSIXct(\u0026quot;2020-10-17 11:05:30\u0026quot;),\rymin = -Inf, ymax = Inf) +\r# Dive 2\rannotate(\u0026quot;rect\u0026quot;, fill = \u0026quot;lightgrey\u0026quot;, alpha = 0.7, xmin = as.POSIXct(\u0026quot;2020-10-17 11:07:12\u0026quot;), xmax = as.POSIXct(\u0026quot;2020-10-17 11:07:50\u0026quot;),\rymin = -Inf, ymax = Inf) +\rgeom_point(aes(group = seq_along(time))) +\rgeom_line() +\rannotate(\u0026quot;text\u0026quot;, label = \u0026quot;Dives\u0026quot;, x = as.POSIXct(\u0026quot;2020-10-17 11:10\u0026quot;), y = 75) +\rannotate(\u0026quot;rect\u0026quot;, fill = \u0026quot;lightgrey\u0026quot;, alpha = 0.7, xmin = as.POSIXct(\u0026quot;2020-10-17 11:10:40\u0026quot;), xmax = as.POSIXct(\u0026quot;2020-10-17 11:11:10\u0026quot;),\rymin = 73, ymax = 77) +\rtheme_classic() +\rlabs(x = \u0026quot;Time\u0026quot;, y = expression(\u0026quot;Heart rate \u0026quot;(\u0026quot;Beats min\u0026quot;^-1))) +\rscale_x_datetime() + # time is already a POSIXct format\rylim(c(50, 125)) +\rtransition_reveal(time) +\renter_fade()\ranimate(plot = hr_plot,\rnframes = 100,\rfps = 10,\rend_pause = 10,\rheight = 600, width =600, res = 100)\rI’ve split this into two parts. Lets break this down:\nMake the heart rate graph (hr_plot). My heart rate data is saved in a variable called heart_rate.\rI have trimmed the data (dplyr::filter) then plotted heart rate over time (lines and points).\rI have annotated the graph with grey rectangles (annotate(\"rect\")) to indicate diving periods.\rI created a legend using annotate for text and another little grey rectangle.\rUsed expression for scientific notation in my axis labels.\rFormatted the x axis as a date time axis (scale_x_datetime). Not critical here.\rtransition_reveal and enter_fade are gganimate functions that describe how the data is revealed. Here I’m saying reveal along the x axis. This may take some time to render when you call the plot.\ranimate is the main function to create the animation.\rI defined the number of frames, the speed (frames per second), how long to pause the gif at the last frame, and the dimensions.\rUse anim_save to save your gif.\rHere’s proof I am a mammal:\rThere are other types of transitions included in gganimate. There is a handy cheatsheet too.\nHappy animating!\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"7fde4ee379305fae8ab11ee765514b42","permalink":"https://jacintak.github.io/post/2021-11-01-gifs-in-r/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/post/2021-11-01-gifs-in-r/","section":"post","summary":"Two ways of making gifs in R using {animation} and {gganimate}","tags":["R stats","code"],"title":"Gifs keep on giving","type":"post"},{"authors":null,"categories":["code"],"content":"\rNB: This was originally a tutorial given to Space Lunch members on 6th October 2021. This is an adapted version. The original version can be found on the Project page.\nIntroduction\rThis is going to be an introduction to a simple workflow for spatial data in R using rasters. I will assume you have some basic knowledge about spatial analyses and co-ordinate systems. This is not meant to be a documentation of the full suite of spatial analysis available in R. Some simple ways of plotting data is covered.\nRasters are stored spatial data in a gridded format.\nEach grid cell contains a single value. E.g. temperature, elevation, species richness\rOften stored in three dimensions (e.g. latitude, longitude and time).\rThe main R package for handling rasters is raster. It’s a base package.\rWe will consider a common workflow of associating rasters with spatial point data (e.g. lat and long).\rI will be sticking to base R throughout.\nIntroducing rasters\rLoading from file\rRasters can be acquired from a range of sources, such as government agencies. There are also R packages to interface directly with online databases but for another time. Often they are saved as an nc file (network Common Data Form) that is imported as a raster with layers and assigned a spatial projection. You’ll see below that other dependent packages are loaded with raster but you won’t need to load each one manually.\nYou can load a raster from a local nc file using the function raster::brick. The :: denotes calling a function from a specific package without loading it with library. Good for quick and dirty functions you won’t use frequently, bad if you are using the package multiple times.\nmy_raster \u0026lt;- brick(\u0026quot;raster_data.nc\u0026quot;)\rI will cheat and use the built in function in raster to query WorldClim for mean annual temperature.\nlibrary(raster)\r## Loading required package: sp\rtemp \u0026lt;- getData(\u0026quot;worldclim\u0026quot;,var=\u0026quot;bio\u0026quot;,res=10)\rSubsetting rasters\rSubset rasters by layers using basic square bracket subsetting for lists.\ntemp \u0026lt;- temp[[1]] # Subset only the first layer - mean annual temperature\rHere I have selected mean annual temperature since we do not need the other variables.\nPlot rasters\rWe can use the basic plot function to view the raster data.\nplot(temp)\rFigure 1: Average annual temperature\rThe default colour scale is horrendous so we will change it to the viridis scale. Here’s an example of using ::. I don’t need the entire viridis package. This is to make a continuous colour palette of 20 colours. And add a title to the graph.\nplot(temp, main = \u0026quot;Mean annual temperature\u0026quot;, col = viridis::viridis(n = 20))\rFigure 2: That’s better\rThere’s one last issue to deal with before this data is ready. WorldClim stores temperature data multiplied by 10 for space saving so we need to divide by 10.\nManipulating rasters\rRasters can be manipulated by base functions. E.g. addition or subtraction between rasters or layers. There are many other functions for analysing rasters and doing spatial analysis (e.g. interpolation) but we won’t cover that here.\ntemp \u0026lt;- temp/10\rplot(temp, main = \u0026quot;Mean annual temperature\u0026quot;, col = viridis::viridis(n = 20))\rFigure 3: That’s much much better\rSpatial point data\rI usually encounter spatial data in the form of decimal latitude and longitudes representing species occurrences or sampling sites. You may already have these data from your own work but for demonstration purposes I will show how to query an online database to get species distribution points from GBIF. This requires an Internet connection and the R package rgbif.\nLet’s query GBIF occurrence points for an widespread bird: The house sparrow (Passer domesticus).\nYou need the unique identification key for the species you want. name_suggest can help with that so you don’t have to manually search GBIF.\rThe data comes as a list with some metadata. .$data is the actual occurrence records. The dot . is a placeholder meaning it represents an R variable (e.g. a dataframe). This is commonly used in tidyverse and piping via magrittr. It is also a cheat’s way of using base functions within a pipe.\rCo-ordinates are stored as decimalLatitude and decimalLongitude. I’ve removed any missing values.\rlibrary(rgbif)\r# get ID key for bird\rbird_key \u0026lt;- name_suggest(q =\u0026quot;Passer domesticus\u0026quot;, rank=\u0026#39;species\u0026#39;)$data$key[1]\r# get occurence points\rbird_points \u0026lt;- occ_search(taxonKey = bird_key) # Get all records, max 500 (see variable limit)\r# exclude metadata\rbird_points \u0026lt;- bird_points$data\r# remove NA latitude or longitude\rbird_points \u0026lt;- bird_points[!is.na(bird_points$decimalLatitude),]\rSince I’ve only searched for one species, the workflow is simple. If I wanted multiple species I would have to use lists and a function like sapply. See help file for occ_search for an example. Avoid for loops.\nPlot spatial data\rLet’s look at the global distribution of points. I will use the base maps package for a simple, low resolution and unprojected world map in R (not recommended for more professional output). The maps package can also be used in ggplot2 via borders(database = \"world\", fill = NA) or geom_polygon(data = map_data(\"world\"), aes(x=long, y = lat, group = group), fill = NA, col= 1); coord_map() may help in these cases.\nlibrary(maps)\rmap(\u0026quot;world\u0026quot;) # get basic world map\rtitle(main = \u0026quot;The distribution of house sparrow\u0026quot;) # plot title\rpoints(decimalLatitude ~ decimalLongitude, bird_points, pch = 16, col = 2) # plot points\rlegend(x = -150, y= -50, legend = \u0026quot;occurence\u0026quot;, pch = 16, col = 2, bty = \u0026quot;n\u0026quot;)\rWe see that points come mainly from around northern Europe.\nFor more advanced mapping in R check out ggmaps, which can interface with Open Street Maps (free) and Google Maps (for a fee), and osmdata, which interfaces directly with OSM and allows you to customise which features to include - check out the related tutorial about mapping cities in R in the Space Club folder or online.\nPutting it all together\rNow we have all the data we need, let’s combine the datasets and plot the occurrence data with the temp raster.\nplot(temp, main = \u0026quot;Mean annual temperature\u0026quot;, col = viridis::viridis(n = 20)) # temp\rpoints(decimalLatitude ~ decimalLongitude, bird_points, pch = 16, col = 1) # bird\rFigure 4: The distribution of sparrows with mean annual temperature\rLet’s do some simple extraction of data.\nWhat range of temperatures do house sparrows live in?\rWe can use our new species distribution points to query the raster and extract values corresponding with the occurrence points. The function to query a raster is raster::extract. The same can be used within tidyverse via mutate.\n# Get temp\rtemps \u0026lt;- extract(temp, SpatialPoints(cbind(bird_points$decimalLongitude, bird_points$decimalLatitude)), method = \u0026quot;bilinear\u0026quot;)\r# Add new column\rbird_temps \u0026lt;- cbind(bird_points, temps) # Remove missing temps\rbird_temps \u0026lt;- bird_temps[!is.na(bird_temps$temps),]\rmethod = \"bilinear\" tells the function to interpolate the average of the nearest 4 cells around the spatial point. This is like a mini version of buffer which will interpolate values within a buffer around a point. If spatial accuracy is not paramount (like here where we have a global scale raster), then this method might reduce the chance of extracting a NA value. The default is to query the exact coordinate.\nOur final dataset contains 500 observations.\nMissing data at this stage could be from a mismatch between the accuracy of the spatial points and the resolution of the raster. Or plain errors in the spatial coordinates.\nNow we can plot the distribution of temperatures:\nhist(bird_temps$temps, main = \u0026quot;Temperature distribution of house sparrows\u0026quot;)\rWe can see they live between -2.7 and 11.6 °C.\nFinally, we can plot the relationship between temperature and latitude:\nplot(temps ~ decimalLatitude, bird_temps, pch = 16)\rEnd\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"1dd2b182f0dd969ff329299f81c4187e","permalink":"https://jacintak.github.io/post/spatial-data/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/post/spatial-data/","section":"post","summary":"A basic workflow with rasters and spatial points","tags":["code","R stats"],"title":"An introduction to spatial data in R","type":"post"},{"authors":null,"categories":["code"],"content":"\rEarlier this year I wrote about learning to forego for loops for apply functions in R. I’m continuing this journey to replace for loops with purrr. I’ll be honest and say that my main motivation for learning purrr is the package name 🐱. purrr is a package that does the same things as mapply and lapply; to apply a function over listed data and also has useful functions for manipulating lists and functional programming.\nObjectively, the functionality of purrr is not that different to base functions. There’s an understandable learning curve and resulting benefit when going from for loops to apply functions, but there’s diminishing return on going from apply to purrr unless you fully leverage the shortcuts of tidyverse syntax (which I have not). The main advantage of purrr is that it uses the tidyverse syntax and pipes. Overall, I don’t think there’s a huge benefit for using purrr over base, unlike for example the advantages of using ggplot2 over base for graphing, but if your code is already written in tidyverse then it makes sense to stick to it and have clear and consistent code (if you are used to reading tidyverse syntax).\nIf you really want to stay on the tidyverse train you can skip learning apply and jump straight to purrr but I’m a fan of using as fewer dependencies as possible and knowing the base R way. There are lots of detailed tutorials about purrr and it’s functions, like this one that discusses the differences with base functions so I recommend checking those out. If you’re already familiar with the tidyverse syntax then purrr is no different.\nHere are some things I’ve learnt about purrr for applying functions to listed data.\nlapply\rlapply takes one argument (data) and applies a function to it. As I found earlier, it’s quite a simple case and doesn’t suit more complex datasets I usually work with. The purrr equivalent is map.\nOne of the advantages of purrr is that it you can specify the format of the output. That is, lapply and map takes a list and produces a list, but map_* where * are a range of output types will give that output type. For example, map_chr will take a list and produce a character vector. This is handy because it skips an intermediate step to transform your resulting list into your desired output format, such as using do.call to turn a list into a data frame.\nAn example\rLet’s use the same code as the previous post:\n# some data to use as a list\rloop_data \u0026lt;- data.frame(col1 = c(11:15), col2 = c(20:24))\r# define variable to change\ra \u0026lt;- seq(0.2, 1, 0.2)\rAs before, loop_data is a data frame with two numeric columns (col1 \u0026amp; col2). We technically won’t use loop_data$col2 but it’s there to create a 5x2 data frame. a is a variable that we need for our function with 5 values.\nWe want to add each element of a to loop_data$col1 and save that in a new column loop_data$col1a. We will also add a as a column in loop_data just so we can keep track of which value was used to calculate col1a. So the final output should have 25 rows (5 observations in loop_data x 5 values of a) and 4 columns (col1, col2, col1a, a).\nNow let’s use map to do the same thing we did with lapply but using tidyverse and pipes 🛁\nloop_data %\u0026gt;% expand_grid(., a) %\u0026gt;% # expand to include all crossed combinations\rgroup_split(a) %\u0026gt;% # split into lists by the value of a for nested lists\rmap_dfr(., function(x){\rx$col1a \u0026lt;- x$col1 + x$a\rreturn(x)\r}) %\u0026gt;% # apply the function to the list and return a data frame\rsummary(.) # show the summary\r## col1 col2 a col1a ## Min. :11 Min. :20 Min. :0.2 Min. :11.2 ## 1st Qu.:12 1st Qu.:21 1st Qu.:0.4 1st Qu.:12.4 ## Median :13 Median :22 Median :0.6 Median :13.6 ## Mean :13 Mean :22 Mean :0.6 Mean :13.6 ## 3rd Qu.:14 3rd Qu.:23 3rd Qu.:0.8 3rd Qu.:14.8 ## Max. :15 Max. :24 Max. :1.0 Max. :16.0\rIf you’re not familiar with piping this is what’s happening:\nThe first line is specifying our list loop_data to be sent down the pipe (%\u0026gt;%). Pipes are read sequentially and the output of one line is used as the input of the next line. This intermediate object is indicated by the dot (.). Sometimes the dot can be left out if the arguments are presented to the function in the expected order but I find it useful to type everything out when learning anyway so that it’s clear what the arguments are. The dot is particularly needed when using base functions within a pipe, as seen in the last line with summary(.) because these functions are expecting an argument that tidyverse functions know how to deal with.\rI use tidyr::expand_grid to create a data frame of all combinations of col1 and a. This has a benefit of adding a as a column.\rThen I use group_split to group the crossed data frame based on values of a. This produces a tibble which are essentially tidyverse lists. split is a base equivalent.\rThen I apply the actual function over the list and specify that I want the output to be a single data frame (the _dfr suffix). This is the equivalent of doing lapply and do.call in the same function.\rFinally I use the base R function summary to show the summary statistics of the result to check it works. There isn’t a tidyverse equivalent of summary so we must use the dot within the function.\rThe end result is exactly the same as the original lapply code. Here is the lapply function from the previous post to compare:\n# Prepare the answer list\rlapply_ans \u0026lt;- replicate(length(a), loop_data, simplify = FALSE)\r# add a column using mapply\rlapply_ans \u0026lt;- mapply(FUN = cbind, lapply_ans, \u0026quot;a\u0026quot; = a, SIMPLIFY = FALSE)\r# apply function\rlapply_ans \u0026lt;- lapply(lapply_ans, FUN = lapply_function)\r# merge to single data frame\rlapply_ans \u0026lt;- do.call(rbind, lapply_ans)\r# view the data\rsummary(lapply_ans)\rSide note:\rrerun(length(a), loop_data) behaves exactly the same as replicate(length(a), loop_data, simplify = FALSE) and is the tidyverse equivalent (unclear for how long according to the dev notes). Then you’ll need to add a as a column, matching the order of the tibble and set the column names, e.g. rerun(length(a), loop_data) %\u0026gt;% map2(a, bind_cols) %\u0026gt;% map(a=...3, rename).\nThe differences:\nI’ve taken a slightly different approach. I define all possible combinations I want to use in the calculations then creating grouped lists.\rI specified the function within the pipe rather than named in the global environment like in the original post. It’s better to name the function if you’re using it multiple times but in this post I’m only using it once, so I’ll get away with it.\rmap also allows formulas which for simple functions (like adding a constant to all values) will simplify the code and let\ryou use anonymous functions. I’m not used to the formula method of writing functions.\rInstead of 5 separate lines of code with the base version, in tidyverse we can do it in a pipe with 4 steps. But you notice that it’s not a huge difference between what the two approaches are doing. Still better than a for loop.\rWe skipped do.call by using map_dfr directly to return a data frame. I could also use map and transform the list into a\rdata frame separately.\rAnd another thing…\rWe need to prepare the input data so that it is crossed; which mean replicating our list across all combinations of col1 and a. expand_grid or similar as used above could be helpful for this, and the data frame could be split into nested lists for applying the function.\nTo contrast, this will only add matching rows of col1 and a together rather than all combinations:\nlist(loop_data$col1, a) %\u0026gt;%\rpmap_dfr(function(x, a) {\rdf \u0026lt;- data.frame(col1 = x,\ra = a,\rcol1a = x + a) # add answer to a new column\rreturn(df)\r})\r## col1 a col1a\r## 1 11 0.2 11.2\r## 2 12 0.4 12.4\r## 3 13 0.6 13.6\r## 4 14 0.8 14.8\r## 5 15 1.0 16.0\rSince map is the equivalent of lapply, then it also doesn’t take multiple inputs, which is why we added a as a column to loop_data. So we turn to mapply and its purrr equivalent.\nmapply\rThe purrr equivalent of mapply is pmap. Specifically, pmap allows for any number of arguments for the function. There is another function, map2 that accepts exactly two arguments but pmap is generalised to allow for more than two. As with map, there are variants with suffixes that specify what output format you want, such as a data frame (pmap_dfr).\nThe tidyverse website goes into the syntax differences between mapply and pmap in more detail.\nLet’s jump to the example using the same loop_function as the original post.\npmap\r# A function to add a value a to a data frame x\rloop_function \u0026lt;- function(x, a) {\rx$col1a \u0026lt;- x$col1 + a # add answer to a new column\rx$a \u0026lt;- a\rreturn(x)\r}\rloop_data %\u0026gt;% rerun(length(a), .) %\u0026gt;% # replicate the list to populate\rlist(a) %\u0026gt;% # define all variables for loop_function within a list\rpmap_dfr(loop_function) %\u0026gt;% # apply the function to the list and return a data frame\rmap_dfc(summary) # show the summary\r## # A tibble: 6 x 4\r## col1 col2 col1a a ## \u0026lt;table\u0026gt; \u0026lt;table\u0026gt; \u0026lt;table\u0026gt; \u0026lt;table\u0026gt;\r## 1 11 20 11.2 0.2 ## 2 12 21 12.4 0.4 ## 3 13 22 13.6 0.6 ## 4 13 22 13.6 0.6 ## 5 14 23 14.8 0.8 ## 6 15 24 16.0 1.0\rNow we don’t have to add a as a column to loop_data, we can specify a for the function. pmap takes a list of arguments for the function, hence we need a list containing both loop_data and a. Don’t make a list before adding it to the list of function arguments (i.e. double list) because it won’t match the nth a variable with the nth element in the loop_data list, and match by rows within lists. For variety, I’ve used map_dfc to call the function summary on the data, rather than summary(.). map_dfc will apply the function by columns instead of rows and produce a data frame.\nThe map2 equivalent is more concise than pmap for this simple example!\nloop_data %\u0026gt;% rerun(length(a), .) %\u0026gt;% map2_dfr(a, loop_function)\rHere is the original mapply example to compare:\n# Prepare the answer list\rmapply_ans \u0026lt;- replicate(length(a), loop_data, simplify = FALSE)\r# mapply function\rmapply_ans \u0026lt;- mapply(mapply_ans, FUN = loop_function, a = a, SIMPLIFY = FALSE)\r# merge to single data frame\rmapply_ans \u0026lt;- do.call(rbind, mapply_ans)\r# view the data\rsummary(mapply_ans)\rYou could also define loop_function as an anonymous function within pmap.\nMake sure the variables are used in the correct order. e.g. loop_data %\u0026gt;% rerun(length(a), .) %\u0026gt;% map_dfr(loop_function, a) will run because you are passing a as a variable into loop_function, but it’s adding a by row within individual data frame rather than matching the nth element of the list. So it’s effectively replicating the data frame 5 times.\nThat’s it. There are many ways of doing the same thing with simple examples. Hope it helps you create purrrfectly sensible code to replace for loops and apply functions to lists.\n","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"6185c03b7b3350951a09ee17eaf6c2e8","permalink":"https://jacintak.github.io/post/purrr/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/post/purrr/","section":"post","summary":"Learning to use purrr","tags":["code","R stats"],"title":"Beyond the valley of intermediate competence","type":"post"},{"authors":[],"categories":["code"],"content":"\rPhylopic is an online database of silhouettes of species. Most are freely available, with various copyright licences. It’s a great resource for scientific illustration or making cool presentations.\nOne way I wanted to use Phylopic was to add a silhouette of an animal directly to an R figure. You could search Phylopic yourself and copy the image id to add the icon to the graph but where’s the fun in that?\nrphylopic is an R package that can search and retrieve icons directly from Phylopic. You can use rphylopic with ggplot2 or base graphics.\nThe process of searching Phylopic and pulling out the image id is not straightforward, and I didn’t find a step-by-step guide I liked for doing so, so here is a reproducible example using the built in dataset beaver1 - a time series of a beaver’s body temperature.\nThe data looks like this and we want to add a beaver icon to the top left corner:\nbeaver_plot \u0026lt;- qplot(beaver1$temp,x = seq_along(beaver1$temp), geom = \u0026quot;line\u0026quot;, xlab = \u0026quot;Time\u0026quot;, ylab = \u0026quot;Temperature\u0026quot;)\rbeaver_plot\rStep-by-step guide to adding a Phylopic icon\r1. Getting the right species\rYou can search Phylopic by species using name_search. You might get multiple hits because there may be multiple matches in the databases. It’s worth cross-referencing the NameBank ID with the website. The NameBank ID is located at the top right of the webpage. In this case we want the first option - 109179.\nlibrary(rphylopic)\rbeaver \u0026lt;- name_search(text = \u0026quot;Castor canadensis\u0026quot;, options = \u0026quot;namebankID\u0026quot;)[[1]] # find names\rbeaver\r2. Extracting the id of the icon you want\rUse name_images to list all the beaver icons available. In this case, there are two versions of the beaver icon we can use - listed as $same[[1]] and $same[[2]] with unique uid. The uid is the unique id of the icon. Again, it’s handy to check the uid with the website. You can find the uid on the website by clicking the actual icon you want to use and copying from the address bar.\nbeaver_id_all \u0026lt;- name_images(uuid = beaver$uid[1]) # list images\rbeaver_id_all\rLet’s use the second icon and extract only that uid.\nbeaver_id \u0026lt;- name_images(uuid = beaver$uid[1])$same[[2]]$uid # get individual image id\rbeaver_id\r3. Getting the icon itself\rNow we can get the actual image using image_data. Each icon is available in different sizes, from a thumbnail (64 px) to large icons (1042 px). We will get a 256 px icon so that the resolution is high enough to avoid pixelation.\nbeaver_pic \u0026lt;- image_data(beaver_id, size = 256)[[1]] # get actual icon, define size. Don\u0026#39;t run this alone\r4. Adding the beaver icon to the plot\rUse add_phylopic to add the icon to a ggplot2 graph. You need to specify the x and y axis co-ordinates for the graph. Use ysize to change the size of the icon. Use alpha to control the transparency. colour will change the colour.\nbeaver_plot + add_phylopic(beaver_pic, alpha = 1, x = 10, y = 37.4, ysize = 10)\rThe final plot\nAnd that’s it! 🦫\nOther uses of rphylopic\rYou can add the icon as a background image but I would question why that would be a good idea from a graphic design perspective. To do so, you don’t need to specify any other variables in add_phylopic.\rYou can also use icons as data points by plotting each icon in place of the regular point within a for loop. You can see an example in the rphylopic documentation.\r","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"117841f0586f12e7501e3f4591c93eee","permalink":"https://jacintak.github.io/post/2021-08-01-rphylopic/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/post/2021-08-01-rphylopic/","section":"post","summary":"A step-by-step guide to rphylopic","tags":["R stats","ggplot2","code"],"title":"How to add a Phylopic icon to your graph in R","type":"post"},{"authors":["Jacinta Kong","A. A. Hoffmann","M. R. Kearney"],"categories":null,"content":"Insect life cycles are adapted to a seasonal climate by expressing alternative voltinism phenotypes—the number of generations in a year. The problem is to understand how this phenotypic variation along latitudinal gradients is generated through the interactions between environmental factors, like temperature, and the traits of organisms, like development rate and dormancy. However, our current understanding is limited by how thermal responses are characterised, competing theories of thermal adaptation and an incomplete understanding of complex life cycles. Using the widely distributed grasshopper genus Warramaba as a model, we aimed to reconcile theories of thermal adaptation and tested their respective predictions. We hypothesised that the egg stage was a critical life stage for generating latitudinal patterns of voltinism in Warramaba. We described patterns of voltinism and thermal response of egg development rate within and among species of Warramaba along a latitudinal temperature gradient. We found a latitudinal pattern of univoltinism at high latitudes and multivoltinism at low latitudes that corresponded with remarkably strong divergence in egg dormancy patterns and thermal responses of egg development. We argue that the switch in voltinism along the latitudinal gradient was generated by the combined predictions of the evolution of voltinism and of thermal adaptation. We conclude that analyses of latitudinal patterns in thermal responses and corresponding life histories need to consider the evolution of thermal response curves within the context of seasonal temperature cycles rather than based solely on optimality and trade-offs in performance.\n","date":1625753100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625753100,"objectID":"5f2733e01568d1babf63e6e6d23a28ae","permalink":"https://jacintak.github.io/talk/SEB2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/talk/SEB2021/","section":"talk","summary":"Thermal adaptation and plasticity of egg development generates latitudinal patterns in insect life cycles under seasonal climates","tags":["PhD","conference","ectotherms","thermal response","life cycles"],"title":"Society for Experimental Biology Annual Meeting","type":"talk"},{"authors":["Jacinta Kong","J.-F. Arnoldi","A. L. Jackson","A. E. Bates","S. A. Morley","J. A. Smith \u0026 N. L. Payne"],"categories":null,"content":"","date":1625655600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625655600,"objectID":"e9c30482f99ff4c0549103725e048bf6","permalink":"https://jacintak.github.io/talk/BESMacro2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/talk/BESMacro2021/","section":"talk","summary":"Ectotherm heat limits track biological rates","tags":["postdoc","conference","ectotherms","temperature","comparative analysis"],"title":"BES Macro 2021","type":"talk"},{"authors":null,"categories":["code"],"content":"\rThis tutorial was originally presented to NERD club on 4/2/2020.\nCity maps\rConsider yourself a hipster?\nDo the clean lines and natural materials of modern scandi make you feel at home?\nIs your basic coffee order a flat white? ☕\nIf the answer to all the above is YES, then here’s a present for you!\nBut wait! This poster costs €30 (thereabouts) online!\rSee example.\nThat’s approximately 9 flat whites you could have had.\n☕☕☕☕☕☕☕☕☕\nCan you make this in R?, you ask, asking for a friend.\nFear not. You can make this yourself in R!\nMaps in R\rIn this tutorial we will replicate a poster like this. We will need R and powerpoint to put in the final touches. You could do it fully in R but powerpoint will make our lives a bit easier. In summary, it requires a bit of GIS wrangling to code in what you want to display.\nThe data is freely available from Openstreetmap, for proprietary haters out there. I will refer to it as OSM.\nWe will be following this tutorial.\nSetup\rYou will need to install the relevant packages: osmdata, tidyverse and sf.\n#install.packages(\u0026quot;osmdata\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;sf\u0026quot;)\rlibrary(osmdata)\rlibrary(tidyverse)\rAs you can see this will use tidyverse and I will be using piping. Don’t worry if you are not a master at piping. The code is written.\nIn a nutshell, instead of function2(function1(X)) to apply function 1 then function 2 to X, you type x %\u0026gt;% function1() %\u0026gt;% function2(). I.E. take X, apply function 1, then apply function 2 to the resulting output. Overall it’s easier to read, hence it’s ‘tidy’.\nOSM data\rOSM stores various features you can explore under available_features(). You can see what is under each feature with available_tags(\"\u0026lt;insert feature name here\u0026gt;\").\n1. Get city co-ordinates\rFor this example we will make a map of Dublin. First we need the latitude and longitude of Dublin. If you want to modify the extent of your map, this is where you change the co-ordinates.\ncity_coords \u0026lt;- getbb(\u0026quot;Dublin Ireland\u0026quot;)\r#city_coords \u0026lt;- c(-6.391,53.2644,-6.114883, 53.416) # to get all the M50\r2. Get map features\rRoads\rWe can get roads by querying OSM for the GPS co-ordinates for Dublin and saving it to a variable called roads.\nroads \u0026lt;- city_coords %\u0026gt;% #pipe!\ropq() %\u0026gt;% # create query for OSM database\radd_osm_feature(key = \u0026quot;highway\u0026quot;, value = c(\u0026quot;motorway\u0026quot;, \u0026quot;primary\u0026quot;, \u0026quot;secondary\u0026quot;, \u0026quot;tertiary\u0026quot;)) %\u0026gt;%\rosmdata_sf() # save it as an simple features format\rroads\rStreets\rWe can do the same for streets.\nstreets \u0026lt;- city_coords%\u0026gt;%\ropq()%\u0026gt;%\radd_osm_feature(key = \u0026quot;highway\u0026quot;, value = c(\u0026quot;residential\u0026quot;, \u0026quot;living_street\u0026quot;,\r\u0026quot;unclassified\u0026quot;,\r\u0026quot;service\u0026quot;, \u0026quot;footway\u0026quot;)) %\u0026gt;%\rosmdata_sf()\rWater\rCan’t forget the Liffey and the canals. Sadly the ocean cannot be mapped.\nwater \u0026lt;- city_coords%\u0026gt;%\ropq()%\u0026gt;%\radd_osm_feature(key = \u0026quot;waterway\u0026quot;, value = c(\u0026quot;canal\u0026quot;, \u0026quot;river\u0026quot;)) %\u0026gt;%\rosmdata_sf()\r3. Plotting\rTime to call ggplot2 and plot our map.\nmap \u0026lt;- ggplot() +\r# roads\rgeom_sf(data = roads$osm_lines,\rinherit.aes = FALSE,\rcolor = \u0026quot;grey\u0026quot;, # colour of feature\rsize = 0.8, # Size on map\ralpha = 0.8) + # transparency\r# streets\rgeom_sf(data = streets$osm_lines,\rinherit.aes = FALSE,\rcolor = \u0026quot;#ffbe7f\u0026quot;,\rsize = 0.2,\ralpha = 0.6) +\r# water\rgeom_sf(data = water$osm_lines,\rinherit.aes = FALSE,\rcolor = \u0026quot;steelblue\u0026quot;,\rsize = 0.8,\ralpha = 0.5) +\r# extent to display\rcoord_sf(xlim = c(city_coords[1],city_coords[3]),\rylim = c(city_coords[2],city_coords[4]),\rexpand = FALSE) +\r# remove axes\rtheme_void()\rmap\r4. Labels\rAt this point it is easier to save the file and add text in powerpoint but if you want to try your hand at ggplot’s annotation features go ahead.\nHere I’ve done one in a dark colour scheme.\ntheme_colour \u0026lt;- \u0026quot;#282828\u0026quot; # dark theme\rdark_map \u0026lt;- map +\rlabs(caption = \u0026quot;Dublin, Ireland\u0026quot;) +\rtheme(axis.text = element_blank(),\rplot.margin=unit(c(1,1,1,1),\u0026quot;cm\u0026quot;),\rpanel.grid.major = element_line(colour = theme_colour),\rpanel.grid.minor = element_line(colour = theme_colour),\rplot.background = element_rect(fill = theme_colour),\rpanel.background = element_rect(fill = theme_colour),\rplot.caption = element_text(size = 24, colour = \u0026quot;white\u0026quot;, hjust = 0.5, vjust = -2, family = \u0026quot;mono\u0026quot;),\rpanel.border = element_rect(colour = \u0026quot;white\u0026quot;, fill=NA, size=2),\raxis.ticks = element_blank())\rdark_map\rSaving our map\rggsave(plot = dark_map, filename = \u0026quot;NERD/dark_dublin.pdf\u0026quot;, width = 11, height = 8.5, device = \u0026quot;pdf\u0026quot;, dpi = 300)\rIf all of that was too much, there’s an R package for it. https://github.com/lina2497/Giftmap\nThere is also a website\nExtra details\rLess is more but if you really want to put more features:\nOther water bodies\rextra_water \u0026lt;- city_coords %\u0026gt;%\ropq()%\u0026gt;%\radd_osm_feature(key = \u0026quot;natural\u0026quot;, value = c(\u0026quot;water\u0026quot;)) %\u0026gt;%\rosmdata_sf()\rdark_map +\rgeom_sf(data = extra_water$osm_polygons,\rinherit.aes = FALSE,\rfill = \u0026quot;steelblue\u0026quot;,\rcolour = NA,\ralpha = 0.5) +\rgeom_sf(data = extra_water$osm_multipolygons,\rinherit.aes = FALSE,\rfill = \u0026quot;steelblue\u0026quot;,\rcolour = NA,\ralpha = 0.5) +\r# extent to display\rcoord_sf(xlim = c(city_coords[1],city_coords[3]),\rylim = c(city_coords[2],city_coords[4]),\rexpand = FALSE)\rParks\rNature reserves including Dublin Bay\npark \u0026lt;- city_coords%\u0026gt;%\ropq()%\u0026gt;%\radd_osm_feature(key = \u0026quot;leisure\u0026quot;, value = c(\u0026quot;park\u0026quot;)) %\u0026gt;%\rosmdata_sf()\rdark_map +\rgeom_sf(data = park$osm_polygons,\rinherit.aes = FALSE,\rfill = \u0026quot;darkgreen\u0026quot;,\rcolour = NA,\ralpha = 0.3) +\rgeom_sf(data = park$osm_multipolygons,\rinherit.aes = FALSE,\rfill = \u0026quot;darkgreen\u0026quot;,\rcolour = NA,\ralpha = 0.3) +\r# extent to display\rcoord_sf(xlim = c(city_coords[1],city_coords[3]),\rylim = c(city_coords[2],city_coords[4]),\rexpand = FALSE)\rEnd\rThat’s the gist of using OSM in R. You can use the same code to make any map, e.g. for a paper.\n","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"c1e6daebce585feaf9f52534df125c22","permalink":"https://jacintak.github.io/post/2021-07-01-OSM-in-R/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/post/2021-07-01-OSM-in-R/","section":"post","summary":"Make a map in R using Open Street Map","tags":["teaching","R stats","code","NERD club"],"title":"OSM in R","type":"post"},{"authors":null,"categories":null,"content":"\rInstallation\r1. List of palettes\rUnderstanding the structure of the palette\r2. Defining and using a palette\r3. Visualise a palette\rPalettes\rPalette by categories\rBirds\rFish\rFrogs\rInverts\rLandscapes\rLizards\rMammals\rPlants\rSnakes\rWarramaba grasshoppers\rcolRoz is a themed colour palette package by Jacinta Kong \u0026amp; Nicholas Wu.\nThe palettes are based on the colour schemes of Australia.\ncolRoz can:\nGenerate a palette of discrete colours of a specified number\rGenerate a gradient continuous colours of a specified number\rFor this, there are three functions described below. Before that, let’s set up this introduction.\nInstallation\rdevtools::install_github(\u0026quot;jacintak/colRoz\u0026quot;)\rcolRoz works with base R and ggplot2 because it is a palette generator and doesn’t have a built in ggplot palette function.\n1. List of palettes\rThe oz_palettes function contains the list of palettes available. Individual palettes are gouped by theme in a list. The oz_palettes variable is a list of these collated lists.\nnames(oz_palettes) # See all palette themes\r[1] \u0026quot;warramaba\u0026quot; \u0026quot;lizards\u0026quot; \u0026quot;landscapes\u0026quot; \u0026quot;birds\u0026quot; \u0026quot;frogs\u0026quot; [6] \u0026quot;snakes\u0026quot; \u0026quot;plants\u0026quot; \u0026quot;fish\u0026quot; \u0026quot;inverts\u0026quot; \u0026quot;mammals\u0026quot; names(oz_palettes$lizards) # See all lizard palettes\r[1] \u0026quot;c.decresii\u0026quot; \u0026quot;c.kingii\u0026quot; \u0026quot;e.leuraensis\u0026quot; \u0026quot;i.lesueurii\u0026quot; [5] \u0026quot;l.boydii\u0026quot; \u0026quot;m.horridus\u0026quot; \u0026quot;m.horridus2\u0026quot; \u0026quot;t.nigrolutea\u0026quot; [9] \u0026quot;v.acanthurus\u0026quot; \u0026quot;v.pilbarensis\u0026quot; \u0026quot;n.levis\u0026quot; \u0026quot;s.spinigerus\u0026quot; [13] \u0026quot;e.kingii\u0026quot; We can call a specific list using subsetting rules for lists.\noz_palettes[[\u0026quot;warramaba\u0026quot;]][[\u0026quot;whitei\u0026quot;]] # Subset the palette for Warramaba whitei, format: [[theme list]][[palette list]]\r[,1] [,2] [,3] [,4] [,5] [,6] [1,] \u0026quot;#E5A430\u0026quot; \u0026quot;#9C7210\u0026quot; \u0026quot;#D7A8B8\u0026quot; \u0026quot;#BAB24F\u0026quot; \u0026quot;#392821\u0026quot; \u0026quot;#9B391B\u0026quot;\r[2,] \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;5\u0026quot; \u0026quot;4\u0026quot; \u0026quot;2\u0026quot; oz_palettes$warramaba$whitei # does the same as above but using list names\r[,1] [,2] [,3] [,4] [,5] [,6] [1,] \u0026quot;#E5A430\u0026quot; \u0026quot;#9C7210\u0026quot; \u0026quot;#D7A8B8\u0026quot; \u0026quot;#BAB24F\u0026quot; \u0026quot;#392821\u0026quot; \u0026quot;#9B391B\u0026quot;\r[2,] \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;5\u0026quot; \u0026quot;4\u0026quot; \u0026quot;2\u0026quot; Understanding the structure of the palette\rLists within lists may seem daunting but you’d rarely need to access the palettes individually. It is also easy enough to add your own palettes if you are comfortable with manually editing package functions in R.\nWe are happy to accept community contributions. Adding pre-chosen hex codes is easy to do. It may take longer to make a palette if we need to chose hex colours from an image.\nThe general structure for a set of palettes is:\npalette \u0026lt;- list(\rpal1 = rbind(c(\u0026lt;hex codes\u0026gt;), c(\u0026lt;order of discrete colours\u0026gt;))\r)\rTwo things to note:\nThe hex codes are stored as a vector in the first row of the list\rThe second row of the list is a vector of the order colours are used when plotting discrete colours\r2. Defining and using a palette\rThe palettes in this package are set as above. The main function is the palette generator. It acts as a housekeeping function to allow R to interpret the desired palette for plotting. The behaviour of this function depends on whether a discrete or continuous palette is desired and the number of colours requested.\nIf a discrete palette of 3 colours is desired, then the function will chose the subset of 3 colours to be included from the full option of colours in a palette. The chosen order of these colours is hard coded in the list of palette.\nNote there is no need to tell colRoz what theme the palette you want is in. Type in the palette name and colRoz will search the entire oz_palette list\npal \u0026lt;- colRoz_pal(name = \u0026quot;ngadju\u0026quot;, n = 3, type = \u0026quot;discrete\u0026quot;)\r# a palette of only 3 colours\rlibrary(ggplot2)\rggplot(iris, aes(Petal.Width, Petal.Length , colour=Species)) +\rgeom_point() +\rscale_colour_manual(values = pal) +\rtheme_classic()\rIf a continuous palette is desired, then the function will use the function colorRampPalette in the grDevices package (included in R) to generate a gradient of colours between the first and last colour in the desired palette.\n\"continuous\" palettes are generated by default if the type argument is left blank. In ggplot2, use the function scale_colour_gradientn to set the continuous scale.\npal \u0026lt;- colRoz_pal(name = \u0026quot;ngadju\u0026quot;, n = 50, type = \u0026quot;continuous\u0026quot;)\rggplot(iris, aes(Petal.Width, Sepal.Length , colour=Petal.Length)) +\rgeom_point() +\rscale_colour_gradientn(colours = pal) +\rtheme_classic()\r***\n3. Visualise a palette\rThe function to plot the palette is only for graphing. Information is taken about the number of colours to plot from the desired palette and the palette is plotted. The name of the palette is shown.\nprint_palette(colRoz_pal(\u0026quot;c.decresii\u0026quot;))# if empty, all colours are shown\rprint_palette(colRoz_pal(\u0026quot;c.decresii\u0026quot;, type = \u0026quot;discrete\u0026quot;, n = 4))\rprint_palette(colRoz_pal(\u0026quot;c.decresii\u0026quot;, type = \u0026quot;continuous\u0026quot;, n = 30))\rPalettes\rcolRoz has a number of palettes sorted by categories:\nBirds\rFish\rFrogs\rInverts\rLandscapes\rLizards\rMammals\rPlants\rSnakes\rWarramaba grasshoppers\rPalette by categories\rBirds\rnames(oz_palettes$birds)\r[1] \u0026quot;p.cincta\u0026quot; \u0026quot;c.azureus\u0026quot; \u0026quot;m.cyaneus\u0026quot; \u0026quot;d.novae\u0026quot; Black-throated finch. Australia’s 2019 Bird of the Year!\nAzure kingfisher photo by Brenton von Takach\nSuperb fairywren photo by Jessica McLachlan\nEmu\nFish\rnames(oz_palettes$fish)\r[1] \u0026quot;r.aculeatus\u0026quot;\rPicasso triggerfish photo by Brenton von Takach. Also called humuhumunukunukuapuaa in Hawaiian (see also the Octonauts episode)\nFrogs\rnames(oz_palettes$frogs)\rNULL\rThere are no frog palettes yet! Send us some and have your name here.\nInverts\rnames(oz_palettes$inverts)\r[1] \u0026quot;p.mitchelli\u0026quot; \u0026quot;k.tristis\u0026quot; \u0026quot;m.oscellata\u0026quot; \u0026quot;a.conica\u0026quot; [5] \u0026quot;v.viatica\u0026quot; \u0026quot;c.brevi\u0026quot; \u0026quot;a.westwoodi\u0026quot; \u0026quot;a.plagiata\u0026quot; [9] \u0026quot;physalia\u0026quot; \u0026quot;c.australasiae\u0026quot; \u0026quot;k.scurra\u0026quot; \u0026quot;l.vestiens\u0026quot; [13] \u0026quot;t.australis\u0026quot; Mitchell’s diurnal cockroach photo by Craig White\nChameleon grasshopper photo by Kate Umbers\nGaudy acacia grasshopper\nGiant green slant-face\nMatchstick grasshopper, VIC. See Vandiemenella grasshoppers\nShort-tailed nudibranch, Port Philip Bay, VIC\nTortoise beetle\nTwo-spots tiger moth\nBluebottle. Undescribed species\nGreen grocer cicada\nKey’s matchstick grasshopper. See more info about K. scurra\nSea cucumber, intertidal VIC\nBiscuit star, Port Phillip Bay, VIC\nLandscapes\rnames(oz_palettes$landscapes)\r[1] \u0026quot;uluru\u0026quot; \u0026quot;shark_bay\u0026quot; \u0026quot;sky\u0026quot; \u0026quot;desert_sunset\u0026quot;\r[5] \u0026quot;desert_dusk\u0026quot; \u0026quot;desert_flood\u0026quot; \u0026quot;salt_lake\u0026quot; \u0026quot;daintree\u0026quot; [9] \u0026quot;spinifex\u0026quot; \u0026quot;nq_stream\u0026quot; \u0026quot;kimberley\u0026quot; \u0026quot;capricorn\u0026quot; Photo from Jordan Iles\nLizards\rnames(oz_palettes$lizards)\r[1] \u0026quot;c.decresii\u0026quot; \u0026quot;c.kingii\u0026quot; \u0026quot;e.leuraensis\u0026quot; \u0026quot;i.lesueurii\u0026quot; [5] \u0026quot;l.boydii\u0026quot; \u0026quot;m.horridus\u0026quot; \u0026quot;m.horridus2\u0026quot; \u0026quot;t.nigrolutea\u0026quot; [9] \u0026quot;v.acanthurus\u0026quot; \u0026quot;v.pilbarensis\u0026quot; \u0026quot;n.levis\u0026quot; \u0026quot;s.spinigerus\u0026quot; [13] \u0026quot;e.kingii\u0026quot; Tawny dragon\nBlue Mountains water skink\nThorny devil\nBlotched blue-tongued skink\nSouth-western spiny tailed gecko\nKing’s skink\nThree-lined knobtail gecko\nFrilled-neck lizard\nEastern water dragon\nBoyd’s forest dragon\nSpiny-tailed monitor\nPilbara rock monitor\nMammals\rnames(oz_palettes$mammals)\r[1] \u0026quot;p.breviceps\u0026quot; \u0026quot;thylacine\u0026quot; Sugar glider\nThylacine (T. cynocephalus)\nPlants\rnames(oz_palettes$plants)\r[1] \u0026quot;n.violacea\u0026quot; \u0026quot;xantho\u0026quot; Blue lily photo by Emma Dalziell\nXanthorrhoea grasstree photo by Sarah Mulhall\nSnakes\rnames(oz_palettes$snakes)\r[1] \u0026quot;a.ramsayi\u0026quot;\rWoma python\nWarramaba grasshoppers\rThese are palettes based on the colours of matchstick grasshoppers in the genus Warramaba.\rYou can read more about matchstick grasshoppers on Jacinta’s website.\nThere are other matchstick grasshopper palettes in the inverts palette.\nnames(oz_palettes$warramaba)\r[1] \u0026quot;grandis\u0026quot; \u0026quot;flavolineata\u0026quot; \u0026quot;whitei\u0026quot; \u0026quot;picta\u0026quot; \u0026quot;virgo\u0026quot; [6] \u0026quot;ngadju\u0026quot; ","date":1624579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624579200,"objectID":"915eb24acd339dfbfc5b9325f0743bea","permalink":"https://jacintak.github.io/project/colRoz/","publishdate":"2021-06-25T00:00:00Z","relpermalink":"/project/colRoz/","section":"project","summary":"Installation\r1. List of palettes\rUnderstanding the structure of the palette\r2. Defining and using a palette\r3. Visualise a palette\rPalettes\rPalette by categories\rBirds\rFish\rFrogs\rInverts\rLandscapes\rLizards\rMammals\rPlants\rSnakes\rWarramaba grasshoppers\rcolRoz is a themed colour palette package by Jacinta Kong \u0026amp; Nicholas Wu.\nThe palettes are based on the colour schemes of Australia.\ncolRoz can:\nGenerate a palette of discrete colours of a specified number\rGenerate a gradient continuous colours of a specified number\rFor this, there are three functions described below.","tags":["side-project"],"title":"colRoz - A colour package for the land down under","type":"project"},{"authors":null,"categories":null,"content":"\rNERD club is a student-led peer-learning and discussion group for staff and postgraduate students in the Departments of Zoology and Botany at Trinity College Dublin. The group meets weekly for topical discussions about science or academia. There are also sub-groups that are dedicated towards specific topics such as R coding and spatial analysis. These sub-groups are focused towards peer-learning where, postgraduates in particular, are encouraged to share their learning experience and expertise in relevant topics.\nI have been an active contributor to NERD club and its sub-groups: R club for R programming and Space Lunch for GIS and spatial analysis. This page documents some of the outputs I have produced for peer-learning activities.\nTutorials\rIntroduction to blogdown\rA short workshop giving an introduction to the blogdown workflow.\nAdvanced R markdown\rThis is a short presentation showing some of the more advanced features of R Markdown using the R package bookdown including: numbered sections, cross-referencing, bibliographies, CSS and making a website with the static HTML builder.\nPDF: Interactive functions and loops in R\rThis tutorial describes how to make an R function that asks the user to input values for the function, and how to run a function within a simple for loop.\nOSM in R\rThis tutorial describes how to interface with Open Street Maps in R to make a fancy map you can print and give to someone but the same code can be used to make maps for any purpose.\nIntroduction to spatial points in R\rThis is a walkthrough of a basic workflow for working with spatial data and rasters in R. Specifically loading a raster, plotting a raster and extracting information from rasters. I use rgbif to get species occurrence records from GBIF and extract temperature data from a raster of global temperatures. A blog post version is here.\nHTML: Fundamental linear regression in R\rThis is a short presentation showing some of the basic features of linear regression in R using lm including: ANOVA tables, summary and residual plots.\nPDF: Model selection in R\rAn introduction to model parsimony and basic ways of selecting linear models and predictor variables.\nPDF: Fundamental linear regression assumptions\rA run through the fundamental assumptions linear regression in R using lm based on residual plots.\nPDF: ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"7760691d5a073820ba7a7e4ee844da9b","permalink":"https://jacintak.github.io/project/NERD-club/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/project/NERD-club/","section":"project","summary":"My contributions to the Zoology \u0026 Botany NERD Club at Trinity College Dublin","tags":["teaching","R stats","code","NERD club"],"title":"NERD club tutorials","type":"project"},{"authors":null,"categories":["code"],"content":"\rThis tutorial was originally presented to NERD club on 18/11/2020.\nThis document contains two examples of functions and an example of how functions can be integrated with loops.\nUser defined functions take the general form of function(inputs){processing inputs; return(output)}\nPredator-Prey interactions\rWe will use a simulation of predator-prey interactions as an example. Predator-prey interactions simulate how many prey a predator can capture after a specific amount of time and for a given density of prey. We use this example with undergraduate biology students to demonstrate statistical modelling, experimental design and collecting data.\nNormally we would get students to do this laboratory practical in class by picking up counters and putting them in jars while blindfolded. We can also see whether the use of different types of jars affects the efficiency of the predator. This is the experimental design:\nResponse variable - Number of prey caught (Ha)\rTotal foraging time - 1 minute, a constant (T)\rPredictor variables:\rPrey density - user defined treatments (H)\rType of jar used - jar with a lid or no lid (yes or no)\rNormally the students will collect data to parameterise the functional response. Instead I’ve created a function that will predict new values of prey captured using a functional response formula that is already parameterised.\nLet’s look at the function:\nAn interactive function\rR has some capacity to be interactive. It can ask a user to input variables.\rThe function functional_response will return the number of prey captured for a given prey density and type of jar used. The function will ask for these two inputs each time the function is run. Look at the code below and try to understand the different components. Then try running the code yourself with different inputs.\n#### Interactive function\r# Simulation of the predator-prey functional response - DO NOT CHANGE\r# To run: click Source (cmd or ctrl+shift+S) or Run All (ctrl+alt+r)\r# Or source(\u0026quot;\u0026lt;insert file location here\u0026gt;\u0026quot;, echo = FALSE)\rfunctional_response \u0026lt;- function(){\r# Introduce the simulation to the user - prints a message\rcat(paste(\r\u0026quot;\u0026quot;,\r\u0026quot;You have some counters (prey), a piece of A4 paper \u0026amp; a jar.\u0026quot;,\r\u0026quot;You spread the counters randomly on the A4 sheet.\u0026quot;,\rsep=\u0026quot;\\n\u0026quot;))\r# Ask the user for the prey density for the functional response\rprey_density \u0026lt;- readline(\u0026quot;How many prey counters are used? \u0026quot;) # ask for prey density\r# Check the user has inputted a number properly\rif(!grepl(\u0026quot;^[0-9]+$\u0026quot;, prey_density)){ # check whether the input contains numeric characters between 0-9 using regex (regular expressions)\rmessage(\u0026quot;Please enter an integer\u0026quot;) # If the input is not a number, tell them to do it again\rreturn(functional_response()) # Return to the beginning of the function and start again\r}\rprey_density \u0026lt;- as.integer(prey_density) # If the prey density input is a number, turn it into an interger\r# Ask the user for the type of jar used\rcat(paste(\r\u0026quot;\u0026quot;,\r\u0026quot;Every second you (the predator) tap the sheet to find and pick up a prey counter while blindfolded.\u0026quot;,\r\u0026quot;You have 1 minute to put as many prey counters as you can in the jar.\u0026quot;,\r\u0026quot;There are two types of jars you can use while handling prey.\u0026quot;,\r\u0026quot;Enter 1 to use a jar with a lid that you have to open and close.\u0026quot;,\r\u0026quot;Enter 2 to use a jar without a lid.\u0026quot;,\r\u0026quot;\u0026quot;,\r\u0026quot;What type of jar is used?\u0026quot;,\rsep=\u0026quot;\\n\u0026quot;))\rlid_used \u0026lt;- menu(c(\u0026quot;Lid\u0026quot;, \u0026quot;No Lid\u0026quot;))\r# Calculate the number of prey caught (the functional response) based on the user defined input above\rif(lid_used != 0){ # Check that the use has chosen the jar used properly (1 or 2)\r# Use this model if using a jar with a lid\rif(lid_used == 1){\rHa \u0026lt;- (0.2 * prey_density)/(1 + 0.2 * 0.03 * prey_density) }\r# Use this model if using a jar without a lid\rif(lid_used == 2){\rHa \u0026lt;- (0.7 * prey_density)/(1 + 0.7 * 0.05 * prey_density) }\r# Add in some variation around the predicted value so that users don\u0026#39;t get the exact parameterised functional response\rHa \u0026lt;- Ha + sample(seq(-3,3), 1) # Make sure there are no negative prey items caught!\rif(Ha \u0026lt; 0){\rHa \u0026lt;- 0 # Make prey caught 0 if less than 0\r}\r# Make sure the number of prey caught doesn\u0026#39;t exceed the number of prey available!\rif(Ha \u0026gt; prey_density){\rHa \u0026lt;- prey_density # If prey caught is greater than the number of prey available, make it the maximum possible\r}\r# Print a message showing the results\rmessage(\u0026quot;The number of prey caught is \u0026quot;, floor(Ha), \u0026quot;. Well done!\u0026quot;) }\r}\r# Actually run the function and tell R that it\u0026#39;s interactive if(interactive()) functional_response()\rNote:\nThe function doesn’t have any inputs in function() because it will ask the user for them each time\rreadline is the function to ask for a single user input\rmenu is the function to ask the user to chose from a number of options\rHere the option is press 1 to use a jar with a lid or press 2 to use a jar without a lid\rThere are two parameterised functional responses - one for a jar with a lid and one for a jar without a lid\rfloor is a function to round the number of prey caught to the lowest whole number\rIn the practical, students will need to run the above function for 10 prey densities, repeated 3 times, for both jar treatments - so 60 times in total. But we don’t have to do that manually - that is what loops are for!\nFunctions and loops\rHere is a non-interactive version of the function above. It doesn’t have the printed messages asking for user input. This time, the function needs 3 inputs as indicated by function(prey_density, lid_used, total_time): the prey density used, the type of jar used and the total foraging time, respectively.\nRun the code chunk to load the function into the R environment:\n# Functional response function\rfunctional_response \u0026lt;- function(prey_density, lid_used, total_time){\r# Check jar type is inputted correctly\rif (!lid_used %in% c(\u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;)) {\rstop(\u0026quot;Lid used is not inputted correctly. Use \u0026#39;yes\u0026#39; or \u0026#39;no\u0026#39; in all lowercase\u0026quot;)\r}\rif(lid_used == \u0026quot;yes\u0026quot;){\rHa \u0026lt;- (0.2 * prey_density * total_time)/(1 + 0.2 * 0.03 * prey_density)\r}\rif(lid_used == \u0026quot;no\u0026quot;){\rHa \u0026lt;- (0.7 * prey_density * total_time)/(1 + 0.7 * 0.05 * prey_density)\r}\rHa \u0026lt;- Ha + sample(seq(-3,3), 1)\rif(Ha \u0026lt; 0){\rHa \u0026lt;- 0\r}\rif(Ha \u0026gt; prey_density){\rHa \u0026lt;- prey_density\r}\r# message(\u0026quot;The number of prey caught is \u0026quot;, floor(Ha))\rreturn(floor(Ha))\r}\rThe function will check that the character vector indicating the type of jar to be used is correct because R is case sensitive. The function will return the number of prey caught Ha as indicated by return(Ha). The function return specifically tells R to tell us the output, otherwise R will keep it to itself! Only one output is allowed (unless extra steps are taken).\rThe rest of the function is the same.\nUsing the functional response function in a loop\rNow let’s use a loop to do our entire experiment in one go! No manual inputs for us. In fact we can do as many treatments or replicates as we want. Let’s do prey densities between 5 and 100 in increments of 5 and 3 replicates. Since the function inputs are required we can set them up in the environment for the function:\n# Set parameters for the function\rtotal_time \u0026lt;- 1 # total foraging time in minutes\rno_treatments \u0026lt;- seq(from = 5, to = 100, by = 5) # prey density treatments between 5 and 100\rreplications \u0026lt;- 3 # number of replications\r# a numeric vector of prey densities for all treatments, jar types and replications prey_density \u0026lt;- rep(rep(no_treatments, replications),2) # repeated twice for each jar type\r# a character vector of the jar type\rlid_used \u0026lt;- sort(rep(c(\u0026quot;no\u0026quot;, \u0026quot;yes\u0026quot;), length(prey_density)/2)) # \u0026quot;yes\u0026quot; or \u0026quot;no\u0026quot;\rBy setting up the parameters outside the function or loop, we can easily modify the parameters of the function and feed the new values into the loop. This helps us debug and is cleaner and easier to read.\nNow for the actual loop itself. We need to be able to store the output of the looped function.\rLists are the fastest way to do so in R because R is designed for lists and matrices.\n# Create an empty list called prey_caught to populate prey_caught \u0026lt;- list()\rfor(i in seq_along(prey_density)){\r# Run the functional response for the nth observation in the vector prey_density representing all our observations and save it to the list\rprey_caught[[i]] \u0026lt;- functional_response(prey_density = prey_density[i],\rlid_used = lid_used[i],\rtotal_time = total_time)\r# Prepare the list for further analysis\rprey_caught[[i]] \u0026lt;- cbind(prey_caught[[i]], prey_density[i]) # add a column for the prey density used to calculate the number of prey caught for that observation (row)\rprey_caught[[i]] \u0026lt;- cbind(prey_caught[[i]], 1/prey_caught[[i]]) # add a column for the inverse of the number of prey caught - for statiscally parameterising the functional response\rprey_caught[[i]][,3] \u0026lt;- ifelse(prey_caught[[i]][,3] == Inf, 0, prey_caught[[i]][,3]) # Housekeeping - turn undefined values of prey caught (from calculating 1 divided by 0) into 0. If the number of prey caught was 0\r}\r# Turn our list into a data frame\rprey_caught \u0026lt;- data.frame(do.call(\u0026quot;rbind\u0026quot;, prey_caught))\r# Label the columns\rcolnames(prey_caught) \u0026lt;- c(\u0026quot;Ha\u0026quot;, \u0026quot;H\u0026quot;,\u0026quot;Ha.1\u0026quot;, \u0026quot;HT.1\u0026quot;) # \u0026quot;.1 is inversed columns\u0026quot;\r# Add type of lid used to our data frame\rprey_caught$lid_used \u0026lt;- lid_used\rHere we are using a for loop with the nth observation denoted i. This can be called anything you want but i is from mathematical notation.\rseq_along is a useful function for telling which observation you are at for a vector - try it out on its own.\rLists can only contain one variable type, unlike a data frame, but that’s why they are fast and memory efficient for high performance computing\rIf we were to add the jar type (a character vector) to the list within the loop, then all our numeric output will be converted to characters (default R behaviour) - not what we want!\rdo.call is a handy function for lists. It collapses multidimensional lists into one dimension, here appending them by rows (i.e. adding new observations by rows at the end)\rVisualising the data\rNow we can plot our results and conduct our linear regression:\nlibrary(tidyverse)\rprey_caught %\u0026gt;% # Absolute values\rggplot(aes(H, Ha, colour = lid_used)) +\rgeom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE, fullrange=TRUE) +\rlabs(x = expression(paste(\u0026quot;Prey density (H)\u0026quot;)),\ry = expression(paste(\u0026quot;Prey captured (Ha)\u0026quot;)),\rcolour = \u0026quot;Jar used\u0026quot;) +\rtheme_classic()\r# Linear model\rsummary(lm(Ha.1 ~ HT.1 * lid_used, prey_caught))\rCall:\rlm(formula = Ha.1 ~ HT.1 * lid_used, data = prey_caught)\rResiduals:\rMin 1Q Median 3Q Max -0.27245 -0.05371 -0.01596 0.00899 0.84476 Coefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.07136 0.02262 3.155 0.00204 **\rHT.1 1.00548 0.40029 2.512 0.01338 * lid_usedyes 0.09517 0.03198 2.976 0.00356 **\rHT.1:lid_usedyes -1.11833 0.56609 -1.976 0.05058 . ---\rSignif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rResidual standard error: 0.1351 on 116 degrees of freedom\rMultiple R-squared: 0.08913, Adjusted R-squared: 0.06558 F-statistic: 3.784 on 3 and 116 DF, p-value: 0.01242\rThat has saved us from running the code 120 times! Now we can do it in one!\n","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"e2b5110073a51d6f0b084ea324679b78","permalink":"https://jacintak.github.io/post/2021-06-01-r-function-loops/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/post/2021-06-01-r-function-loops/","section":"post","summary":"A tutorial about interactive functions and loops in R","tags":["teaching","R stats","code","NERD club"],"title":"R Club: Functions \u0026 Loops","type":"post"},{"authors":[],"categories":["code"],"content":"\rThis post on R bloggers describes a handy function for formatting really small P values in ANOVA tables (more than 3 decimal places) with \\(\u0026lt;0.001\\). I find this easier to read when I need to present a formatted table, e.g. in teaching.\nThe original function doesn’t cover all ways of creating ANOVA tables in R so I have extended the function to cover more cases. The fixp function below will work for ANOVA tables (x) generated by anova(lm(...)) and summary(aov(lm(...))), as well as the model coefficients table generated by coef(summary(lm(...))).\nA function to format P values\rfixp \u0026lt;- function(x, dig=3){\r# Convert to a data frame\rif(is.data.frame(x) | is.matrix(x)){\rx \u0026lt;- as.data.frame(x)\r} else {\rx \u0026lt;- as.data.frame(x[[1]])\r}\r# Check column order if(substr(names(x)[ncol(x)],1,2) != \u0026quot;Pr\u0026quot;){\rwarning(\u0026quot;The name of the last column didn\u0026#39;t start with Pr. This may indicate that p-values weren\u0026#39;t in the last row, and thus, that this function is inappropriate.\u0026quot;)\r}\r# Round P values to \u0026quot;dig\u0026quot; decimal places, default 3 x[,ncol(x)] \u0026lt;- round(x[,ncol(x)], dig)\r# for(i in 1:nrow(x)){\rif(x[i,ncol(x)] == 0 \u0026amp; !is.na(x[i,ncol(x)])){\rx[i,ncol(x)] \u0026lt;- paste0(\u0026quot;\u0026lt;0.\u0026quot;, paste0(rep(0,dig-1), collapse=\u0026quot;\u0026quot;), \u0026quot;1\u0026quot;)\r}\r}\rx\r}\rThe main modification to the original function is to expand the conversion of x to a data frame to accept lists and matrices. summary(aov(lm(...))) creates an object with class summary.aov which is a list and the coefficients table is a matrix. Although anova(lm(...)) creates a data frame that will work with the function without a fatal error, the function anova has its own way of “pretty” printing [to quote the help file] which is not compatible with the character vector in the P value column and thus will show a P value of 1. So forcing to a data frame is necessary. A minor modification is to ignore the NAs in the Residual row created by the data frame which would otherwise give an error.\nThe three decimal places for P values is coded into the function by default and can be changed by the dig option. For example, dig = 1 will give you \\(\u0026lt;0.1\\). You can then call your ANOVA table and the fixp function through knitr::kable() or your favourite HTML/LaTeX table formatter. e.g. kable(fixp(anova(lm(...))), digits = ...). If you don’t want to print NAs, it’s probably better to use the options in your chosen formatting function - e.g. the knitr.kable.NA option in kable.\n","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"f339a7c6751f3f89f044087a04fd2da2","permalink":"https://jacintak.github.io/post/2021-05-01-formatting-p-values/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/post/2021-05-01-formatting-p-values/","section":"post","summary":"An R function for formatting really small P values to print ANOVA or regression coefficient tables","tags":["R stats","code"],"title":"Formatting P values","type":"post"},{"authors":null,"categories":[],"content":"\rWhere do animals live and why? These are some of the questions that ecologists are interested in. Sure, we can talk about patterns of abundance in an area in terms of abiotic or biotic factors or niche variation. But what if there’s more to animals than that?\nWhat if a young animal is concerned not just about eating, being eaten and living to reproduce but also with their finances, housing, commute and social mobility? What if a larger or older individual lives where they live not because they can outcompete smaller individuals for limited resources, but because they have accrued greater capital over time and thus have higher purchasing power?\nNone of these questions are answered by current ecological theory. We need an alternative explanation for animal distributions and abundances. Here, I present to you the socio-economic theory of animal abundance. I illustrate this theory using the Australian ghost crab (Ocypode cordimana) as a case study.\nA case study on ghost crabs\rGhost crabs are a common intertidal species in tropical sandy beaches. Forget beach front digs with a sea view. They have literal digs on the beach. What’s more, Ocypode cordimana is a species of ghost crab that lives on K’garri (Fraser Island), Australia, so they have prime access to the largest sand island in the world in the luckiest country in the world.\nOn K’garri, O. cordimana burrows are distributed from the low tide mark to the high sand dunes. The size of the burrows are indicative of the size of the home owner. Larger holes and thus larger crabs are found further up the beach from the shore. Higher densities of smaller holes are found closer to the low tide mark and lower densities of larger holes are found further away from the shore.\nA boring ecologist might hypothesise that this size-abundance gradient is explained by intraspecific competition or gradients of vegetation cover. But here is the socio-economic explanation:\nGhost crabs are nocturnal (hence, ghosty; not because they are terrible friends – or are they?). At night they head down to the low tide line and forage among the detritus. This is the Central Business District (CBD) where crabs do their biz and partay. Ghost crabs need to commute every day and, like all reasonable beings, they want to avoid the rush hour traffic least they succumb to road rage. And it would not do to be seen participating in such vulgar behaviour in this day and age. Contests are better left to the dishonest fiddler crabs – the cheaters. Rather, all crabs openly carry weapons (claws) as a deterrent via mutually assured destruction.\nA schematic of a ghost crab. Not to scale.\nCrabs could minimise their commute and live close to the CBD. But living here is dangerous. The sand is fine – poor digging quality so only small houses can be built. The close proximity to the CBD and the ocean means the area gets inundated at high tide, which makes insurance premiums go through the roof. Housing density is high and competition for space is fierce. No-one enjoys hearing their neighbours through the fine grain sand. The only crabs that can live here are small, young crabs (including grad students) who can only afford to live in these inner city slums and dream of living further away from the gangland crime.\nIn more recent times, the inner city has been undergoing gentrification. Young working professional crabs (Yuppies) and two-crab social groups with double incomes and no kids (DINKs) have been attracted by the convenience of the commute and short distance to local amenities. These crabs are larger than the typical inner city crab, have the income to create quality residences, and think the incoming tide adds character and charm to their property. These crabs enjoy an overpriced flat white with their avo on toast. You’ll often find them scurrying about to their barber appointments for their frothy bubble beards (plaid not included).\nThe socio-economic theory of ghost crab abundance on K’garri\nBeyond the inner city transition zone lies the urban sprawl known as suburbia. Suburbia is where the hopes and dreams of the young go to die and are replaced with a well manicured lawn. Here, the sand is not as wet, infrequently gets inundated, and one could afford to build a large home. The commute to the CBD is a little longer than the inner city but there’s the best of both worlds as access to the world class foredunes is equidistant away. Perfect for the weekend escape from the mediocrity.\nFinally, at the base of the dunes are the largest houses. The coarse sand and the roots of the foredune vegetation permit the largest burrows worthy of the largest and wealthiest crabs. Their commute is the longest but they don’t care about that, if they even need to commute at all. Their elevated position on the dune slope gives them the greatest vistas of the population and they live on the urban-rural fringe with easy access to silver-green, xerophytic spaces.\nBeyond the dune crest lies The Sticks as the dune transitions to woodland containing, you guessed it, sticks. Not the kind of place for crabs so few crabs are found there.\nAnd that is the socio-economic theory of animal abundance applied to ghost crabs.\nHappy April Fools. There is no intellectual basis for applying the concentric zone model of cities to explain real ecological patterns. I initially conceptualised this during my undergrad ecology field trip to K’garri many years ago. This post is dedicated to Prof. Gimmie Walter, who heard it first – Happy retirement!\nThis post was originally published on EcoEvo@TCD on 1st April 2021.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"511e7e792e67d878cc129443c4a237ba","permalink":"https://jacintak.github.io/post/2021-04-01-the-socio-economic-theory-of-animal-abundance/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/post/2021-04-01-the-socio-economic-theory-of-animal-abundance/","section":"post","summary":"What if there's more to animals than what current ecological theory thinks?\n","tags":["fun","ecology","crabs","theory"],"title":"The socio-economic theory of animal abundance","type":"post"},{"authors":[],"categories":["code"],"content":"\rIf it ain’t broke, don’t fix it?\rSo you’ve spent a lot of time learning and practising R and you’re pretty comfortable with using functions, if else statements and loops like they teach at introductory programming. What more is there to improve?\nIf the answer is no or you subscribe to the quote above, then turn back now. If yes, continue.\nI think that even if one has the skills to do fundamental programming competently, there’s always room for improvement or something new to learn. Or you know that there’s a better, more efficient, way to do it but something is holding you back. For me, it’s usually the latter.\nIn a milestone of using R I think I have wrapped my head around replacing for loops with the apply family, specifically mapply. The last hurdle in delving into functional programming.\nI’ve used iterative coding quite a bit over the years and I’ve been using for loops to do so. As I’ve gotten more competent with applying basic concepts (like loops and functions), I’ve been moving towards optimising my code with more advanced R methods. I started with using more manual functions and sourcing functions from external scripts but I was still relying on loops to apply those functions iteratively.\nI know loops are inefficient. I’ve waited days for computationally intensive loops on large datasets to finish. I know that apply and co. can be more computationally efficient but in your typical learning something new way, they hadn’t really clicked for me…until now.\nI’ve been trying to use apply family functions where appropriate for years but I’ve never felt comfortable with using them to use them from the start. So, I default back to loops to save time and frustration.\nI think the slow uptake is because the syntax is different to the logic of loops that are taught, even if apply’s logic is better from a computing perspective. The syntax and the logic is also inconsistent within the apply family; a known disadvantage over similar functions (like purrr::map).\nBut let’s focus on a specific case before this becomes a cooking blog: replacing for loops. I’m going to assume that you are competent with manual functions, for loops and lists, and that you want to improve your code. I’m going to focus on lists because they are an efficient way of storing lots of similarly structured data in R.\nHere are two ways to replace a for loop.\nAn example loop\rLet’s create an example scenario and data:\n# some data to use\rloop_data \u0026lt;- data.frame(col1 = c(11:15), col2 = c(20:24))\r# define variable to change\ra \u0026lt;- seq(0.2, 1, 0.2)\rloop_data is a data frame with two numeric columns (col1 \u0026amp; col2). We technically won’t use loop_data$col2 but it’s there to create a 5x2 data frame.\ra is a variable that we need for our function. There are 5 values.\rWe want to add each element of a to loop_data$col1 and save that in a new column loop_data$col1a. We will also add a as a column in loop_data just so we can keep track of which value was used to calculate col1a. So the final output should have 25 rows (5 observations in loop_data x 5 values of a) and 4 columns (col1, col2, col1a, a).\nWe will be storing our data in lists in all our scenarios. Note that I create the list to hold the answers (loop_ans) before the function rather than to append newly calculated answers sequentially to the list within the function. I use the same replicate function before all the examples. You could also start with an empty list.\n# data sets stored as a list - must not simplify or it will reduce to a matrix!\rloop_ans \u0026lt;- replicate(length(a), loop_data, simplify = FALSE)\r# A function to add a value a to a data frame x\rloop_function \u0026lt;- function(x, a) {\rx$col1a \u0026lt;- x$col1 + a # add answer to a new column x$a \u0026lt;- a # add a to a new column\rreturn(x) # give us the updated data frame\r}\r# Let\u0026#39;s loop\rfor(i in seq_along(a)){\rloop_ans[[i]] \u0026lt;- loop_function(loop_ans[[i]], a = a[i]) }\r# merge to single data frame\rloop_ans \u0026lt;- do.call(rbind, loop_ans)\r# view the data\rsummary(loop_ans)\r## col1 col2 col1a a ## Min. :11 Min. :20 Min. :11.2 Min. :0.2 ## 1st Qu.:12 1st Qu.:21 1st Qu.:12.4 1st Qu.:0.4 ## Median :13 Median :22 Median :13.6 Median :0.6 ## Mean :13 Mean :22 Mean :13.6 Mean :0.6 ## 3rd Qu.:14 3rd Qu.:23 3rd Qu.:14.8 3rd Qu.:0.8 ## Max. :15 Max. :24 Max. :16.0 Max. :1.0\rThat’s the loop - should be familiar to you. Merging into a single data frame is optional if you want to keep using lists. Now let’s look at lapply for a less elegant solution (!).\n1. lapply\rlapply takes a list as input, does stuff and gives a list as output. Hence, the l in lapply stands for list. The difference with loops and lapply is that lapply can only take one input - your data frame (or element in list). This means that we need to add the corresponding value of a as a column in each element of lapply - in other words to do part of what loop_function did but outside the loop/lapply. Thus, each data frame in the input list should have three columns: col1, col2 \u0026amp; a.\nIncidentally, we can add the corresponding a value as a column using mapply and cbind.\n# the function only accepts one element: x\rlapply_function \u0026lt;- function(x){\rx$col1a \u0026lt;- x$col1 + x$a\rreturn(x)\r}\r# Prepare the answer list\rlapply_ans \u0026lt;- replicate(length(a), loop_data, simplify = FALSE)\r# add a column using mapply\rlapply_ans \u0026lt;- mapply(FUN = cbind, lapply_ans, \u0026quot;a\u0026quot; = a, SIMPLIFY = FALSE)\r# apply function\rlapply_ans \u0026lt;- lapply(lapply_ans, FUN = lapply_function)\r# merge to single data frame\rlapply_ans \u0026lt;- do.call(rbind, lapply_ans)\r# view the data\rsummary(lapply_ans)\r## col1 col2 a col1a ## Min. :11 Min. :20 Min. :0.2 Min. :11.2 ## 1st Qu.:12 1st Qu.:21 1st Qu.:0.4 1st Qu.:12.4 ## Median :13 Median :22 Median :0.6 Median :13.6 ## Mean :13 Mean :22 Mean :0.6 Mean :13.6 ## 3rd Qu.:14 3rd Qu.:23 3rd Qu.:0.8 3rd Qu.:14.8 ## Max. :15 Max. :24 Max. :1.0 Max. :16.0\rAs you see it’s not as simple as the loop or mapply and requires mapply anyway 🤷\nSo we can do better…\n2. mapply\rThe m in mapply stands for multiple because it takes multiple arguments and applies them to the data. There are some key differences in the structure of the data and the function compared to lapply:\nWe can use the original loop function with two variables!\rThe additional variables (a in this example) are written after the function FUN is defined in mapply\rWe can also use the original list (loop_data) without further modification!\rWe need to tell mapply not to simplify the output into a matrix by default. Note the use of upper case in SIMPLIFY.\r# Prepare the answer list\rmapply_ans \u0026lt;- replicate(length(a), loop_data, simplify = FALSE)\r# mapply function\rmapply_ans \u0026lt;- mapply(mapply_ans, FUN = loop_function, a = a, SIMPLIFY = FALSE)\r# merge to single data frame\rmapply_ans \u0026lt;- do.call(rbind, mapply_ans)\r# view the data\rsummary(mapply_ans)\r## col1 col2 col1a a ## Min. :11 Min. :20 Min. :11.2 Min. :0.2 ## 1st Qu.:12 1st Qu.:21 1st Qu.:12.4 1st Qu.:0.4 ## Median :13 Median :22 Median :13.6 Median :0.6 ## Mean :13 Mean :22 Mean :13.6 Mean :0.6 ## 3rd Qu.:14 3rd Qu.:23 3rd Qu.:14.8 3rd Qu.:0.8 ## Max. :15 Max. :24 Max. :16.0 Max. :1.0\rWhat mapply is doing is using the nth element of a with the corresponding nth element in the list loop_data. So the fifth value of a (1.0) is used in the calculations on the 5th data frame in loop_data.\nWe’ve replace the for loop with a mapply function! 👏\nHere’s to functional programming. Next up is purrr::map…\n","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"448df46b72ab89a781756c38dbaeb701","permalink":"https://jacintak.github.io/post/using-mapply/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/using-mapply/","section":"post","summary":"How I learnt to love replacing for loops with apply","tags":["code","R stats"],"title":"Leaving the valley of intermediate competence","type":"post"},{"authors":null,"categories":null,"content":"\rR/exams is an R package that generates a reproducible workflow for designing, producing and marking exams.\nHere, I provide a short walkthough for generating an online test for Blackboard - see the R/exams website for some tutorials.\nInstallation\rInstall R/exams via CRAN with install.packages(\"exams\").\nRunning the package for the first time\rexams uses rtools to create zip files. Make sure the proper rtools is installed. An error message will appear if rtools does not have permission to create zip files.\nTo give permission for creating .zip for windows:\rControl Panel \u0026gt; System and Security \u0026gt; System \u0026gt; Advanced System Settings \u0026gt; Environmental Variables \u0026gt; add “C:/RTools/bin”\nYou may also need to install dependent packages like tth for math notation.\nTypes of questions\rSingle correct answer MCQ (schoice)\rMultiple correct answer MCQ (mchoice)\rNumeric answer (num)\rString (string)\rCloze is another option but is not supported by blackboard, this permits a combination of the above for the MCQ answer list\nBuilding an exam question\rThere are several file types that are supported when writing your question but I have stuck with ‘.Rmd’. There are four parts of a question file:\nThe question\rThe answer list\rThe solution list for giving feedback\rThe meta-information\rEach of these sections are defined by a header tag marked by =====. # does not work as a tag.\nQuestion\rThe tag for defining the question is:\nQuestion\n========\nThe various random number generators in R will be your friend.\nsample() # pick n random number(s) from a vector of discrete numbers\rrunif() # generate a vector of continuous numbers, can set min and max\rrnorm() # generate a vector of continuous numbers from a normal distribution with a defined mean and standard deviation\rThings to note\rCode chunk options are in effect. By default figures will have captions, turn it of with fig.caption = \"\". Other useful figure options are figure sizes. Figure options can also be defined when compiling the exam\rYou can show code with echo = TRUE, turn off R formatting with results = \"asis\" \u0026amp; hide results with results = \"hide\"\rLaTeX formatting is fine in markdown and outside code chunks\rTo allow for randomised questions within a question file I had to escape the R code chunk to render the output as html, else the randomised question would be rendered as R output\rYou can generate a file to go with the question using any R write to file function. Include the file in the question as normal for Rmarkdown - [filename](filelink). Leave this as default or exams will not be able to find the file\rThe answer list\rThe answer subheading is defined by the tag:\nAnswer\n========\nBulleted markdown after this tag will be considered the options for an MCQ answer list.\nThings to note\rexams contains several helper functions to make it easy to generate lists of answers.\nanswerlist accepts a vector of answers. It also generates the answer subheading so there is no need to type it in.\rRecommend using html in case LaTeX does not render properly, particularly when called as a string in an R code chunk.\nDo not randomise the answer list here, use the metadata, else an incorrect answer will be assigned to be the correct one.\ncomment = NA in the code chunk options will remove the # from the R output.\nThere are also helper functions for various things. mchoice2string() turns the solutions vector above into binary responses for the meta-information section. num_to_choice generates a MCQ list of numbers for a numeric answer.\rSolution\rYou can provide feedback via the solution header, including which answers are correct\nSolution\n========\nYour solution here or correct answer: code for answer (or answer[])\nAnswerlist\n-———\n* True\n* False\n* etc.\nImportant There should be no spaces after the header tag title, i.e. markdown formatting\nQuestion metainformation\rThis is an important section of the question because it defines the correct answer. Metainformation is defined by the tag:\nMeta-information\n================\nUseful variables are:\nexname = title of question, becomes name of the pool in blackboard\rextype = type of question (num/schoice/mchoice)\rexsolution: order of correct answers in binary (e.g. 01010) for MCQ or R code for numeric output - e.g. extol = the tolerance range for numeric questions\rexshuffle = Whether to shuffle the answers or not. This can be used to randomly select a subset of answers from an answer list. Provide a number of answers for non-numeric questions (e.g. 4 for 4 answers). TRUE/FALSE is also accepted.\rAn example question\rThis example displays the correct answer and 3 randomly chosen option out of 6 possible answers.\nQuestion\r========\rWhat is your name?\r\\```{r question, results = \u0026quot;hide\u0026quot;, echo=F}\r# list of possible answer as a character vector\rknights_of_camelot \u0026lt;- c(\u0026quot;Arthur, King of the Britons\u0026quot;, \u0026quot;Sir Lancelot the Brave\u0026quot;,\r\u0026quot;Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot\u0026quot;,\r\u0026quot;Sir Galahad the Pure\u0026quot;,\r\u0026quot;Sir Bedevere the Wise\u0026quot;,\r\u0026quot;Patsy\u0026quot;)\r# solution to the vector above solutions \u0026lt;- c(FALSE, TRUE, FALSE, FALSE, FALSE, FALSE)\r# explanations (feedback for students)\rexplanations \u0026lt;- c(\u0026quot;I didn\u0026#39;t vote for him\u0026quot;,\r\u0026quot;His favourite colour is blue\u0026quot;,\r\u0026quot;He doesn\u0026#39;t know the capital of Assyria\u0026quot;,\r\u0026quot;His favourite colour is blue. No, yel...\u0026quot;,\r\u0026quot;Knows nothing about swallows\u0026quot;,\r\u0026quot;Clip Clop\u0026quot;)\r\\```\r\\```{r answerlist, echo=F, results = \u0026quot;asis\u0026quot;}\r# helper function to format the list of possible answers\ranswerlist(knights_of_camelot, markup = \u0026quot;markdown\u0026quot;)\r\\```\rSolution\r========\r\\```{r solutionlist, echo = FALSE, results = \u0026quot;asis\u0026quot;}\ranswerlist(ifelse(solultions, \u0026quot;True\u0026quot;, \u0026quot;False\u0026quot;), explanations, markup = \u0026quot;markdown\u0026quot;)\r\\```\rMeta-information\r================\rexname: Bridgekeeper\rextype: schoice\rexsolution: `\\r mchoice2string(solutions)`\rexshuffle: 4\rThis renders like this:\nExam 1\rQuestion\rWhat is your name?\rSir Bedevere the Wise\rArthur, King of the Britons\rSir Galahad the Pure\rSir Lancelot the Brave\rSolution\rFalse. Knows nothing about swallows\rFalse. I didn\u0026#39;t vote for him\rFalse. His favourite colour is blue. No, yel.\rTrue. His favourite colour is blue\rYou could also skip the solutions vector and include it in the explanations vector like c(\"False. I didn't vote for him\"). Of course, the solution does not appear immediately in blackboard but make sure the option for solutions and feedback to appear is checked.\nCompiling the exam\rI have written a script to compile the exam.\rThe compiling function (exams2blackboard) requires a list of file names to generate the exam. Each file represents a question. Versions of a question (n) generate a pool of questions. The list should not contain subdirectories or files not to be included in the exam. exams2html is a means of checking a file/list of exam questions renders properly in html (or exams2pdf).\nThere are several means of customising the metadata of the exam. Here I have:\nturned off partial marks - is TRUE by default\rused custom directories to search for the questions and save the output zip.\rset 10 copies for each question using the variable n\rset the name of the zip file using the variable name\rset the number of points for each question to 1, default = 10\rlibrary(\u0026quot;exams\u0026quot;)\roptions(device.ask.default = FALSE)\r## content and output directory\rmydir \u0026lt;- \u0026quot;C:/Users/kongj/OneDrive - TCDUD.onmicrosoft.com/Teaching/Biostats/Midsem MCQ\u0026quot;\r## define exam questions (each item in list is a pool)\rmyexam \u0026lt;- list.files(paste0(mydir,\u0026quot;/questions\u0026quot;), pattern = \u0026quot;.Rmd\u0026quot;)\r# render single question\r#exams2html(list(\u0026quot;question3.Rmd\u0026quot;), edir = paste0(mydir,\u0026quot;/questions\u0026quot;))\r## generate .zip with Blackboard exam with n replicates\rexams2blackboard(file = myexam, n = 10, name = \u0026#39;Jacinta\u0026#39;, dir = mydir,\redir = paste0(mydir,\u0026quot;/questions\u0026quot;),\reval = list(partial = FALSE, negative = FALSE),\rpoints = 1\r)\rThings to note\rYou can create a mix of questions in the exam by defining it in the list of questions but it is not recommended for generating pools of questions\rIf the exam is complied with no partial marks, then the blackboard exam will have no partial marks even if the option is checked within blackboard\rYou can generate a pool of questions by compiling a single question and uploading that zip file to Blackboard under “Import Pool”. This is useful for creating random block tests in Blackboard if the compiled zip file is not suitable as an test right away\r","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"5282367909d2d0b11aedadcd51872d29","permalink":"https://jacintak.github.io/teaching/rexams/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/teaching/rexams/","section":"teaching","summary":"Reproducible exams","tags":["teaching","R stats","code"],"title":"Quick guide to R/exams","type":"teaching"},{"authors":null,"categories":["code"],"content":"\rI’m not usually a dark background person but I’m open to the dark side. I wanted to make a solid coloured graph with a transparent background that would show up nicely but ggplot2 doesn’t have a set theme for that. A clean solid fill and transparency requires some specific customisation so here is a reproducible example for you using the built-in trees dataset:\ntree_graph \u0026lt;- ggplot(data = trees, mapping = aes(x = Height, y = Girth)) + geom_point(size = 0.5, colour = \u0026quot;#B8DE29FF\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE, col = \u0026quot;#B8DE29FF\u0026quot;) + geom_abline(intercept = 0, slope = 1, col = \u0026quot;white\u0026quot;, lwd = 0.5, lty = 2) + theme_classic() +\rtheme(plot.background = element_rect(fill = \u0026quot;transparent\u0026quot;, color = NA),\rpanel.background = element_rect(fill = \u0026quot;transparent\u0026quot;),\raxis.text = element_text(colour = \u0026quot;#B8DE29FF\u0026quot;, size = 8),\raxis.title = element_text(colour = \u0026quot;#B8DE29FF\u0026quot;, size = 8),\raxis.line = element_line(colour = \u0026quot;#B8DE29FF\u0026quot;),\raxis.ticks = element_line(colour = \u0026quot;#B8DE29FF\u0026quot;))\rggsave(tree_graph, filename = \u0026quot;tree_graph.png\u0026quot;, bg = \u0026quot;transparent\u0026quot;, type = \u0026quot;cairo\u0026quot;, width = 10, height = 10, dpi = 300)\rThere are a couple of generally useful elements added on purpose:\ngeom_smooth creates an automatically fitted linear model (defined using method = \"lm\"). I have turned off plotting the standard errors (on by default) and manually set the colour.\rgeom_abline is your standard straight line\rtheme is where the customisation begins:\rplot.background \u0026amp; panel.background are set to transparent\rThe various axis elements are set to the fill colour (a nice viridis green) and desired text size\rggsave specifies that the background is transparent and to save it using the Cairo engine (type = \"cairo\"). Cairo will create a vector based image so resizing the png isn’t an issue since the small font size is already defined.\rYou can also use cairo-png but the graph height and width options appear to be ignored.\rIf you don’t save it as a Cairo png, then the text will still have a white outline and won’t be a clean solid fill\r","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"1d72fb03f0dd36e9d9434e4189ad6b8e","permalink":"https://jacintak.github.io/post/2021-02-01-transparent-graphing-for-dark-backgrounds/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/post/2021-02-01-transparent-graphing-for-dark-backgrounds/","section":"post","summary":"Joining the dark side of ggplot","tags":["code","conference","talks","poster","R stats"],"title":"Transparent graphing for dark backgrounds","type":"post"},{"authors":["Jacinta Kong","J.-F. Arnoldi","A. L. Jackson","A. E. Bates","S. A. Morley","J. A. Smith \u0026 N. L. Payne"],"categories":null,"content":"","date":1610010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610010000,"objectID":"2877c41b350b5b6527267dde2925bc80","permalink":"https://jacintak.github.io/talk/IEA2021/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/talk/IEA2021/","section":"talk","summary":"The Irish Ecological Association Conference","tags":["postdoc","conference","ectotherms","temperature"],"title":"IEA2021","type":"talk"},{"authors":null,"categories":null,"content":"\rTools for forcasting or predicting\rResources for forcasting\rMacroclimate\rMicroclimate\rMicroclimate simulation packages for R\rThings to consider\rTraits\rR packages for modelling\rNicheMapR, DEB \u0026amp; biophysical ecology\rInternational Symposium and Thematic School on DEB Theory for Metabolic Organisation\rTools for forcasting or predicting\rI am interested in developing a toolbox for biologists to make trait-based predictions or forecasts about how ectotherms respond to environmental change that are grounded in theory. Fostering stronger links between our understanding of terrestrial ectotherm thermal biology and the aquatic realm is important to identify general patterns.\nResources for forcasting\rA good model needs good input data!\nA pretty important component of modelling species responses to environmental variables are the environmental variables themselves. In fact, it’s already half the data. Ideally, you would have climatic data at the scale of the target organism, which I will generally refer to as microclimate, but this is often a challenge to acquire. But if you have access to climate data, e.g. gridded weather stations which I will refer to as macroclimate, then there are some solutions for generating microclimate data.\nMost experimental biologists will have access to plenty of data on organism traits but may not have the right environmental data for a model. The good news is with the increasing ease of generating and storing big data, data is now more accessible than ever before!\nMacroclimate\rIf you don’t have access to gridded weather station data for your area of interest, there are some publicly available datasets online for various environmental variables at various scales and resolutions:\nWorldClim\rCliMond with CLIMEX and Bioclim datasets\rClimatic Research Unit\rMicroclimate\rThere are currently a few publicly available datasets of pre-calculated microclimate grids. All these examples were made using NicheMapR:\nmicrolim for global scale\rMicroclimOz for Australia only (the lucky country)\rMicroclimUS for USA\rMicroclimate simulation packages for R\rThere are a few packages for R to simulate microclimates from gridded macroclimate data:\nNicheMapR\rMicroclima\rTrenchR\rThe one I used in my work is NicheMapR. The default function to generate microclimate in NicheMapR uses the Climatic Research Unit dataset to generate default microclimate output. I used input data at a higher spatial and temporal resolution than the default setting, in addition to querying gridded soil type data. Although the input data to run my scripts is not available, NicheMapR was used to generate the microclim and MicroclimOz datasets which are publicly available under certain soil type parameters.\nThings to consider\rCheck if the environmental datasets are at the resolution and scale appropriate for your intended purposes. If you want to simulate microclimate, you need to make sure you have all the environmental variables needed for the microclimate package: temperature, precipitation, soil type, topography, wind speed, solar radiation etc. You may have to collate input data from multiple sources.\nTraits\rThere are even a few databases for organism trait data:\nThe Insect Developmental Database (IDD) contains temperature-rate data for mostly insect species of agricultural interest\rFor physiological limits GlobTherm is a great initiative but covers CtMax and CtMin only.\rA few recent metaanalyses have combed through the thermal biology literature so you don’t have to! Then you can extract the information you want. Isn’t open, reproducible science great? Here’s a recent example\rDell et al. 2013 complied a dataset on various biological/ecological traits measured at different tempertures for species from all habitats.\rR packages for modelling\rIn my PhD I wrote a custom function for my code to calculate development rate. But there is also a package for that!\ndevRate is a package with commonly used temperature-rate functions, from statistical functions to biophysical ones. You can get it from CRAN.\nThere is a similar package called rTPC. It’s not on CRAN but is available on GitHub.\nNicheMapR, DEB \u0026amp; biophysical ecology\rFortunately, the NicheMapR package is a pretty complete toolkit to model the biology of an organism based on its environment and available resources. Not only does it have a microclimate model but it also has an implementation of the standard DEB model and a biophysical heat and water budget model for ecotherms and endotherms. All that is missing are the correct input parameters.\nHeat and/or water budget equations are useful for calculating the body temperature of an organism but don’t measure growth or development. You could use the estimates of body temperture for calculating development/growth rates but for most small ectotherms that are isothermic, this seems a bit excessive.\nDEB parameters and Add-My-Pet\rYou can find standard DEB parameters at Add-My-Pet and there is a vignette in NicheMapR which goes through the ectotherm model, the DEB model and how to use the DEB parameters from Add-My-Pet in NicheMapR.\nInternational Symposium and Thematic School on DEB Theory for Metabolic Organisation\rInterested in learning Dynamic Energy Budget modelling? There is an international symposium, tele-course and school/workshop that is an intensive course in DEB and its applications. The symposium is held every two years. The most recent one was April 2019.\nWith the resources available you can pretty much develop a mechanistic model without collecting your own data.\nIf there are other useful datasets or links not listed here, let me know!\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"22846843e25e77b108339fefcfafa8e4","permalink":"https://jacintak.github.io/project/toolbox/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/toolbox/","section":"project","summary":"Forecasting the environmental physiology of ectotherms","tags":["ectotherms"],"title":"A toolbox for trait-based forecasting","type":"project"},{"authors":null,"categories":null,"content":"\rIntroduction\rbiostats.tutorials is an R package of learnr tutorials for introductory biostatistics and R at an undergraduate level.\nThe package is still in development so stay tuned for updates. You can checkout various tutorials about using R for NERD club (postgraduates and staff) at Trinity College Dublin here.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a63672db39b3f1207d056852b8d141d0","permalink":"https://jacintak.github.io/project/biostats-tutorials/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/biostats-tutorials/","section":"project","summary":"learnr tutorials for teaching undergraduate biostatistics","tags":["teaching","R stats","code"],"title":"Biostatistics tutorials","type":"project"},{"authors":[],"categories":[],"content":"\rWelcome to my revamped website!\rAfter banging my head against blogdown and Hugo on and off for several months (years), I think I understand enough about it to refresh my site. This has been my weekend coding hobby. It’s taken so long that the under-workings of Hugo and the Academic theme used here have changed in the meantime.\nSetting up a site is simple enough if you follow the instructions online. The template builds a skeleton of the website and all you need to do is replace the placeholder text with your information. Simple right?\nNot quite. The underlying file structure takes some getting used to and you’ll have to trust the magic as your site gets compiled. But once you pass that hurdle, the system is quite powerful and the results are neat.\nThere are many features to take advantage of. My interest was in having one site for code documentation and blogging as I was not satisfied with having a “regular” website (WordPress) with a blog and my code documentation separately on GitHub Pages.\nMy code documentation was also messing up my GitHub. I’m not a big fan of having documentation (e.g. a static website) lumped together with package files. I also didn’t like having an orphan branch for my documentation (no winners here). I can take advantage of the features of Hugo to move my static sites off their respective repositories and into one central one. It’s better this way as some of the static pages weren’t even relevant to the rest of the repo.\nThere are some features that are missing/not easily implemented compared to WordPress that I will miss. You can’t comment (without third party systems) or “follow” people, you can’t schedule posts and you can’t track views unless you set up Google Analytics or similar. The workflow is a bit tedious for fixing minor things (like spelling mistakes).\nI will keep blog posts on the WordPress site as an archive. I have linked to relevant blog posts in this site.\nThere are some peculiarities that require things to be done a certain way:\nThings I noted\rI changed the name of the “Public” folder to “docs” so that my site can be built from the “docs” folder on GitHub. This seems the simplest way to organise this. The alternative is to have the “Public” folder on a different branch.\rSometimes it’s a challenge to troubleshoot why things aren’t working they way you want. E.g. I couldn’t work out why the country wouldn’t show up in the document when including an address in YAML\rA header image/thumbnail can be included in the same folder as the content but it must be called “featured” or it won’t be recognised. File names are case-sensitive\rYou can have as many folders as you want called whatever but the (r)markdown file of the page must be called “index”\rIn the “Courses” feature the parent page must be called \"_index\" or it won’t work. 🤷\rRmd and the “Courses” feature don’t play nicely. Defining a table of contents via bookdown outputs in the YAML doesn’t work and calling toc: true directly in the YAML (following the guide) doesn’t work either.\rbookdown and blogdown don’t play nicely together in the same project either. blogdown will try to render the bookdown pages as a site rather than letting bookdown do its thing and make a nice gitbook.\rTo allow this behaviour, you need to make use of the static file builder (See below)\rDespite the template saying “Upcoming talks” Hugo doesn’t render things in the future (incl blog posts). You need publishDate in the YAML\remojis are a nice feature 😄\rUsing the static folder to render a gitbook\rTo render rmd files into another output than blogdown::html_page you can put the rmd in the “static” folder then write a script that compiles the site in a specific order. To permit a gitbook page within the site:\nSave the bookdown files in the “static” folder\rIt doesn’t work with files in the “content” folder, these will get the usual treatment\rCreate a folder called “R” in the root directory\rCreate an R script called “build.R” in the “R” folder\rAdd the render functions you need.\rblogdown::build_dir(\"static\") is a wrapper for plain rmarkdown::render() as is so it works best with simple files\rbuild_dir doesn’t work for our gitbook example because we want a gitbook that knits with bookdown::render_book, otherwise you will get a bunch of HTML files from the standard render function\rThere’s something funny going on with the working directories. Running render_book(\"static/index.rmd\") doesn’t work, neither does the full address. Instead I had to change the working directory for the function to find the right files. I’m not sure where the function is looking as the project working directory is the root directory.\rThe above means that all the rmd files for the gitbook also get copied into the “docs” folder. I don’t think it’s avoidable. It’s also s l o w e r to build the site because the gitbook gets rendered every time (unless you “comment it off” in the build.R script.\nI also have a line to render my CV rmd into a PDF saved into the static folder. That PDF is then copied to the “docs” folder so I have an updated CV without needing to manually create one every time I update the original rmd. I think this is pretty handy.\nHere’s what my build.R file contains:\n# Make CV PDF\rrmarkdown::render(\u0026#39;content/cv/index.Rmd\u0026#39;, output_format = rmarkdown::pdf_document(keep_tex = FALSE), output_dir = \u0026quot;static/files/\u0026quot;, output_file = \u0026quot;Kong_JD_CV.pdf\u0026quot;)\r# make gitbook\r# blogdown::build_dir(\u0026quot;static\u0026quot;) doesn\u0026#39;t work because we want a gitbook that knits with render_book\r# whereas build_dir uses rmakrdown::render() thus giving html files\rold \u0026lt;- getwd()\rsetwd(\u0026quot;static/teaching/GLM/\u0026quot;)\rbookdown::clean_book(clean = TRUE)\rbookdown::render_book(input = \u0026quot;index.Rmd\u0026quot;)\rsetwd(old)\rHaving a drafts folder\rHugo will not render draft blog posts by default but blogdown will still render the files for your local site and these files get pushed to GitHub. If you don’t want your repo to contain spoilers, then you need to separate your draft posts from the published posts.\nOne solution to stop blogdown from rendering .rmd files is to keep them in the “static” folder but Hugo will copy these files to the “public” (or “docs” folder in my case). I could not find an option to tell Hugo to ignore some files in the “static” folder. This does not solve our spoiler problem.\nWe can have a “draft” folder under “content” and tell Hugo to ignore it in the config.toml file (ignorefile) but that doesn’t stop blogdown from rendering the file.\nI haven’t found a solution to stop blogdown from rendering and Hugo from copying the file but Hugo doesn’t add every folder from the root directory to “Public”, only folders that match the template. So I have a folder called “drafts” which contains my drafts. blogdown will still render the files every time they are saved while using serve_site but they won’t interfere with the site itself. When I’m ready to publish them I can copy them to the “content/post” folder. I also added the “drafts” folder to my .gitignore.\nblogdown has a handy function to generate a new blog post. By default it will add the new files to “content/post” but I changed this to write directly to the “drafts” folder via the variable subdir and with a custom date that is used to name the folder:\nblogdown::new_post(ext = \u0026quot;.Rmd\u0026quot;, title = \u0026quot;test\u0026quot;, subdir = \u0026quot;../drafts/\u0026quot;, date = \u0026quot;2021-01-01\u0026quot;)\rblogdown does theoretically have a means of excluding files…\rIn blogdown::build_site there is a function (list_rmds) that lists files in the “content” folder and excludes files beginning with _:\nfiles = files[!grepl(\u0026quot;^_\u0026quot;, basename(files)) | grepl(\u0026quot;^_index[.]\u0026quot;, basename(files))]\rBut you’ll notice it doesn’t exclude files called _index without another ! in front of grepl. For example, if I have an file called _drafts.Rmd, then build_site will ignore it. But if I have a file called _index.Rmd, then build_site will render it. I don’t know what the behaviour of this is supposed to be so I’m not sure if it is a mistake.\nThis function is not present in preview_site which means that the live preview is going to build all your .Rmd files regardless and it will show up on the live preview.\nEither way, your drafts will still get pushed to GitHub unless you specify the files in .gitignore (e.g. **/_*.Rmd) so I wouldn’t say using _ in your file names is an easier option.\nOther customisations via “layouts/partials” templates\rBecause Hugo copies any folder in the root directory into “docs” which matches the theme template, it will override any files in the “themes” folder that matches the name of the folder in the root directory. This means that you can create custom templates without modifying the original template. Thus, having a folder called “layouts/partials” will override any “partials” templates within the “theme” folder.\nI have added some minor customisations to reflect personal preference:\nAdded markdown to the author list in page_metadata_authors.html so that I can customise my name and bold it in the list of authors under publications\rChanged the site footer to include blogdown\rChanged page_metadata.html to show both the last modified and published date. Last modified date is default.\rI’m not saying goodbye to the grasshoppers so my flavicon is a grasshopper emoji 🦗\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"80ce68f768381e67e27ecd2d3c2de49e","permalink":"https://jacintak.github.io/post/2021-01-01-new-year-new-look/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/post/2021-01-01-new-year-new-look/","section":"post","summary":"Welcome to my revamped website!","tags":["website","github","R stats"],"title":"New Year, New Look!","type":"post"},{"authors":null,"categories":null,"content":"Move. Adapt. Die. Ectotherms have adapted their entire life cycle to the rhythm of the seasons. The egg stage is a critical part of insect life cycles that must resist or tolerate environmental fluctuations. How immobile eggs do so and how these mechanisms change in their function or importance over geographical gradients is a complex story.\nMy past and ongoing research focuses on disentangling the effects of microclimatic variation and inter- and intra-specific variation in physiological traits on life history phenotypes and phenology. The relative effects of environmental and individual variation on phenology.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a5ba6bdabb26526efa479d81b00aa22e","permalink":"https://jacintak.github.io/project/life-cycles/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/life-cycles/","section":"project","summary":"How are ectotherm life cycles adapted to seasonal climate cycles?","tags":["ectotherms","life cycles"],"title":"Predicting ectotherm life cycles","type":"project"},{"authors":null,"categories":null,"content":"Temperature affects biological processes on many levels of biological organisation yet many temperature-dependent traits show great diversity within and among species with direct consequences for how organisms respond to a warming climate.\nTo understand the effect of the temperature-dependence of biological rates shapes trends in physiological trait diversity, my colleages and I take a comparative approach on a macroecological scale using the non-linear relationship between biological rates and temperature as a null hypothesis.\nWe find that much variation in widely characterised critical temperature limits in ectotherms can be explained by the non-linear relationship between biological rates and temperature, acclimation temperature, and rates of heating. Theses results suggest that rising temperatures are not as detrimental to ectotherms as previously suggested and that ectotherms have unappreciated capabilities of responding to thermal variability, at least based on laboratory derived measures of thermal tolerance.\nThis is an Science Foundation Ireland funded position with Nicholas Payne\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"4833c7bc854c275d509d55a420575619","permalink":"https://jacintak.github.io/project/temperature-dependence/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/temperature-dependence/","section":"project","summary":"Temperature affects biological processes on many levels of biological organisation","tags":["ectotherms","thermal adaptation","comparative analysis"],"title":"Thermal dependence of biological rates","type":"project"},{"authors":[],"categories":[],"content":"\rI was recently featured on Humans of BioSciences, a series about the people of the School of Biosciences at the University of Melbourne (my alma mater).\nYou can read the Twitter thread and the full interview here.\n","date":1608163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"f218a3ba89a9ae200e942d523996c41b","permalink":"https://jacintak.github.io/post/2020-12-17-jacinta-humans-of-biosciences/","publishdate":"2020-12-17T00:00:00Z","relpermalink":"/post/2020-12-17-jacinta-humans-of-biosciences/","section":"post","summary":"Find out what I've been doing during a pandemic","tags":["PhD","postdoc"],"title":"Jacinta @ Humans of BioSciences","type":"post"},{"authors":["Jacinta Kong","A. A. Hoffmann","M. R. Kearney"],"categories":null,"content":"Abstract Insect life cycles are adapted to a seasonal climate by expressing alternative voltinism phenotypes—the number of generations in a year. Variation in voltinism phenotypes along latitudinal gradients may be generated by developmental traits at critical life stages, such as eggs. Both voltinism and egg development are thermally determined traits, yet independently derived models of voltinism and thermal adaptation refer to the evolution of dormancy and thermal sensitivity of development rate, respectively, as independent influences on life history. To reconcile these models and test their respective predictions, we characterized patterns of voltinism and thermal response of egg development rate along a latitudinal temperature gradient using the matchstick grasshopper genus Warramaba. We found remarkably strong variation in voltinism patterns, as well as corresponding egg dormancy patterns and thermal responses of egg development. Our results show that the switch in voltinism along the latitudinal gradient was explained by the combined predictions of the evolution of voltinism and of thermal adaptation. We suggest that latitudinal patterns in thermal responses and corresponding life histories need to consider the evolution of thermal response curves within the context of seasonal temperature cycles rather than based solely on optimality and trade-offs in performance. This article is part of the theme issue ‘Physiological diversity, biodiversity patterns and global climate change: testing key hypotheses involving temperature and oxygen’.\n","date":1548892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548892800,"objectID":"42bd2b0f81290fb0ceae73a15db1c4ff","permalink":"https://jacintak.github.io/publication/2019-PTRSB/","publishdate":"2019-01-31T00:00:00Z","relpermalink":"/publication/2019-PTRSB/","section":"publication","summary":"Abstract Insect life cycles are adapted to a seasonal climate by expressing alternative voltinism phenotypes—the number of generations in a year. Variation in voltinism phenotypes along latitudinal gradients may be generated by developmental traits at critical life stages, such as eggs. Both voltinism and egg development are thermally determined traits, yet independently derived models of voltinism and thermal adaptation refer to the evolution of dormancy and thermal sensitivity of development rate, respectively, as independent influences on life history.","tags":["ectotherms","thermal adaptation","thermal response","life history"],"title":"Linking thermal adaptation and life-history theory explains latitudinal patterns of voltinism","type":"publication"},{"authors":["M. R. Kearney","J. Deutscher","Jacinta Kong","A. A. Hoffmann"],"categories":null,"content":"Abstract The phenological response is among the most important traits affecting a species\u0026rsquo; sensitivity to climate. In insects, strongly seasonal environments often select for a univoltine life-cycle such that one seasonal extreme is avoided as an inactive stage. Through understanding the underlying mechanisms for univoltinism, and the consequences of its failure, we can better predict insect responses to climate change. Here we combine empirical data and simulation studies to investigate the consequences of an unusual diapause mechanism in a parthenogenetic matchstick grasshopper, Warramaba virgo, from arid southern Australia. Our field body temperature measurements indicate that this species is a thermoconformer and our laboratory studies of the thermal response of feeding rate imply strong constraints on winter activity. However, the species exhibits no obligate winter diapause, and eggs can develop in one month under constant temperatures approximating the mean soil temperature at the time of oviposition (summer). We show that diurnal temperature cycles exceeding a peak of 36 degrees C inhibit egg development in summer, and that this is sufficient to prevent autumnal hatching of eggs. Development is also strongly retarded below 24 degrees C. Microclimate-driven simulation studies of egg development show that these thermal responses provide robust maintenance of a univoltine life cycle, thereby resulting in survival of heat stress as an egg (due to limited developmental state) and avoidance of cold stress as a nymph and adult (due to overwintering in the soil as an egg). This article is protected by copyright. All rights reserved.\n","date":1520899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520899200,"objectID":"31372bb950c3acadf4ead569bb0179e8","permalink":"https://jacintak.github.io/publication/2018-inte-zool/","publishdate":"2018-03-13T00:00:00Z","relpermalink":"/publication/2018-inte-zool/","section":"publication","summary":"Abstract The phenological response is among the most important traits affecting a species\u0026rsquo; sensitivity to climate. In insects, strongly seasonal environments often select for a univoltine life-cycle such that one seasonal extreme is avoided as an inactive stage. Through understanding the underlying mechanisms for univoltinism, and the consequences of its failure, we can better predict insect responses to climate change. Here we combine empirical data and simulation studies to investigate the consequences of an unusual diapause mechanism in a parthenogenetic matchstick grasshopper, Warramaba virgo, from arid southern Australia.","tags":["ectotherms","thermal adaptation","life cycles","microclimate","mechanistic models"],"title":"Summer egg diapause in a matchstick grasshopper synchronises the life cycle and buffers thermal extremes","type":"publication"},{"authors":["J. L. Maino","Jacinta Kong","A. A. Hoffmann","M. G. Barton","M. R. Kearney."],"categories":null,"content":"Abstract Mechanistic models of the impacts of climate change on insects can be seen as very specific hypotheses about the connections between microclimate, ecophysiology and vital rates. These models must adequately capture stage-specific responses, carry-over effects between successive stages, and the evolutionary potential of the functional traits involved in complex insect life-cycles. Here we highlight key considerations for current approaches to mechanistic modelling of insect responses to climate change. We illustrate these considerations within a general mechanistic framework incorporating the thermodynamic linkages between microclimate and heat, water and nutrient exchange throughout the life-cycle under different climate scenarios. We emphasise how such a holistic perspective will provide increasingly robust insights into how insects adapt and respond to changing climates.\n","date":1476144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476144000,"objectID":"3d90d66f6179b68b1be5bd8594f7add1","permalink":"https://jacintak.github.io/publication/2016-COIS/","publishdate":"2016-10-11T00:00:00Z","relpermalink":"/publication/2016-COIS/","section":"publication","summary":"Abstract Mechanistic models of the impacts of climate change on insects can be seen as very specific hypotheses about the connections between microclimate, ecophysiology and vital rates. These models must adequately capture stage-specific responses, carry-over effects between successive stages, and the evolutionary potential of the functional traits involved in complex insect life-cycles. Here we highlight key considerations for current approaches to mechanistic modelling of insect responses to climate change. We illustrate these considerations within a general mechanistic framework incorporating the thermodynamic linkages between microclimate and heat, water and nutrient exchange throughout the life-cycle under different climate scenarios.","tags":["ectotherms","thermal adaptation","life cycles","microclimate","mechanistic models"],"title":"Mechanistic models for predicting insect responses to climate change","type":"publication"},{"authors":["Jacinta Kong","J. K. Axford","A. A. Hoffmann","M. R. Kearney"],"categories":null,"content":"Abstract High-throughput genomic methods are increasingly used to investigate invertebrate thermal responses with greater dimensionality and resolution than previously achieved. However, corresponding methods for characterizing invertebrate phenotypes are still lacking. To scale up the characterization of invertebrate thermal responses, we propose a novel use of thermocyclers as temperature-controlled incubators.\nHere, we tested the performance of thermocyclers as incubators and demonstrated the application of this method to efficiently characterize the thermal responses of model and non-model invertebrates.\nWe found the thermocyclers performed with high precision, accuracy and resolution under various and fluctuating ambient conditions. We were able to successfully characterize the temperature-dependent development of grasshopper eggs (Warramaba virgo), as well as the effects of fluctuating temperature cycles on the survival of mosquito eggs (Aedes aegypti) and developmental success of Drosophila simulans larvae, all with similar survival rates to conventional methods.\nThermocyclers are a general and transferrable means to scale up current methods of incubating small invertebrates. They permit rapid characterization of high-dimensional physiological responses to natural thermal regimes. When combined with existing approaches in thermal and evolutionary biology, these methods will advance our understanding of, and ability to predict, biological adaptations and responses to environmental changes.\n","date":1462924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462924800,"objectID":"51c065cdd0243c50c8f0d0926b444014","permalink":"https://jacintak.github.io/publication/2016-MEE/","publishdate":"2016-05-11T00:00:00Z","relpermalink":"/publication/2016-MEE/","section":"publication","summary":"Abstract High-throughput genomic methods are increasingly used to investigate invertebrate thermal responses with greater dimensionality and resolution than previously achieved. However, corresponding methods for characterizing invertebrate phenotypes are still lacking. To scale up the characterization of invertebrate thermal responses, we propose a novel use of thermocyclers as temperature-controlled incubators.\nHere, we tested the performance of thermocyclers as incubators and demonstrated the application of this method to efficiently characterize the thermal responses of model and non-model invertebrates.","tags":["ectotherms","thermal adaptation","thermal response"],"title":"Novel applications of thermocyclers for phenotyping invertebrate thermal responses","type":"publication"},{"authors":null,"categories":null,"content":"\rResearch and teaching appointments\rQualifications\rRefereed journal articles\rPreprints\rResearch highlights\rResearch grants or awards\rOther Awards and Scholarships\rTeaching contributions and course development\rConference presentations and invited talks\rProfessional service and affiliations\rProfessional development qualifications\rCommunity outreach and communication\rLast updated 28 December 2022\nResearch and teaching appointments\rPostdoctoral Researcher\nCarleton University, Canada\n1.2023 – Present\rTeaching and Research Fellow\nDepartment of Zoology, School of Natural Sciences.\nTrinity College Dublin, Dublin, Ireland\n5.2019 – 1.2023\rResearch Assistant\nIntraspecific variation in mechanistic species distribution modelling,\nThe University of Melbourne, Australia\n5.2018 – 12.2018\rComparative Animal Physiology tutor (Second Year undergraduate)\nSchool of BioSciences, the University of Melbourne, Australia\n2017– 2018\rEcology in Changing Environments tutor (Third Year undergraduate)\nSchool of BioSciences, the University of Melbourne, Australia\n2016– 2018\rComparative Animal Physiology residential tutor,\nUniversity College, the University of Melbourne, Australia\n2017– 2018\rFirst Year Chemistry residential tutor\nSt John’s College, the University of Queensland Australia\n8.2014 – 10.2014\rFirst Year Biology tutor (Science Learning Centre tutor),\nthe Faculty of Science, the University of Queensland, Australia\n2012\rPeer Assisted Study Session leader: first year statistics\nThe Peer Assisted Study Session office and the School of Mathematics and Physics, the University of Queensland Australia\n8.2012 – 10.2012\rPeer Assisted Study Session leader: first year ecology\nThe Peer Assisted Study Session office, the University of Queensland Australia\n2011 – 2012\rVolunteer laboratory technician. Animal husbandry.\nWhite Evolutionary Physiology Laboratory, the University of Queensland\n3.2012 – 10.2012\rQualifications\rDoctor of Philosophy (Science)\nThe University of Melbourne, Australia\nTitle: Predicting ectotherm life cycles under a variable climate: Physiological diversity of matchstick grasshopper eggs and their ecological and evolutionary implications\nRepository access: http://hdl.handle.net/11343/225704\n1.2015 – completed 14 Aug 2019, conferred 5.10.2020\nBachelor of Science (Honours Class I, University Medal)\nThe University of Queensland, Australia\nTitle: The effect of temperature on the relationship between metabolic rate and mass: Tests of the Metabolic Theory of Ecology\nConferred 6.12.2013\nBachelor of Science\nThe University of Queensland, Australia\nConferred 17.12.2012\nRefereed journal articles\rIosilevskii G+, Kong JD+, Meyer CG, Watanabe YY, Papastamatiou YP, Royer MA, Nakamura I, Sato K, Doyle TK, Harman L, Houghton JDR, Barnett A, Semmens JM, Maoiléidigh NÓ, Drumm A, O’Neill R, Coffey DM, Payne NL (2022) A general swimming response in exhausted obligate swimming fish. Royal Society Open Science. 9: 211869. DOI: 10.1098/rsos.211869. +Joint first author.\nKearney MR, Jasper ME, White VL, Aitkenhead IJ, Blacket MJ, Kong JD, Chown SL, Hoffmann AA. (2022) Parthenogenesis without costs in a grasshopper with hybrid origins. Science. 376: 1110 – 1114 DOI: 10.1126/science.abm1072. Altmetric score: 204\nSchwanz LE, Gunderson A, Iglesias-Carrasco M, Johnson MA, Kong JD, Riley J, Wu NC. (2022) Best practices for building and curating databases for comparative analyses. Journal for Experimental Biology. 225: jeb243295. DOI: 10.1242/jeb.243295. Altmetric score: 12\nKong JD, Hoffmann AA, Kearney MR. (2019) Linking thermal adaptation and life-history theory explains latitudinal patterns of voltinism. Philosophical Transactions of the Royal Society B: Biological Sciences. 374(1778). DOI: 10.1098/rstb.2018.0547. Altmetric score: 8\nKearney MR, Deutscher J, Kong JD, Hoffmann AA. (2018) Summer egg diapause in a matchstick grasshopper synchronises the life cycle and buffers thermal extremes. Integrative Zoology. 13(4): 437–449. DOI: 10.1111/1749-4877.12314. Altmetric score: 2\nMaino JL, Kong JD, Hoffmann AA, Barton MG, Kearney MR. (2016) Mechanistic models for predicting insect responses to climate change. Current Opinion in Insect Science. 17: 81 – 86. DOI: 10.1016/j.cois.2016.07.006. Altmetric score: 6\nKong JD, Axford JK, Hoffmann AA, Kearney MR. (2016) Novel applications of thermocyclers for phenotyping invertebrate thermal responses. Methods in Ecology and Evolution. 7(10): 1201 – 1208. 2016. DOI: 10.1111/2041-210X.12589. Altmetric score: 20\nPreprints\rKong JD, Wu NC. (2022) Can we improve our ability to identify climate vulnerability in ectotherm life cycles?. bioRxiv. DOI: 10.1101/2022.12.14.520433.\nKong JD+, Arnoldi J-F+, Jackson AL, Bates AE, Morley SA, Smith JA, Payne NL+. (2022) Heating tolerance of ectotherms is explained by temperature’s non-linear influence on biological rates. bioRxiv. DOI: 10.1101/2022.12.06.519315. +Joint first author.\nResearch highlights\rORCID\nGoogle scholar\nH-index: 4 (GS)\nTotal citations: 99\nHigh attention papers in the top 5% of all research outputs scored by Altmetric:\rScience: 20 News outlets, 1 blog, 86 tweeters. Altmetric score: 212\rRoyal Society Open Science: 90 tweeters. Altmetric score: 61\rEcology/evolution studies in some of the best journals in their respective disciplines, and in the top 25% of all research outputs scored by Altmetric:\rJournal of Experimental Biology. Altmetric score: 12\rPhilosophical Transactions of the Royal Society B. Altmetric score: 8. Citations: 20\rCurrent Opinion in Insect Science. Altmetric score: 6. Citations: 43\rI have peer reviewed for:\nGlobal Change Biology\rEntomologia Experimentalis et Applicata\rMethods in Ecology and Evolution\rNature Ecology and Evolution\rEcological Entomology\rThe Canadian Entomologist\rEnvironmental Entomology\rJournal of Fish Biology\rCurrent Zoology\rScientific Reports\rConservation Physiology\rAmerican Naturalist\rFunctional Ecology\rResearch grants or awards\rTOTAL: $13 750 AUD 🇦🇺\n$2 500 AUD. 6.2018 Holsworth Wildlife Research Endowment\nEquity Trustees Charitable Foundation \u0026amp; the Ecological Society of Australia\r$300 AUD. 12.2018 2nd runner up student presentation\nAustralian and New Zealand Society for Comparative Physiology and Biochemistry, Australia\r$500 AUD. 8.2018 Student Travel Grant\nAustralian Entomological Society, Australia\r$1 500 AUD. 2018 Science Abroad Travelling Scholarship\nFaculty of Science, the University of Melbourne, Australia\r$950 AUD. 2018 FH Drummond Travel Award\nSchool of BioSciences, the University of Melbourne, Australia\r$1 500 AUD. 2018 School of BioSciences Travelling Scholarship\nSchool of BioSciences, the University of Melbourne, Australia\r$6 000 AUD. 6.2018 Holsworth Wildlife Research Endowment\nEquity Trustees Charitable Foundation\r$500 AUD. 6.2018 Three Minute Thesis (3MT) People’s Choice Winner\nThe University of Queensland Undergraduate Research Conference\rOther Awards and Scholarships\r$13 541 AUD. 2018 Research Training Program Scholarship, Australian Government\r$26 682 AUD. 2017 Research Training Program Scholarship, Australian Government\r$200 AUD. 2017 Runners-up in the Sustainability Prize photo competition, Graduate Student Association, the University of Melbourne\r$26 288 AUD. 2016 Australian Postgraduate Award, Australian Government\r$25 849 AUD. 2015 Australian Postgraduate Award, Australian Government\r2014 University Medal 2013, the University of Queensland, Australia\r2010 – 2013 Dean’s Commendation for Academic Excellence (formerly Dean’s Commendation for High Achievement), the Faculty of Science, the University of Queensland, Australia\rTeaching contributions and course development\rTrinity College Dublin implements a 4 year degree program with 2 years of general subjects (e.g. biological sciences stream, ~250 students) and 2 years towards a specific major (e.g. zoology, ~ 35 students). Total of 60 credits per year. Degree consists of mandatory core subjects and electives.\n2022\nLecturer \u0026amp; course development: Statistics and computation for biologists (BYU22S01). 2nd year undergraduate core subject, 5 credits. Trinity College Dublin. Developed course lectures and practical material, implemented novel R packages for interactive teaching within the R environment (learnr package). In-person lectures \u0026amp; practicals.\rLecturer \u0026amp; course development: Animal Diversity I (ZOU33003). 3rd year undergraduate, core zoology major subject, 5 credits. Trinity College Dublin. Developed course lectures and practical material. In-person lectures \u0026amp; practicals.\rLecturer \u0026amp; course development: Animal Diversity II (ZOU33004). 3rd year undergraduate, core zoology major subject, 5 credits. Trinity College Dublin. Developed course lectures and practical material. In-person lectures \u0026amp; practicals.\rContributing staff: Marine Biology (ZOU33000). 3rd year undergraduate, core zoology major subject, 5 credits. Trinity College Dublin. Resident field course based in Killary Fjord, Ireland.\r2021\nLecturer \u0026amp; course development: Statistics and computation for biologists (BYU22S01). 2nd year undergraduate core subject, 5 credits. Trinity College Dublin. Developed course lectures and practical material, implemented novel R packages for interactive teaching within the R environment (learnr package). Hybrid delivery: remote and in-person lectures \u0026amp; remote and in-person practicals.\rLecturer \u0026amp; course development: Animal Diversity I (ZOU33003). 3rd year undergraduate, core zoology major subject, 5 credits. Trinity College Dublin. Developed course lectures and practical material. Hybrid delivery: remote lectures \u0026amp; in-person practicals.\rLecturer \u0026amp; course development: Animal Diversity II (ZOU33004). 3rd year undergraduate, core zoology major subject, 5 credits. Trinity College Dublin. Developed course lectures and practical material. Hybrid delivery: remote lectures \u0026amp; in-person practicals.\r2020\nLecturer (Module co-ordinator) \u0026amp; course development: Statistics and computation for biologists (BYU22S01). 2nd year undergraduate. Trinity College Dublin. Developed course lectures and practical material, implemented reproducible workflow for online exams. Adapted for remote delivery.\rLecturer \u0026amp; course development: Animal Diversity I (ZOU33003). 3rd year undergraduate. Trinity College Dublin. Developed course lectures and practical material. Module co-coordinator. Adapted for remote delivery.\rLecturer \u0026amp; course development: Animal Diversity II (ZOU33004). 3rd year undergraduate. Trinity College Dublin. Developed course lectures and practical material. Adapted for remote delivery.\r2019\nLecturer \u0026amp; course development: Statistics and computation for biologists (BYU22S01). 2nd year undergraduate. Trinity College Dublin. Developed course lectures and practical material, implemented reproducible workflow for online exams.\rLecturer \u0026amp; course development: Animal Diversity I (ZOU33003). 3rd year undergraduate. Trinity College Dublin. Developed course lectures and practical material. Acting module co-coordinator.\rLecturer \u0026amp; course development: Animal Diversity II (ZOU33004). 3rd year undergraduate. Trinity College Dublin. Developed course lectures and practical material.\r2017\nGuest lecturer: 2nd year Comparative Animal Physiology, University of Melbourne\rCourse development: 2nd year Biostatistics, University of Melbourne. Evaluated course context and provided feedback.\r2016\nGuest lecturer: 2nd year Comparative Animal Physiology, University of Melbourne\rConference presentations and invited talks\r2022\nCan we improve our ability to understand ectotherm thermal tolerance? Society for Experimental Biology Animal Biology Early Career Researcher Symposium. 3-7th October. Tvärminne Zoological Station, Finland.\rCan we improve our ability to identify climate vulnerability in ectotherm life cycles? British Ecological Society Annual Meeting, Edinburgh, UK\r2021\nEnergetic turnover explains the inflexibility of upper thermal tolerances in ectotherms. Irish Ecological Association Meeting, University College Cork, Ireland (online)\rEctotherm heat limits track biological rates. British Ecological Society Macroecology Special Interest Group meeting (online)\rThermal adaptation and plasticity of egg development generates latitudinal patterns in insect life cycles under seasonal climates. Society for Experimental Biology Annual Meeting (online)\r2019\nDetangling the complex problem of climate adaptation of insects living in a seasonal world. Victorian Biodiversity Conference, University of Melbourne, VIC, Australia\rLocal adaptation of thermal responses generates voltinism patterns of matchstick grasshoppers, Warramaba (Orthoptera: Morabidae), along a latitudinal gradient. British Ecological Society Annual Meeting, Belfast, N. Ireland, UK\r2018\nSelection against overwintering shapes thermal performance curves for development. Australian and New Zealand Society for Comparative Physiology and Biochemistry Conference, Monash University, VIC, Australia\rEnvironmental and developmental drivers at the egg stage generate divergent life cycles in wingless arid zone grasshoppers (Orthoptera: Warramaba). Australian Entomological Society Conference, Alice Springs, N.T., Australia\rThe egg stage drives life cycle adaptation to climate in the widely distributed matchstick grasshoppers (Vandiemenella and Warramaba, Orthoptera: Morabidae). ‘The height, breadth and depth of physiological diversity: variation across latitudinal, altitudinal and depth gradients’ Animal Biology Satellite Meeting, Florence, Italy, Society for Experimental Biology\rMicroclimate-driven mechanistic models to examine clinal adaptation at the egg stage in a parthenogenetic grasshopper. Society for Experimental Biology Annual Conference, Florence, Italy\r2017\nDoes variation in egg developmental responses to temperature generate divergent life-cycles in a genus of flightless grasshoppers (Warramaba spp.)? School of BioSciences Postgraduate Symposium, the University of Melbourne, Parkville, Australia\rEgg development drives life cycles in Warramaba spp. grasshoppers. Australian and New Zealand Society for Comparative Physiology and Biochemistry Conference, Daintree Rainforest Observatory, QLD, Australia\rMechanistic models for understanding and predicting insect responses to climate change. Australian Entomological Society Conference, Terrigal, N.S.W., Australia\r2016\nPredicting insect egg development under variable climates. School of BioSciences Postgraduate Symposium, the University of Melbourne, Parkville, Australia\rPredicting egg development in the parthenogenetic grasshopper Warramaba virgo (Orthoptera: Morabidae). Australian and New Zealand Society for Comparative Physiology and Biochemistry Conference, Western Sydney University, N.S.W., Australia\r2015\nNovel applications of thermocyclers for high-throughput phenotyping of invertebrate thermal response. Australian and New Zealand Society for Comparative Physiology and Biochemistry Conference, Fowler’s Gap, N.S.W., Australia\r2013\nEvery Breath You Take Links Metabolism and Ecology\rThree Minute Thesis, Undergraduate Research Conference, the University of Queensland, Australia The University of Queensland\rEvery Breath You Take Links Metabolism and Ecology. Summer Research Introduction Session 2013, invited by the Office of Undergraduate Education, the University of Queensland, Australia\rFlying foxes and you: Exploring the exposure of society to so-called “rats with wings”\rBachelor of Science Welcome Day, invited by the Faculty of Science, the University of Queensland, Australia\r2012\nFlying foxes and you: Exploring the exposure of society to so-called “rats with wings”\rAdvanced Study Program in Science Student Conference, the University of Queensland, Australia\r2011\nFemale-biased dispersal in the Eastern Water Dragon (Physignathus lesueurii lesueurii)\rAdvanced Study Program in Science Student Conference, the University of Queensland, Australia\rProfessional service and affiliations\r2015 – Present Member: Australian and New Zealand Society for Comparative Physiology and Biochemistry (ANZSCPB)\r2019 – Present Member: British Ecological Society (BES) \u0026amp; Irish Ecological Association (IEA)\r2018 Member: Royal Society of Victoria (RSV)\r2017 – Present Member: Society for Experimental Biology (SEB)\r2015 – 2020 Member: Australian Entomological Society (AES)\r2019 – 2020 Member: European Society for Evolutionary Biology (ESEB)\r2017 – 2018 President, BioSciences Postgraduate Society, the University of Melbourne\r2016 – 2017 Vice President, BioSciences Postgraduate Society, the University of Melbourne\rProfessional development qualifications\r26 Jan 2021 Epigeum Research Integrity Course\r19 Mar 2021 Dynamic Energy Budget Course\rCommunity outreach and communication\r2022\nKong JD, Chown SL, Hoffmann AA and Kearney MR. (2022) Reply to Adamo: No Signs of Pathogen Susceptibility in Warramaba virgo. Science. eLetter. DOI: 10.1126/science.abm1072.\rConference Organiser. Society for Experimental Biology Animal Biology Early Career Researcher Symposium. 3-7th October. Tvärminne Zoological Station, Finland.\r2021\nContributor to the Trinity Walton Club STEM@Universi-TY program, Trinity College Dublin (2021-present)\rMentor for Irish Ecological Association mentoring meeting. 7th January\rThe Socio-Economic Theory of Animal Abundance. April Fools blog post for EcoEvo@TCD. 1st April\r2020\nProfiled on Humans of BioSciences by the School of BioSciences, the University of Melbourne, Australia. Website \u0026amp; Twitter. 17th December\rGuest interview with Newstalk radio, Ireland. 14th January\r2019\nMentor for BES Women in Science Mentoring Program\rHome and Away: 3 part blog series for EcoEvo@TCD\rHome and Away: Would a Rosella by any other name smell as sweet (online 1 Nov)\rHome and Away: Monotreme mistakes (online 22 Nov)\rHome and Away: Australian expats (online 12 Dec)\rVictorian Biodiversity Conference Volunteer, Melbourne, Australia\r2018\nBig Ideas in Macrophysiology. Report on the 2018 Animal Biology Satellite Meeting, Florence Italy. Society for Experimental Biology Magazine\rThe University of Melbourne Open Day Volunteer, the University of Melbourne, Melbourne, Australia\r2017\nThe University of Melbourne Open Day Volunteer, the University of Melbourne, Melbourne, Australia\rSession Chair, BioSciences Postgrad Symposium, the University of Melbourne, Australia\rVictorian Biodiversity Conference Volunteer, Melbourne, Australia\rBlog post for Graduate Student Association, University of Melbourne on the biodiversity photo competition\r2016\nSession Chair, BioSciences Postgrad Symposium, the University of Melbourne, Australia\rNovel applications of thermocyclers for characterising invertebrate thermal responses. Video for Methods in Ecology and Evolution\r2015\nThe University of Melbourne Open Day Volunteer, the University of Melbourne, Melbourne, Australia\rThe Real Life of a Research Student. Science Undergraduate Research Journal (SURJ), Issue 2. The University of Queensland, Australia\rJacinta on Zoology \u0026amp; Research. Interview with BITE BACK, Black Dog Institute, Australia. 31st August. Link likely broken\r2014\nBrisbane Open House Volunteer, Brisbane Open House, Brisbane, Australia\rStudent Chaperone for the International Baccalaureate World Student Conference, the University of Queensland, Australia\rMoreton Bay Research Station Open Day Assistant, Moreton Bay Research Station, Brisbane, Australia\r2013\nScience Mentor to Science Undergraduate Students, appointed by the Faculty of Science, the University of Queensland, Australia\r2012\nMoreton Bay Research Station Open Day Assistant, Moreton Bay Research Station, Brisbane, Australia\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9780949a3024bbdfdb7ce88dfaa4afe3","permalink":"https://jacintak.github.io/CV/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/CV/","section":"","summary":"Research and teaching appointments\rQualifications\rRefereed journal articles\rPreprints\rResearch highlights\rResearch grants or awards\rOther Awards and Scholarships\rTeaching contributions and course development\rConference presentations and invited talks\rProfessional service and affiliations\rProfessional development qualifications\rCommunity outreach and communication\rLast updated 28 December 2022\nResearch and teaching appointments\rPostdoctoral Researcher\nCarleton University, Canada\n1.2023 – Present\rTeaching and Research Fellow\nDepartment of Zoology, School of Natural Sciences.\nTrinity College Dublin, Dublin, Ireland","tags":null,"title":"CV","type":"page"}]