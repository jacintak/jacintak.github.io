<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Modelling | Jacinta&#39;s Website</title>
    <link>https://jacintak.github.io/teaching/GLM/</link>
      <atom:link href="https://jacintak.github.io/teaching/GLM/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistical Modelling</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language>
    <image>
      <url>https://jacintak.github.io/img/icon-192.png</url>
      <title>Statistical Modelling</title>
      <link>https://jacintak.github.io/teaching/GLM/</link>
    </image>
    
    <item>
      <title>Simple regression</title>
      <link>https://jacintak.github.io/teaching/GLM/GLM1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jacintak.github.io/teaching/GLM/GLM1/</guid>
      <description>
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;div id=&#34;overall-lecture-aims&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall lecture aims&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Identify the structure of general linear models&lt;/li&gt;
&lt;li&gt;Describe how linear models are parametrized&lt;/li&gt;
&lt;li&gt;Understand how to use linear models&lt;/li&gt;
&lt;li&gt;Understand how to interpret and evaluate linear models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recommended reading:
Chapter 5.5 Getting Started with R&lt;/p&gt;
&lt;div id=&#34;by-the-end-of-this-lecture-you-should&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;By the end of this lecture you should:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Describe the structure of a general linear model&lt;/li&gt;
&lt;li&gt;Understand how a simple linear model is parametrised&lt;/li&gt;
&lt;li&gt;Understand what the parameters of a linear model represent&lt;/li&gt;
&lt;li&gt;Parametrise a simple linear model from given information&lt;/li&gt;
&lt;li&gt;Interpret a simple linear model in a graphical format&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use built-in R datasets and R libraries&lt;/li&gt;
&lt;li&gt;Conduct a simple linear model&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;height-and-girth-of-trees&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Height and girth of trees&lt;/h1&gt;
&lt;p&gt;Imagine we measured the height and girth of trees to test the hypothesis that thicker trees are taller.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/linear-model-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How should test our hypothesis? We could assign trees to categories like “thick” or “thin” trees and do a t-test. What does our t-test say?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/t-test-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
    Welch Two Sample t-test

data:  Height by Thickness
t = 1.6339, df = 28.168, p-value = 0.1134
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.9240387  8.2181563
sample estimates:
mean in group Thick  mean in group Thin 
           78.00000            74.35294 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;but-this-isnt-the-best-way&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;But&lt;/strong&gt; this isn’t the best way&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How did we define “thick” or “thin”?&lt;/li&gt;
&lt;li&gt;We lost information using groups - tree girth is not categorical data, it’s continuous&lt;/li&gt;
&lt;li&gt;This increases unexplained variance = &lt;em&gt;bad!&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;When might you want to group data?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Linear regression&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;models the linear relationship between 2 continuous variables&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Continuous response variable (Y)&lt;/li&gt;
&lt;li&gt;Continuous predictor variable (X)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:regression-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/regression-model-1.png&#34; alt=&#34;Fit a line&#34; width=&#34;400&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Fit a line
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-linear-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What does linear mean?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;By definition, all predictor parameters should not be divided by each other&lt;/li&gt;
&lt;li&gt;No parameter is an exponent&lt;/li&gt;
&lt;li&gt;No parameter is multiplied or divided by another&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not a linear model:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(H_a = \frac{a \times H \times T}{1 + a \times H \times T}\)&lt;/span&gt; e.g. Holling’s Type II predation equation&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(y = \frac{\beta_1 x}{\beta_2 x}\)&lt;/span&gt; e.g. enzyme rate of reactions&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-do-we-use-a-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why do we use a linear model?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Quantify a relationship
&lt;ul&gt;
&lt;li&gt;E.g. relationship between drug concentration and time&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Predict what will happen
&lt;ul&gt;
&lt;li&gt;E.g. growth at different time points&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Explain as much about the response variable as possible&lt;/li&gt;
&lt;li&gt;Partition variation
&lt;ul&gt;
&lt;li&gt;E.g. Phenotypic variation = genetic variation + environmental variation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-a-general-linear-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Structure of a general linear model&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = intercept + slope \times X\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/general-linear-model-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Intercepts and slopes can be mathematically represented by any symbol or letter but conventionally the equation is written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i = \beta_0 + \beta_1 \times x_i + \varepsilon_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; refers to individual data points.&lt;/p&gt;
&lt;div id=&#34;variables-of-a-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variables of a linear model&lt;/h2&gt;
&lt;p&gt;Variables are &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; = response variable, what you are interested in, e.g. height&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; = predictor variable, what you are manipulating, e.g. girth&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-of-a-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameters of a linear model&lt;/h2&gt;
&lt;p&gt;Parameters are what we don’t know and we need to work out (&lt;em&gt;parametrise&lt;/em&gt;). They are &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; = population intercept, value of y when x is 0, constant&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; = population slope, unit change in y with a unit change in x&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt; = the residual of the model (how much the line gets wrong for each data point or what is left over), also called &lt;strong&gt;random error&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ordinary-least-squares-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ordinary least squares regression&lt;/h1&gt;
&lt;p&gt;Ordinary least squares regression (OLS) is the technique used to parametrise the linear model by finding the ‘best fitting’ line.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The aim is to make the sum of the squared residuals, &lt;span class=&#34;math inline&#34;&gt;\(\sum{\varepsilon_i^2}\)&lt;/span&gt;, as small as possible&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Meaning calculating the difference between each point and the line, squaring that difference and adding it all up (Figure &lt;a href=&#34;#fig:residuals&#34;&gt;2&lt;/a&gt;).&lt;br /&gt;
Residuals are squared to make all values positive - remember points can fall above and below the best fit line.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:residuals&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/residuals-1.png&#34; alt=&#34;Residuals (blue lines) are the difference between the data point and the predicted line (black line)&#34; width=&#34;500&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Residuals (blue lines) are the difference between the data point and the predicted line (black line)
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;finding-the-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finding the intercept&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why not extrapolate to 0?&lt;/li&gt;
&lt;li&gt;We know &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt; - the means of x and y&lt;/li&gt;
&lt;li&gt;We can rearrange the linear equation to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = \bar{y} - \beta_1 \bar{x}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the estimated slope&lt;/li&gt;
&lt;li&gt;But &lt;code&gt;R&lt;/code&gt; will do all this for you!&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;halfway-there-revision-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Halfway there! Revision time!&lt;/h1&gt;
&lt;p&gt;Test your understanding so far by answering the questions below. Click &lt;code&gt;Code&lt;/code&gt; to see the answer.&lt;/p&gt;
&lt;p&gt;What types of variables are used in simple linear regression?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Both the response and predictor variable should be continuous. (Categorical predictor variables are also allowed as we will see in later lectures)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the method for parameterising linear models called?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Ordinary least squares regression. The aim is to minimise the sum of squared residuals.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is random error in linear regression?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;The variation in data that cannot be explained by the linear model. i.e. the difference between the predicted value of the model and the observations: the residuals. Denoted ε.&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-a-linear-model-tell-us&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What does a linear model tell us?&lt;/h1&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What will Y be, given a new value of X?&lt;/li&gt;
&lt;li&gt;Does the population slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; differ to 0?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;back-to-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to trees&lt;/h2&gt;
&lt;p&gt;The function &lt;code&gt;lm()&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt; stands for linear model. It will do the OLS calculations for us. The function takes data in the form &lt;code&gt;lm(Y ~ X, data)&lt;/code&gt;. Use &lt;code&gt;summary()&lt;/code&gt; on your &lt;code&gt;lm&lt;/code&gt; to get more information.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Height ~ Girth, data = trees)

Coefficients:
(Intercept)        Girth  
     62.031        1.054  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;(Intercept)&lt;/code&gt; is the value of the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;code&gt;Girth&lt;/code&gt; is the slope of the model, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;br /&gt;
We then put these numbers in our linear model equation to get the &lt;strong&gt;parametrised linear model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 \times X\)&lt;/span&gt;&lt;br /&gt;
becomes &lt;span class=&#34;math inline&#34;&gt;\(Height =\)&lt;/span&gt; 62.031 + 1.054 &lt;span class=&#34;math inline&#34;&gt;\(\times Girth\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:tree-plot-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/tree-plot-2-1.png&#34; alt=&#34;Does the above equation match this line?&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Does the above equation match this line?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-a-new-value-of-y&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating a new value of Y&lt;/h2&gt;
&lt;p&gt;We can use the parametrised equation to work out the height of a tree for any value of girth. If a tree is 10cm thick, what is its predicted height?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Height =\)&lt;/span&gt; 62.031 + 1.054 &lt;span class=&#34;math inline&#34;&gt;\(\times Girth\)&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Height =\)&lt;/span&gt; 62.031 + 1.054 &lt;span class=&#34;math inline&#34;&gt;\(\times 10\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Height =\)&lt;/span&gt; 72.571 cm&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;does-the-population-slope-beta_1-differ-to-0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Does the population slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; differ to 0?&lt;/h2&gt;
&lt;p&gt;Typically we are more interested in the value of the slope than the intercept. We want to know if the slope of the fitted line is statistically different to 0 because that represents our hypotheses:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Null hypothesis (in orange, Figure &lt;a href=&#34;#fig:tree-slopes&#34;&gt;4&lt;/a&gt;):
&lt;span class=&#34;math display&#34;&gt;\[H_0: \beta_1 = 0\]&lt;/span&gt;
Alternative hypothesis (Figure &lt;a href=&#34;#fig:regression-slopes&#34;&gt;5&lt;/a&gt;):
&lt;span class=&#34;math display&#34;&gt;\[H_1: \beta_1 \neq 0\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:tree-slopes&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/tree-slopes-1.png&#34; alt=&#34;Hypotheses of linear models&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Hypotheses of linear models
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:regression-slopes&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/regression-slopes-1.png&#34; alt=&#34;Regression lines can have positive (blue) or negative (red) slopes, either are H1&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Regression lines can have positive (blue) or negative (red) slopes, either are H1
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-linear-model-parameters-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing linear model parameters in R&lt;/h3&gt;
&lt;p&gt;R will conduct a statistical test on the model parameters for us. You can see it using &lt;code&gt;summary(lm())&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Height ~ Girth, data = trees)

Residuals:
     Min       1Q   Median       3Q      Max 
-12.5816  -2.7686   0.3163   2.4728   9.9456 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  62.0313     4.3833  14.152 1.49e-14 ***
Girth         1.0544     0.3222   3.272  0.00276 ** 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 5.538 on 29 degrees of freedom
Multiple R-squared:  0.2697,    Adjusted R-squared:  0.2445 
F-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The P values above test whether our slope is significantly different to 0&lt;/li&gt;
&lt;li&gt;Tested like a single parameter t-test (why?)
&lt;ul&gt;
&lt;li&gt;What are the degrees of freedom?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Based on the R output above, does our estimated slope for Girth differ to 0 and what can we conclude about our hypothesis?&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-distribution-functions-pdf&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probability distribution functions (PDF)&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Linear models need a descriptor of how response variables (or errors) should be distributed&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Probability distributions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E.g. Gaussian/normal distribution&lt;/li&gt;
&lt;li&gt;Grouped by “family”&lt;/li&gt;
&lt;li&gt;Other types exist but not covered here – same principles apply, called generalised linear models (GLM)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:normal&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM1_files/figure-html/normal-1.png&#34; alt=&#34;a normal distribution with mean = 0 and sd  = 1&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: a normal distribution with mean = 0 and sd = 1
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models-in-the-gaussian-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear models in the Gaussian family&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Same principles, different names&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression
&lt;ul&gt;
&lt;li&gt;continuous response&lt;/li&gt;
&lt;li&gt;one or more continuous predictors&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Analysis of Variance (ANOVA)
&lt;ul&gt;
&lt;li&gt;continuous response&lt;/li&gt;
&lt;li&gt;categorical predictors&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Analysis of Covariance (ANCOVA)
&lt;ul&gt;
&lt;li&gt;continuous response&lt;/li&gt;
&lt;li&gt;A mix of continuous and categorical predictors&lt;/li&gt;
&lt;li&gt;Special case with special assumptions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;you-have-seen-these-statistical-tests-in-previous-lectures&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;You have seen these statistical tests in previous lectures&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
    Welch Two Sample t-test

data:  Height by Sex
t = -12.924, df = 192.7, p-value &amp;lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -15.14454 -11.13420
sample estimates:
mean in group Female   mean in group Male 
            165.6867             178.8260 &lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sum Sq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean Sq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;F value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;F)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8974.12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8974.1197&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;165.1145&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.8618e-28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Residuals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;206&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11196.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;54.3509&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A one-way ANOVA is mathematically identical to a t-test&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this module, don’t focus on memorising what kind of test fits what kinds of data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They are all variants on general linear models anyway…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are interested in &lt;strong&gt;slopes&lt;/strong&gt; and (to a lesser extent) intercepts&lt;/li&gt;
&lt;li&gt;Understand how these models relate to your &lt;em&gt;hypotheses &amp;amp; experimental design&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;revision-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Revision time!&lt;/h1&gt;
&lt;p&gt;Test your understanding so far by answering the questions below. Click &lt;code&gt;Code&lt;/code&gt; to see the answer.&lt;/p&gt;
&lt;p&gt;What is the function in R to conduct a linear regression?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;The function is lm().&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do you find a predicted value of a response for a given predictor value using a parameterised linear model?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Substitute the value of the predictor in the parameterised linear model and solve it.

if Y = 10 * X + 6 and X = 2:
   Y = 10 * 2 + 6 = 20 + 6
   Y = 26 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the probability distribution function of linear regression?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;The normal distribution. Also called Gaussian.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple regression</title>
      <link>https://jacintak.github.io/teaching/GLM/GLM2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jacintak.github.io/teaching/GLM/GLM2/</guid>
      <description>
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;div id=&#34;overall-lecture-aims&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall lecture aims&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Identify the structure of general linear models&lt;/li&gt;
&lt;li&gt;Describe how linear models are parametrized&lt;/li&gt;
&lt;li&gt;Understand how to use linear models&lt;/li&gt;
&lt;li&gt;Understand how to interpret and evaluate linear models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recommended reading:
Chapter 5.5 Getting Started with R&lt;/p&gt;
&lt;div id=&#34;by-the-end-of-this-lecture-you-should&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;By the end of this lecture you should:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Understand the parameters of a linear model with multiple predictor variables&lt;/li&gt;
&lt;li&gt;Parametrise and use a multiple regression model to predict values from the R output of a linear regression&lt;/li&gt;
&lt;li&gt;Interpret and complete ANOVA tables with multiple predictor variables&lt;/li&gt;
&lt;li&gt;Understand an interaction between two predictor variables&lt;/li&gt;
&lt;li&gt;Conduct a multiple regression in R with 2 continuous or categorical predictor variables or a combination&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;previously-we-parametrised-slopes-and-intercepts-of-simple-linear-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Previously, we parametrised slopes and intercepts of simple linear models&lt;/h1&gt;
&lt;p&gt;With one continuous response variable and one continuous predictor variable&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = \beta_0 + \beta_1 X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; = intercept and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; = slope&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/general-lm-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So far these parameters have been &lt;em&gt;fixed&lt;/em&gt;…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-of-a-linear-model-can-be-random-or-fixed&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameters of a linear model can be &lt;strong&gt;random&lt;/strong&gt; or &lt;strong&gt;fixed&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Important to consider when designing experiments – you decide!&lt;/li&gt;
&lt;li&gt;Changes the hypotheses &amp;amp; interpretation of results&lt;/li&gt;
&lt;li&gt;Fixed:
Constant across all groupings (how observations are related – e.g. populations of individuals)&lt;/li&gt;
&lt;li&gt;Random:
Vary across groupings, or represent a random sample from a larger population&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;fixed-slopes-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed slopes linear model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All groups (Frequency of exercise in this example) have the same slope&lt;/li&gt;
&lt;li&gt;Response of groups to predictor variable x is consistent across groups&lt;/li&gt;
&lt;li&gt;Cannot extrapolate beyond groups&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Thus can only draw conclusions for these groups&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fixed-slopes-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/fixed-slopes-model-1.png&#34; alt=&#34;Height of students of various frequencies of exercise and hand width&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Height of students of various frequencies of exercise and hand width
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-slopes-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random slopes linear model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All groups have different slopes&lt;/li&gt;
&lt;li&gt;Response of groups to predictor variable x varies among groups&lt;/li&gt;
&lt;li&gt;Can extrapolate beyond observed groups&lt;/li&gt;
&lt;li&gt;Identity of group is not the main question&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Thus can generalise to other groups outside study&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:random-slopes-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/random-slopes-model-1.png&#34; alt=&#34;Each group has a different slope&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Each group has a different slope
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;distinguishing-between-fixed-or-random-is-important&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Distinguishing between Fixed or random is important&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Changes the way Analysis of Variance (ANOVA) is calculated&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Changes formula for Mean Sum of Squares&lt;/li&gt;
&lt;li&gt;Changes numerator/denominator of F ratio&lt;/li&gt;
&lt;li&gt;Thus changes interpretation&lt;/li&gt;
&lt;li&gt;Called Type I (fixed), II (random) or III (mixed)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;t-tests-simple-linear-models-anova&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;t-tests, simple linear models, ANOVA&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Predictor variables can be categorical (called factors in R), like exercise in Figures &lt;a href=&#34;#fig:fixed-slopes-model&#34;&gt;1&lt;/a&gt; &amp;amp; &lt;a href=&#34;#fig:random-slopes-model&#34;&gt;2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Can have two or more sub-categories (called levels in R)
&lt;ul&gt;
&lt;li&gt;E.g. sex: male/female, level of drug: low/medium/high&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically, the relationship between a continuous response variable &amp;amp; categorical predictors is analysed using a special case of statistical models: One-way Analysis of Variance (ANOVA)&lt;/p&gt;
&lt;p&gt;Confusingly, ANOVA can refer to how data are spread around a single mean (&lt;em&gt;variance&lt;/em&gt;) and also a way of comparing means (via how similar their variances are) for experimental designs with a predictor variable with any number of levels.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-one-way-anova-with-2-or-more-levels&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fixed one-way ANOVA with 2 or more levels&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;level = subsets within categories
&lt;ul&gt;
&lt;li&gt;e.g. male/female, low/medium/high&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;We are testing means for each level&lt;/li&gt;
&lt;li&gt;What is the null hypothesis?&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0:\)&lt;/span&gt; All levels come from the same population (have the same mean)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;pulse-rate-and-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pulse rate and exercise&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/pulse-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do an ANOVA and get the following output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Analysis of Variance Table

Response: Pulse
           Df  Sum Sq Mean Sq F value  Pr(&amp;gt;F)  
Exer        2   900.5  450.23  3.3783 0.03618 *
Residuals 189 25188.2  133.27                  
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Conclusions: We can reject the null hypothesis that there is no difference in mean pulse rate of students with different amount of exercise&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Report the degrees of freedom (df), the F ratio and the P value like
&lt;span class=&#34;math display&#34;&gt;\[F_{2,189} = 3.38, P = 0.036\]&lt;/span&gt;
Where 2 is the df of the groups (&lt;code&gt;Exer&lt;/code&gt;) and 189 is the df of the residual error (&lt;code&gt;Residuals&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-anova-table&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The ANOVA table&lt;/h1&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;56%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Source of variation&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SS&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;MS&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;F&lt;/th&gt;
&lt;th&gt;P&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Variance among groups&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSR&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;number of levels - 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(MSR = \frac{SSR}{df}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{MSR}{MSE}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Variance within group&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSE&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;number of observations - 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(MSE = \frac{SSE}{df}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Total error&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSY&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;total number of observations - 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;calculating-group-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating group means&lt;/h2&gt;
&lt;p&gt;Check &lt;code&gt;summary()&lt;/code&gt; to see group means&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Pulse ~ Exer, data = survey)

Residuals:
    Min      1Q  Median      3Q     Max 
-41.188  -7.968  -0.188   7.812  32.032 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)   71.968      1.184  60.763   &amp;lt;2e-16 ***
ExerNone       4.796      3.040   1.578    0.116    
ExerSome       4.219      1.752   2.408    0.017 *  
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 11.54 on 189 degrees of freedom
  (45 observations deleted due to missingness)
Multiple R-squared:  0.03452,   Adjusted R-squared:  0.0243 
F-statistic: 3.378 on 2 and 189 DF,  p-value: 0.03618&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Groups are calculated in alphabetical order - Here “Frequent” group is calculated first&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Estimate&lt;/code&gt; column shows the &lt;strong&gt;difference in means&lt;/strong&gt; - see below&lt;/li&gt;
&lt;li&gt;P values test differences in &lt;strong&gt;pairs of means&lt;/strong&gt; BUT should we be doing multiple comparisons? (no)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model coefficients:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(Intercept)    ExerNone    ExerSome 
  71.968421    4.796285    4.219079 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R-calculated means:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Freq     None     Some 
71.96842 76.76471 76.18750 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Manually calculated means for “None” group:
71.9684211 + 4.7962848 = 76.7647059&lt;/p&gt;
&lt;p&gt;Manually calculated means for “Some” group:
71.9684211 + 4.2190789 = 76.1875&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;anova-and-hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ANOVA and hypothesis testing&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Important to remember:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ANOVA is a test of means (analysis of variance about the mean)&lt;/li&gt;
&lt;li&gt;F-ratio tells you that there is a difference in the means between pairs of groups but NOT which pairs are different&lt;/li&gt;
&lt;li&gt;Need post-hoc tests like Tukey’s Honest Significant Differences (not covered in this lecture) to see &lt;em&gt;which pairs&lt;/em&gt; are significantly different&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;recap-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recap time!&lt;/h1&gt;
&lt;p&gt;Test your understanding so far by answering the questions below. Click &lt;code&gt;Code&lt;/code&gt; to see the answer.&lt;/p&gt;
&lt;p&gt;What is the significance of fixed and random variables for drawing conclusions?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;It changes the inferred conclusion. In fixed models, the identities of the groupings are the main level of inference whereas in random models we consider the identity of the group to be randomly selected from a wider pool of candidates. Thus we can generalise our findings beyond our selected groups in random models but are limited to the groups we selected in fixed models. 

For example, if we counted the population of 5 towns in Ireland and analysed the data using a fixed model, then we could only state our conclusion for those 5 towns and cannot draw conclusions for the whole of Ireland.

With a random model, we consider those 5 towns to be representitive of the whole of Ireland, we are not interested in those 5 towns in particular, thus we can generalise our conclusions to the whole of Ireland. E.g. those 5 towns were randomly selected from all the candidate towns to survey.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the function to conduct an Analysis of Variance in R?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Either summary(aov()) or anova(lm()) will do a Type I ANOVA. Type II and Type III models require additional packages.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many groups were in the categorical predictor variable in this one-way analysis of variance? &lt;span class=&#34;math inline&#34;&gt;\(F_{5,24} = 14.23, P &amp;lt; 0.001\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;6 groups. The degree of freedom for group is 5 and this is calculated from the number of groups - 1. So 5 + 1 = 6.&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple regression&lt;/h1&gt;
&lt;p&gt;We can include more than 1 predictor variable in our models&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Multiple regression: Linear models with two or more predictor variables&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Models the effect of multiple predictors on response&lt;/li&gt;
&lt;li&gt;Two-way ANOVAs etc. are special cases of multiple regression&lt;/li&gt;
&lt;li&gt;If 3 categorical predictors, then three-way ANOVA etc…&lt;/li&gt;
&lt;li&gt;With more predictor variables, the model becomes complex
&lt;ul&gt;
&lt;li&gt;And potentially less informative/difficult to interpret&lt;/li&gt;
&lt;li&gt;Remember the trade-off between generality &amp;amp; complexity&lt;/li&gt;
&lt;li&gt;Keep it simple&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;height-and-hand-width&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Height and hand width&lt;/h2&gt;
&lt;p&gt;Imagine we asked the class &lt;em&gt;is there a relationship between your height and the width of your hand?&lt;/em&gt;&lt;br /&gt;
The linear model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ height  = \beta_0 + \beta_1 hand \space width + \varepsilon_i\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:hand-height&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/hand-height-1.png&#34; alt=&#34;The relationship between hand width and height of students&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: The relationship between hand width and height of students
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Good, but let’s ask is it also important to consider the sex of the student?&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:hand-height-sex&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/hand-height-sex-1.png&#34; alt=&#34;The relationship between hand width and height of male (yellow) or female (purple) students&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: The relationship between hand width and height of male (yellow) or female (purple) students
&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Does including information about sex improve our ability to detect/understand patterns?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The linear model now has two predictor variables and is written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ height  = \beta_{0_{sex}} + \beta_1 hand \space width + \varepsilon_i\]&lt;/span&gt;
Notice that the intercept parameter (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0_{sex}}\)&lt;/span&gt;) now specifies that it is dependent on the sex of the student.&lt;/p&gt;
&lt;p&gt;Since Sex is a categorical predictor with 2 levels, this model now describes two lines - one for each level of the predictor (male/female)&lt;/p&gt;
&lt;div id=&#34;does-student-height-vary-with-hand-width-and-between-sexes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Does student height vary with hand width and between sexes?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Continuous response&lt;/li&gt;
&lt;li&gt;Categorical predictor&lt;/li&gt;
&lt;li&gt;Continuous predictor (also called covariate)&lt;/li&gt;
&lt;li&gt;Can be analysed using multiple linear models&lt;/li&gt;
&lt;li&gt;Two types:
&lt;ul&gt;
&lt;li&gt;Additive model (&lt;strong&gt;Fixed slopes model&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Interactive model (&lt;strong&gt;Random slopes model&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Analysis of Covariance is a special case with unique assumptions (not considered here), else called a general linear model&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-slopes-additive-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed slopes (additive model)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;No interaction term&lt;/li&gt;
&lt;li&gt;Same slope&lt;/li&gt;
&lt;li&gt;Intercept for each sex&lt;/li&gt;
&lt;li&gt;in &lt;code&gt;R&lt;/code&gt;: &lt;code&gt;lm(Y ~ A + B, data)&lt;/code&gt; where A &amp;amp; B are the two predictor variables&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;H0: There is no effect of A or B on the response variable
H1: There is an effect of A or B on the response variable&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:same-slope&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/same-slope-1.png&#34; alt=&#34;the lines are parallel to each other&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: the lines are parallel to each other
&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;(Intercept)      Wr.Hnd     SexMale 
 137.686951    1.594446    9.489814 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Parametrised slope for females:&lt;br /&gt;
&lt;em&gt;Female height&lt;/em&gt; = 137.687 + 1.594 &lt;em&gt;hand width&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Parametrised slope for males:&lt;br /&gt;
&lt;em&gt;Male height&lt;/em&gt; = (137.687 + 9.49) + 1.594 &lt;em&gt;hand width&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember R shows the difference between parameter estimates so you need to extract the correct values&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;random-slopes-interactive-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random slopes (interactive model)&lt;/h2&gt;
&lt;p&gt;The random slopes model has in interaction term. In R this is coded &lt;code&gt;lm(Height ~ Wr.Hnd * Sex, survey)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The linear model is now:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ height  = \beta_{0_{sex}} + \beta_{1_{sex}} hand \space width + \varepsilon_i\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:random-slopes&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM2_files/figure-html/random-slopes-1.png&#34; alt=&#34;The lines have different slopes and intercepts&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: The lines have different slopes and intercepts
&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;   (Intercept)         Wr.Hnd        SexMale Wr.Hnd:SexMale 
   147.4497131      1.0385045     -7.1567184      0.9020249 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To parametrise the model we now need to calculate different values for slopes. In the coefficient list, &lt;code&gt;Wr.Hnd:SexMale&lt;/code&gt; is the &lt;strong&gt;difference in the value of the slope relative to the slope for female&lt;/strong&gt; &lt;code&gt;Wr.Hnd&lt;/code&gt;. Remember: female is alphabetically before male so R calculates the variances for females first, then males.&lt;/p&gt;
&lt;p&gt;Parametrised slope for females:&lt;br /&gt;
&lt;em&gt;Female height&lt;/em&gt; = 147.45 + 1.039 &lt;em&gt;hand width&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Parametrised slope for males:&lt;br /&gt;
&lt;em&gt;Male height&lt;/em&gt; = (147.45 -7.157) + (1.039 + 0.902) &lt;em&gt;hand width&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;H0: There is no effect of A or B or their interaction on the response variable
H1: There is an effect of A or B or their interaction on the response variable&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multiple-regression-anova-table&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The multiple regression ANOVA table&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Variance of predictors (SS) is partitioned out from total SS in order it is entered in to R&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sum Sq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean Sq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;F value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;F)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wr.Hnd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7298.70354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7298.70354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150.128633&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.359137e-26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2912.37262&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2912.37262&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;59.905231&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.604092e-13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wr.Hnd:Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.06608&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.06608&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.852589&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.749916e-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Residuals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;203&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9869.11550&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48.61633&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;the-anova-table-for-2-predictors-and-their-interaction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ANOVA table for 2 predictors and their interaction&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Source of variation&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SS&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;MS&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;F&lt;/th&gt;
&lt;th&gt;P&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Factor A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSR of A&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;number of levels of A - 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Factor B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSR of B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;number of levels of B - 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Factor A x B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSR of A &amp;amp; B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;df of A x df of B&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Within error&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSE&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;levels of A x levels of B X (number of observations - 1)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Total error&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;SSY&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(levels of A x levels of B X number of observations) - 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;interpreting-the-anova-table&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpreting the ANOVA table&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;P(Interaction term) &amp;gt; 0.05, thus the effect of hand width on
student height is not dependent of the sex of the student&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sum Sq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean Sq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;F value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;F)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wr.Hnd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7298.70354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7298.70354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150.128633&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.359137e-26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2912.37262&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2912.37262&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;59.905231&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.604092e-13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Wr.Hnd:Sex&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.06608&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.06608&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.852589&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.749916e-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Residuals&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;203&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9869.11550&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48.61633&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;the-interaction-term-ab&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The interaction term A:B&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Measures the dependence of the level of one factor on the level of the other factors&lt;/li&gt;
&lt;li&gt;If interaction term is not significant:
&lt;ul&gt;
&lt;li&gt;effect of one factor on other is additive (independent of each other)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If significant:
&lt;ul&gt;
&lt;li&gt;indicates synergistic or antagonistic effects between factors&lt;/li&gt;
&lt;li&gt;Should be main conclusion of analysis&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-fundamentals-of-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The fundamentals of Linear regression&lt;/h1&gt;
&lt;p&gt;That’s the basics of linear regression!&lt;br /&gt;
They are widely used in biological statistics so you’ll likely come across them when reading scientific studies.&lt;/p&gt;
&lt;p&gt;The same concepts apply for any number or combination of predictor variables. Two, three, four predictor variables and so forth…it just becomes more complex to parameterise.&lt;/p&gt;
&lt;p&gt;Remember, the aim is to quantify how much variation in the response variable is attributable to each predictor variable - minimise those residuals!&lt;/p&gt;
&lt;p&gt;Common statistical analyses you should be able to do by applying the concepts in these lectures:
1 continuous response variable &amp;amp;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 continuous predictor variable - simple linear regression&lt;/li&gt;
&lt;li&gt;2 continuous predictor variables (with/without their interaction) - multiple regression&lt;/li&gt;
&lt;li&gt;1 categorical predictor variable - One-way ANOVA&lt;/li&gt;
&lt;li&gt;1 continuous predictor variable &amp;amp; 1 categorical predictor variable (with/without their interaction) - ANCOVA/multiple regression&lt;/li&gt;
&lt;li&gt;2 categorical predictor variables (with/without their interaction) - Two-way ANOVA&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;recap-time-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recap time!&lt;/h1&gt;
&lt;p&gt;Test your understanding so far by answering the questions below. Click &lt;code&gt;Code&lt;/code&gt; to see the answer.&lt;/p&gt;
&lt;p&gt;What is the difference between an additive and interactive linear model?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;An additive model does not describe an interaction between the two predictor variables. An interactive model describes that the effect of one predictor variable on the response variable is dependent on the second predictor variable.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the function to conduct an interactive multiple regression model in R?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;lm(Y ~ A * B, data) where A &amp;amp; B are the two predictor variables. * denotes the interaction.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A multiple regression of crab shell width (response) with crab body depth (continuous predictor) for two species (categorical predictor: B or O) and their interaction had the following coefficients:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.6942643&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BD&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.5449206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;spO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.2531624&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BD:spO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1756934&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;What is the predicted shell length (mm) of a species O crab with a body depth of 15mm?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Answer: 36.97951 mm

Because B is alphabetically before O, the first parameter coefficients refer to species B. 
The intercept for species O is 2.6942643-1.2531624 = 1.441102
The slope for species O is  2.5449206-0.1756934 = 2.369227
Thus the whole model for species O is:
  shell length = 2.369227 * body depth + 1.441102

Using a value of 15 for body depth:
  shell length = 2.369227 * 15 + 1.441102
               = 36.97951 mm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Generalised linear models</title>
      <link>https://jacintak.github.io/teaching/GLM/GLM3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jacintak.github.io/teaching/GLM/GLM3/</guid>
      <description>
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;div id=&#34;overall-lecture-aims&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overall lecture aims&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Identify the structure of general linear models&lt;/li&gt;
&lt;li&gt;Describe how linear models are parametrized&lt;/li&gt;
&lt;li&gt;Understand how to use linear models&lt;/li&gt;
&lt;li&gt;Understand how to interpret and evaluate linear models&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;by-the-end-of-this-lecture-you-should&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;By the end of this lecture you should:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Know how the coefficient of determination is calculated and its interpretation&lt;/li&gt;
&lt;li&gt;Understand the assumptions of linear regression&lt;/li&gt;
&lt;li&gt;Be able to evaluate the appropriateness of a linear model to data based on residual plots&lt;/li&gt;
&lt;li&gt;Be able to identify generalised linear models appropriate for data&lt;/li&gt;
&lt;li&gt;Create residual plots in R&lt;/li&gt;
&lt;li&gt;Conduct a generalised linear model in R&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;by-now-you-should-be-comfortable-with-building-and-interpreting-basic-linear-models-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;By now you should be comfortable with building and interpreting basic linear models in R&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;but how do we know whether our model is a “good” one?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We need to evaluate our model. There are a few things we should consider:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How much variation in the data is explained by the model?&lt;/li&gt;
&lt;li&gt;Are linear models appropriate for our hypotheses?&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-a-linear-model-tell-us&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What does a linear model tell us?&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What will a new value of Y be, given a new value of X?&lt;/li&gt;
&lt;li&gt;Does the population slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; differ to 0?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How much variation in Y can be explained its linear relationship with X?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Coefficient of determination (&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Partitioning variance (F ratio)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-much-variation-in-y-can-be-explained-its-linear-relationship-with-x&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How much variation in Y can be explained its linear relationship with X?&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{Var_{\text{explained by the line}}}{Var_{\text{not explained by the line}}} = Ratio\]&lt;/span&gt;
Interpreting ratios:&lt;br /&gt;
Ratio &amp;gt; 1 = Line explains more than residual&lt;br /&gt;
Ratio ≤ 1 = Line explains very little (null hypothesis)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We need to know the total amount of variation and all possible sources of variation (like the F-ratio &amp;amp; ANOVA)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;what-is-or-isnt-explained-by-the-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is or isn’t explained by the line?&lt;/h2&gt;
&lt;div id=&#34;isnt-sum-of-squares-of-the-error-ssy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Isn’t: Sum of Squares of the Error (SSY)&lt;/h3&gt;
&lt;p&gt;The bit not explained by the null (total variation in the data). Remember, the null is &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt; = the mean of Y.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SSY = y_i - \bar{y}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:SSY&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/SSY-1.png&#34; alt=&#34;Black line is the mean of Y. Blue lines are the difference between the mean and a single observation = SSY&#34; width=&#34;500&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Black line is the mean of Y. Blue lines are the difference between the mean and a single observation = SSY
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;isnt-sum-of-squares-of-the-residual-sse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Isn’t: Sum of Squares of the Residual (SSE)&lt;/h3&gt;
&lt;p&gt;The bit not explained by the line&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SSE = y_i - \hat{y_i}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:SSE&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/SSE-1.png&#34; alt=&#34;Residuals (blue lines) are the difference between the data point and the predicted line (black line)&#34; width=&#34;500&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Residuals (blue lines) are the difference between the data point and the predicted line (black line)
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;isnt-sum-of-squares-of-the-regression-ssr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Isn’t: Sum of Squares of the Regression (SSR)&lt;/h3&gt;
&lt;p&gt;How well the line estimates the mean of Y&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[SSR = \hat{y_i} - \bar{y}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:SSR&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/SSR-1.png&#34; alt=&#34;Residuals (blue lines) are the difference between the null hypothesis (mean of observations, dashed lines) and the predicted line (black line)&#34; width=&#34;500&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Residuals (blue lines) are the difference between the null hypothesis (mean of observations, dashed lines) and the predicted line (black line)
&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice that since SSY is all variation in the data:&lt;br /&gt;
SSY = SSE + SSR&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;so-which-bits-do-we-use-to-evaluate-model-fit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;So which bits do we use to evaluate model “fit”?&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{Var_{\text{explained by the line}}}{Var_{\text{not explained by the line}}} = Ratio\]&lt;/span&gt;
We want the number above the line (nominator) to be larger than the number below the line (denominator), otherwise we cannot be confident that our results are different to randomly generated numbers.&lt;/p&gt;
&lt;p&gt;Interpreting ratios:&lt;br /&gt;
Ratio &amp;gt; 1 = Line explains more than residual&lt;br /&gt;
Ratio ≤ 1 = Line explains very little (null hypothesis)&lt;/p&gt;
&lt;p&gt;In other words, we want to know how much varaition is captured by the model relative to the total variation in our data.&lt;/p&gt;
&lt;p&gt;What is the error that tells us how much varaition the line is (H1) explaining relative to our null hypothesis (H0)?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;SSR - Sum of squares of the regression.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the error that tells us the total variation in our data?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;SSY - Sum of squares of the error. Sometimes called SST - total sum of squares&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can evaluate how much varaition the model explains by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{SSR}{SSY} = R^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Why can’t we use SSE as the denominator?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;SSE depends on the total variation of Y. We could have more variation simply by having more data but the ratio values are the same. So using sum of squares does not tell us about how much variation is explained by our model in a way that is unbiased. &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-determination-r2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Coefficient of determination &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;This is the proportion of variation that your model (your line) explains&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1 = no deviance from line (good)&lt;br /&gt;
0 = strong deviance from line (not good)&lt;/p&gt;
&lt;p&gt;It is related to correlation coefficients (r). Basically, &lt;span class=&#34;math inline&#34;&gt;\(R^2 = r^2\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:fit&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/fit-1.png&#34; alt=&#34;Which fits better?&#34; width=&#34;300&#34; /&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/fit-2.png&#34; alt=&#34;Which fits better?&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Which fits better?
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;R will calculate &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; for you. Going back the tree height and girth example, the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; is 0.27 (Multiple R-squared).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Height ~ Girth, data = trees)

Residuals:
     Min       1Q   Median       3Q      Max 
-12.5816  -2.7686   0.3163   2.4728   9.9456 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  62.0313     4.3833  14.152 1.49e-14 ***
Girth         1.0544     0.3222   3.272  0.00276 ** 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 5.538 on 29 degrees of freedom
Multiple R-squared:  0.2697,    Adjusted R-squared:  0.2445 
F-statistic: 10.71 on 1 and 29 DF,  p-value: 0.002758&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Conclusion: the model (your fitted line) explains 27 % of total variation in data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;But…&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Biological data is messy, thus low &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; may be biologically acceptable&lt;/li&gt;
&lt;li&gt;Low &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; does not always mean a bad model&lt;/li&gt;
&lt;li&gt;Consider multiple regression to explain more variance
&lt;ul&gt;
&lt;li&gt;but adding more parameters is not always better. Why?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;but-there-are-other-things-to-check-that-are-more-important&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;But there are other things to check that are more important!&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;We make assumptions of the error structure in linear regression (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Remember:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[Y_i = \beta_0 + \beta_1 \times X_i + \varepsilon_i\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does not change our estimates of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Affects our confidence intervals of the estimate and thus hypothesis testing&lt;/li&gt;
&lt;li&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt; is random, our assumptions also apply to the response variable &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions-of-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions of linear regression&lt;/h1&gt;
&lt;p&gt;Linear models assume that the relationship between the response and predictor is &lt;em&gt;linear&lt;/em&gt;. In addition to this main condition, there are 4 assumptions of linear regression:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Normality&lt;/li&gt;
&lt;li&gt;Homogeneity of Variance&lt;/li&gt;
&lt;li&gt;Independence&lt;/li&gt;
&lt;li&gt;Fixed X&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ALWAYS&lt;/strong&gt; check these assumptions every time you fit a model. No exceptions! No excuses!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;residual-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Residual plots&lt;/h2&gt;
&lt;p&gt;In R, we can evaluate models from &lt;strong&gt;residual plots&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Model &amp;lt;- lm(Y ~ X, data) # build a model
plot(Model) # show residual plots&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 4 plots called. In order:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Residuals vs fitted values
&lt;ul&gt;
&lt;li&gt;bits left over vs what the model predicted&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Standardised residual quantile-quantile plot
&lt;ul&gt;
&lt;li&gt;what is the spread of the residuals?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Scale-Location
&lt;ul&gt;
&lt;li&gt;like plot 1 but shown differently&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Residuals vs Leverage
&lt;ul&gt;
&lt;li&gt;are there any influential data points?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;mammal-brain-and-body-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mammal brain and body size&lt;/h2&gt;
Let’s look at these assumptions in detail using the relationship between brain mass and body mass for different mammals. The dataset is called &lt;code&gt;mammals&lt;/code&gt; in the &lt;code&gt;MASS&lt;/code&gt; package.&lt;br /&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:mammal-brains&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/mammal-brains-1.png&#34; alt=&#34;The relationship between mammal body size and brain size&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: The relationship between mammal body size and brain size
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Normality&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Population Y values and error terms (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;) are normally distributed for each level of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The distribution of the response variable, Y, should be normally distributed (not skewed). We can graphically check this using a histogram of brain size.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/mammal-hist-1.png&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/mammal-hist-2.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantile-Quantile plots&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can also visualise the spread of data with a quantile-quantile plot.&lt;/li&gt;
&lt;li&gt;The linear line is the expected relationship following a normal distribution.
&lt;ul&gt;
&lt;li&gt;Do our points follow the line?&lt;/li&gt;
&lt;li&gt;What does it mean when the points don’t follow the line?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/mammal-quantile-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-residuals-of-the-model-should-be-also-normally-distributed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The residuals of the model should be also normally distributed&lt;/h2&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:standard&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/standard-1.png&#34; alt=&#34;Looks like distribution of brain size is skewed to the right. What does this mean biologically?&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Looks like distribution of brain size is skewed to the right. What does this mean biologically?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-happens-when-the-data-is-not-normal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What happens when the data is not normal?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Collect more data, increase sample size for each level of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Could be a sampling bias or sample size is too low&lt;/li&gt;
&lt;li&gt;Use a non-parametric test
&lt;ul&gt;
&lt;li&gt;e.g. Spearman’s Rank Correlation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Ignore it (with good reason). Linear regressions are robust to skewness&lt;/li&gt;
&lt;li&gt;Some data are never normal
&lt;ul&gt;
&lt;li&gt;e.g. counts - should use Poisson distribution&lt;/li&gt;
&lt;li&gt;Fit another statistical model with more appropriate error structures&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Transform the data&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;applying-a-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applying a transformation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Some transformations:
&lt;ul&gt;
&lt;li&gt;Log or natural log&lt;/li&gt;
&lt;li&gt;Square root or cube root&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Note: &lt;code&gt;log(0)&lt;/code&gt; or &lt;code&gt;log(-1)&lt;/code&gt; is undefined so you could make data positive and greater than 0 before you log transform them.&lt;/li&gt;
&lt;li&gt;More sophisticated transformations not covered in this module&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;transforming-brain-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transforming brain size&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let’s try some transformations on the data&lt;/li&gt;
&lt;li&gt;What is the transformation doing?&lt;/li&gt;
&lt;li&gt;Which would you choose?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/transform-1.png&#34; width=&#34;800&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;re-run-the-model-with-transformed-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Re-run the model with transformed data&lt;/h2&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:new-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/new-graph-1.png&#34; alt=&#34;That looks better!&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: That looks better!
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you transform data, then the model estimates refer to the transformed units. Remember to transform them back to their correct units.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;homogeneity-of-variance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Homogeneity of Variance&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Population Y values and error terms (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;) have the same variance for each level of the predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Related to the assumptions of Normality but more important!&lt;/li&gt;
&lt;li&gt;Look at patterns in standardised residuals:
&lt;ul&gt;
&lt;li&gt;Quantile plot&lt;/li&gt;
&lt;li&gt;Relationship with fitted values (predicted Y values from line)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Causes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;small sample size&lt;/li&gt;
&lt;li&gt;outliers&lt;/li&gt;
&lt;li&gt;non-normally distributed variables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fixes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Have properly designed experiments&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Collect enough data&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Deal with it like as normality&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;should-expect-a-normal-distribution-of-standardised-residuals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Should expect a &lt;strong&gt;normal distribution&lt;/strong&gt; of standardised residuals&lt;/h2&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:qqnormresid&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/qqnormresid-1.png&#34; alt=&#34;data falls along line = good&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: data falls along line = good
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;are-there-trends-in-the-residual-vs-fitted-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Are there trends in the residual vs fitted values?&lt;/h2&gt;
&lt;p&gt;Expect no relationship between standardised (or non-standardised) residuals and fitted values of model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;straight line = good&lt;/li&gt;
&lt;li&gt;no humps or valleys&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/resid-1.png&#34; width=&#34;800&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;independence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Independence&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Population Y values and error terms (&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;) are independent&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;They do not influence each other (not autocorrelated)&lt;/li&gt;
&lt;li&gt;Often because of inappropriate experimental design
&lt;ul&gt;
&lt;li&gt;time series&lt;/li&gt;
&lt;li&gt;pseudo-replication&lt;/li&gt;
&lt;li&gt;repeated measurements&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Increases Type I error&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;dealing-with-independence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dealing with independence&lt;/h2&gt;
&lt;p&gt;Best thing is to choose a different model&lt;br /&gt;
Consider using random effects models or choosing better variables (i.e. good experimental design)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-x&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Fixed X&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;The predictor variable (&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;) is fixed. i.e. a known constant&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Called Type I model or fixed effects model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Often broken in biological stats&lt;/li&gt;
&lt;li&gt;Predictor variables can be random
&lt;ul&gt;
&lt;li&gt;Called Type II (random effects model)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Hypothesis testing of Type I applies to Type II&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;other-regression-diagnostics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Other regression diagnostics&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;How well does the model fit the data?
&lt;ul&gt;
&lt;li&gt;Coefficient of determination &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Is a simple linear regression appropriate?
&lt;ul&gt;
&lt;li&gt;e.g. polynomial or curvilinear model&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Are there effects of outliers in the model?&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;outliers-leverage-and-influence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outliers, leverage and influence&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Outliers are abnormal or unusual observations relative to the rest of the data that can cause biases during analysis&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Outliers can be checked before applying a model. How?&lt;/li&gt;
&lt;li&gt;Sometimes caused by experimental error but sometimes significantly different data points are not outliers
&lt;ul&gt;
&lt;li&gt;Natural variation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Leverage = how much x influences y&lt;/li&gt;
&lt;li&gt;Influence = how much x influences the slope of the line (Cook’s Distance)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If outliers are caused by experimental error or bias, you could justify excluding it from data.&lt;br /&gt;
&lt;strong&gt;But &lt;em&gt;never&lt;/em&gt; delete observations to force a better model fit&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outliers-in-the-mammal-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outliers in the mammal dataset&lt;/h2&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:outliers&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/outliers-1.png&#34; alt=&#34;Looks like humans, water opossums &amp;amp; musk shrew have high influence on the regression&#34; width=&#34;300&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Looks like humans, water opossums &amp;amp; musk shrew have high influence on the regression
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting it all together&lt;/h1&gt;
&lt;p&gt;These assumptions can be checked by looking at the residual plots. R shows residual plots using the function &lt;code&gt;plot(lm())&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/residual-plot-1.png&#34; width=&#34;800&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s evaluate the residual plot, starting from the top left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are the residuals vs fitted values equal (i.e. a straight line)? If there are humps or valleys, the model may not be appropriate for the data.&lt;/li&gt;
&lt;li&gt;Are the standardised residuals normally distributed? Linear models assume that residuals are normally distributed. If not, your model is inappropriate for your data or your data is skewed in some way.&lt;/li&gt;
&lt;li&gt;Is there a pattern to your &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\mbox{Standardised residuals}}\)&lt;/span&gt;? Linear models assume equal variance so there should be no pattern in your residuals.&lt;/li&gt;
&lt;li&gt;Are there any outlier data points that have strong leverage in the model? E.g. potential outliers or influential data points.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;ancova&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ANCOVA&lt;/h1&gt;
&lt;p&gt;Remember Analysis of Covariance deals with the effect of two predictor variable, a continuous and categorical variable, on a continuous response variable. Like an ANOVA adjusted for the effect of an additional continuous covariate.&lt;br /&gt;
Follows all the assumptions above &lt;em&gt;plus two extra ones&lt;/em&gt;:&lt;/p&gt;
&lt;div id=&#34;covariate-values-cover-a-similar-range-across-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Covariate values cover a similar range across groups&lt;/h2&gt;
&lt;p&gt;Data from groups should overlap across the range of the continuous variable&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:ancova&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/ancova-1.png&#34; alt=&#34;Which of these violates ANCOVA assumptions?&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/ancova-2.png&#34; alt=&#34;Which of these violates ANCOVA assumptions?&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Which of these violates ANCOVA assumptions?
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If the first assumption is not met, ANCOVA fails to separate the effects of the two predictors on the response variable.&lt;/p&gt;
&lt;p&gt;Extending regression models beyond the range of data could be &lt;em&gt;extrapolation&lt;/em&gt; and lead to incorrect conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-slopes-are-similar-across-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Regression slopes are similar across groups&lt;/h2&gt;
&lt;p&gt;Like a fixed or additive linear regression.&lt;br /&gt;
If the second assumption is not met, you can still fit an “ANCOVA-like” model to the data with different slopes for different groups (i.e. a mixed or random model). Then it’s not a true ANCOVA in the classic sense.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-data-are-never-normal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some data are never normal&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Non-linear data - e.g. exponential growth of bacteria, human populations&lt;/li&gt;
&lt;li&gt;Counts or proportions, number of cells in petri dish, proportion of animals that survive&lt;/li&gt;
&lt;li&gt;Categorical data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simple linear regressions aren’t the most appropriate for these data and may give distorted results.&lt;br /&gt;
&lt;em&gt;But&lt;/em&gt; we can use linear regression to analyse these data if we relax our assumptions a bit…called “general linear models”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalised-linear-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Generalised linear models&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;General linear models&lt;/em&gt;, or just linear models (LM), refer to linear regression of data following a normal probability distribution, fixed or random. If we do a linear regression on based on a non-normal probability distribution that’s called a &lt;em&gt;general&lt;strong&gt;ised&lt;/strong&gt; linear model&lt;/em&gt; (usually abbreviated to GLM).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are many types but we will briefly describe two common probability distributions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binomial regression - for binomial data following a binomial distribution
&lt;ul&gt;
&lt;li&gt;yes/no&lt;/li&gt;
&lt;li&gt;alive/dead&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Poisson regression - for count or contingency table data following a Poisson distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt; GLMs are done using &lt;code&gt;glm(Y ~ X, data, family = &amp;lt;insert here&amp;gt;)&lt;/code&gt;. &lt;code&gt;family&lt;/code&gt; refers to the underlying probability distribution. Using &lt;code&gt;family = gaussian&lt;/code&gt; is the same as &lt;code&gt;lm()&lt;/code&gt; for a normal distribution. So:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;poisson&lt;/code&gt; for a Poisson distribution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;binomial&lt;/code&gt; for a binomial distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But the model outputs are interpreted differently to normal distributions because they &lt;em&gt;link&lt;/em&gt; the relationship between the predictor and the response differently.&lt;/p&gt;
&lt;div id=&#34;poisson-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Uses a log linear link function where the response variable (counts) is on a log scale&lt;/li&gt;
&lt;li&gt;Interpretation is similar to lm with estimates for the intercept, slopes and differences in estimates among groups
&lt;ul&gt;
&lt;li&gt;Estimates are the &lt;strong&gt;log of the expected count&lt;/strong&gt; as a function of the predictor variables&lt;/li&gt;
&lt;li&gt;Cannot have negative or 0 values because log(0) does not work - must deal with them. How?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Test overall effects of predictors by comparing models with different predictors representing null and alternative hypotheses&lt;/li&gt;
&lt;li&gt;See later lectures about Chi-Squared tests (&lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;number-of-oystercatchers-wetland-birds-in-south-africa&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Number of Oystercatchers (wetland birds) in South Africa&lt;/h3&gt;
&lt;p&gt;Data is &lt;code&gt;waders&lt;/code&gt; from &lt;code&gt;MASS&lt;/code&gt;. I have maniuplated the data so it is suitable for analysis - code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create data set of oystercatcher counts (column 1) and site labels
waders &amp;lt;- data.frame(Oystercatcher = MASS::waders[,1],
                     site = letters[1:15])
# poisson regression
summary(glm(Oystercatcher + 1 ~ site, waders, family = poisson))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = Oystercatcher + 1 ~ site, family = poisson, data = waders)

Deviance Residuals: 
 [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients:
            Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)   2.5649     0.2774   9.248  &amp;lt; 2e-16 ***
siteb         2.0402     0.2948   6.920 4.51e-12 ***
sitec         2.7233     0.2863   9.512  &amp;lt; 2e-16 ***
sited        -2.5649     1.0377  -2.472   0.0134 *  
sitee         1.7918     0.2996   5.981 2.22e-09 ***
sitef         0.4308     0.3563   1.209   0.2266    
siteg         4.3665     0.2791  15.645  &amp;lt; 2e-16 ***
siteh         1.9124     0.2971   6.436 1.23e-10 ***
sitei         4.1058     0.2796  14.683  &amp;lt; 2e-16 ***
sitej         1.8539     0.2983   6.215 5.12e-10 ***
sitek         3.5984     0.2811  12.800  &amp;lt; 2e-16 ***
sitel         1.7918     0.2996   5.981 2.22e-09 ***
sitem         0.5705     0.3470   1.644   0.1001    
siten        -2.5649     1.0377  -2.472   0.0134 *  
siteo        -2.5649     1.0377  -2.472   0.0134 *  
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 5.3666e+03  on 14  degrees of freedom
Residual deviance: 1.6653e-14  on  0  degrees of freedom
AIC: 114.06

Number of Fisher Scoring iterations: 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why did I add 1 to the variable Oystercatcher in the poisson regression?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;1 is a dummy variable to remove 0s from the counts - or log(0) will mess up the estimation of parameters&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the link function for a poisson regression?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;(natural) log link&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the expected number of Oystercatchers at site k?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;474 oystercatchers

Remember in linear regression (and by extension GLMs) the Intercept estimate is the estimated coefficient for the first site (site a) and the rest are the difference between site a and the respective site. So for site k you need to add the estimated coefficients together.

Coefficient for k:  2.5649 + 3.5984 = 6.1633
But remember this is log counts so you need to transform it back to regular counts:
  exp(6.1633) = 474.993
Then remember we added a dummy variable so you need to subtract that from our estimate:
  474.993 - 1 = 473.993 which rounded to the nearest whole number (as counts are discrete variables) is 474!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial (logistic) regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Uses a logit link function so the response variable is the &lt;strong&gt;log odds&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Estimates are log odds and the probability of an event
&lt;ul&gt;
&lt;li&gt;Intercept is log odds of first group&lt;/li&gt;
&lt;li&gt;Estimates for other groups are the ratio of log odds&lt;/li&gt;
&lt;li&gt;Transform from log odds to odds using &lt;code&gt;exp()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is the function to conduct an Poisson regression in R?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;glm(Y ~ X, data, family = poisson)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;melanoma-tumor-thickness-and-survival&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Melanoma tumor thickness and survival&lt;/h3&gt;
&lt;p&gt;Data is &lt;code&gt;Melanoma&lt;/code&gt; from &lt;code&gt;MASS&lt;/code&gt;. I have maniuplated the data so it is suitable for analysis - code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Melanoma &amp;lt;- MASS::Melanoma
Melanoma &amp;lt;- subset(Melanoma, Melanoma$status != 3) # remove observations of other causes of death
Melanoma$status &amp;lt;- Melanoma$status - 1 # create binary variables: 0 = died, 1 = alive

# binary regression
model &amp;lt;- glm(status ~ thickness, Melanoma, family = binomial)
summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = status ~ thickness, family = binomial, data = Melanoma)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8588  -1.2162   0.6729   0.7485   1.8844  

Coefficients:
            Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)  1.61134    0.25496   6.320 2.62e-10 ***
thickness   -0.24853    0.06354  -3.911 9.18e-05 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 232.84  on 190  degrees of freedom
Residual deviance: 213.45  on 189  degrees of freedom
AIC: 217.45

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(status ~ thickness, Melanoma, pch = 19) # plot
# plot binomial regression in blue
newdat &amp;lt;- data.frame(thickness=seq(min(Melanoma$thickness), max(Melanoma$thickness),len=100))
newdat$status &amp;lt;- predict(model, newdata=newdat, type=&amp;quot;response&amp;quot;)
lines(status ~ thickness, newdat, col=&amp;quot;blue&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM3_files/figure-html/Melanoma-1.png&#34; width=&#34;300&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Is there a relationship between Melanoma tumor thickness (mm) and whether a patient survives?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Yes. The thicker the tumour, the greater the odds of death. The P value of the slope of the binomial regression is significantly different from 0. P &amp;lt; 0.001.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the coefficient estimate of &lt;code&gt;-0.24853&lt;/code&gt; for the variable &lt;code&gt;thickness&lt;/code&gt; represent?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;The estimate represents the log odds of survival as a function of tumour thickness. In other words, we expect the log odds of survival to decrease by 0.25 for a 1 mm increase in tumour thickness. 

We can transform log odds to odds by taking the exponential: exp(-0.24853) = 0.7799465. 1 - 0.7799465 = 0.22, so for every 1 mm increase in tumour thickness we expect the odds of survival to decrease by 22 %&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the probability of survival with a tumor 5 mm thick?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;The logit formula is logit = p/(1-p) and the regression equation is logit(p) = 1.61134 - 0.24853 * thickness
so logit(p) =  1.61134 - 0.24853 * 5 = 0.36869

and to turn logit p into probability (p):
p = exp(logit(p))/(1 + exp(logit(p)))
  = exp(0.36869)/(1 + exp(0.36869))
  = 0.5911424

The probability of surviving is 59 %&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wear suncreeen!&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-messages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Take home messages&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Always check assumptions&lt;/li&gt;
&lt;li&gt;Interpret model in a biological context
&lt;ul&gt;
&lt;li&gt;Biological data is messy: good model can have low R2&lt;/li&gt;
&lt;li&gt;Outliers aren’t always mistakes&lt;/li&gt;
&lt;li&gt;Statistical significance &lt;span class=&#34;math inline&#34;&gt;\(\neq\)&lt;/span&gt; biological significance&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Is the model appropriate for the question?&lt;/li&gt;
&lt;li&gt;Can my experiment actually test my hypothesis?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Never&lt;/em&gt; delete observations to force a better model fit or fit assumptions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear model assumptions</title>
      <link>https://jacintak.github.io/teaching/GLM/GLM4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jacintak.github.io/teaching/GLM/GLM4/</guid>
      <description>
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://jacintak.github.io/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;hr /&gt;
&lt;p&gt;This rmarkdown document contains some code for you to play with. You will need to copy and paste the code into your own R script (or Rmarkdown file). There are some questions for you to answer too. Click &lt;code&gt;code&lt;/code&gt; to see the answers to the questions. Some questions don’t have answers but you should think about them.&lt;/p&gt;
&lt;p&gt;Checking that our data meets the assumption of linear models is pretty important. Else, we might make the wrong conclusion! Unfortunately, these checks may be ignored or forgotten about but that’s no excuse.&lt;/p&gt;
&lt;p&gt;Here are some things to look out for:&lt;/p&gt;
&lt;div id=&#34;normality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Normality&lt;/h1&gt;
&lt;p&gt;Normality can be checked using Quantile-Quantile plots.&lt;br /&gt;
Quantile-Quantile (Q-Q) plots are useful alternatives to visualising distributions to density plots or histograms. They are easier to assess distribution and normality with than histograms.&lt;/p&gt;
&lt;div id=&#34;q-q-plot-of-single-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Q-Q plot of single variables&lt;/h2&gt;
&lt;p&gt;We can see whether a single variable has a normal distribution - specifically that the distribution is symmetrical or not skewed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;qqnorm(chickwts$weight) # the weight of chicks fed different diets (built in dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the quantiles of our observations are plotted against the theoretical quantiles if our observations followed a normal distribution.&lt;br /&gt;
&lt;strong&gt;Question&lt;/strong&gt; - What would you expect to see if our observations followed a normal distribution?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;Observations to fall along a straight line at approx a 45 degree angle&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can add a reference line to the above to help us evaluate how linear the Q-Q plot is. We can make the line red and thicker for fun.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;qqnorm(chickwts$weight) # the weight of chicks fed different diets (built in dataset)
qqline(chickwts$weight, col = &amp;quot;red&amp;quot;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do the observations follow a normal distribution?&lt;/p&gt;
&lt;p&gt;Compare the above with the histogram and density plots&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(density(chickwts$weight), col = &amp;quot;purple&amp;quot;) # density plot, purple for fun
hist(chickwts$weight, col = &amp;quot;yellow&amp;quot;) # histogram, yellow for fun&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Would you also conclude that the distribution of chick weights is normal?&lt;/p&gt;
&lt;p&gt;For comparison look at the Q-Q plots of a gamma distribution (distinctly not normal)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;qqnorm(rgamma(100, 3, 5))
qqline(rgamma(100, 3, 5)) # You should be able to see the skewness in the data. Compare with hist()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-happens-with-data-that-is-not-continuous&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What happens with data that is not continuous?&lt;/h2&gt;
&lt;p&gt;Q-Q plots also work with visualising data that is not continuous.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;qqnorm(Loblolly$age) # the ages of pine trees, can also try rbinom(100, 10, 0.5)
qqline(Loblolly$age)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt; - What is the main difference between the Q-Q plots of the continuous and discrete data?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;You can see the discrete nature of the observations in the clustered groupings in the QQ plot - the staircase pattern&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do you think that is?&lt;br /&gt;
You can also see it using &lt;code&gt;hist()&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q-q-plots-of-residuals-for-assessing-normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Q-Q plots of residuals for assessing normality&lt;/h2&gt;
&lt;p&gt;Q-Q plots permit comparison of two probability distributions when one distribution is the expected and the other is the observed distribution, then we can evaluate how well our observations follow our expected distribution. Using Q-Q plots we can assess skewness or identify outliers or influential points.&lt;/p&gt;
&lt;p&gt;Q-Q plots are automatically generated when calling plot on a linear model (&lt;code&gt;lm&lt;/code&gt;). It’s the second graph called (defined using which). You can also make one using &lt;code&gt;qqplot()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(lm(Height ~ Girth, trees), which = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does that look normal to you?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-use-of-q-q-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another use of Q-Q plots&lt;/h2&gt;
&lt;p&gt;We can also compare the distribution of two variables. If they are distributed equally then they should fall along the straight line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;qqplot(trees$Height, trees$Girth)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;par(mfrow = c(2,1))
hist(trees$Height)
hist(trees$Girth)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;homoscedasticity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Homoscedasticity&lt;/h1&gt;
&lt;p&gt;Homoscedasticity is the statistical term for homogeneity of variance. The opposite is called &lt;em&gt;Heteroscedasticity&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt; - What would you expect to see in a bar plot showing means and standard deviation for the assumption of homogeneity of variance to be met?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;We want the variation across our grouped observations to be similar so the error bars of standard deviation should also be similar.&lt;/code&gt;&lt;/pre&gt;
Look at this graph - does it show homoscedasticity or heteroscedasticity?
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:homo&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM4_files/figure-html/homo-1.png&#34; alt=&#34;Bar plot of mean of two groups (A &amp;amp; B). Error bars indicate standard deviation&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Bar plot of mean of two groups (A &amp;amp; B). Error bars indicate standard deviation
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here, you can see that group &lt;code&gt;B&lt;/code&gt; has a higher standard deviation than group &lt;code&gt;A&lt;/code&gt;. So, we would conclude that there is heteroscedasticity. We do not want heteroscedasticity because it would bias the calculations of variance if we were to do an ANOVA. Remember that ANOVA is a test of variance.&lt;/p&gt;
The same concept applies for scatter plots. Look at this plot - does it show homoscedasticity or heteroscedasticity?
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:scatter&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM4_files/figure-html/scatter-1.png&#34; alt=&#34;A scatter plot and a fitted model&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: A scatter plot and a fitted model
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here, you can see that as the value of &lt;code&gt;x&lt;/code&gt; increases so does the scatter around the trend line in &lt;code&gt;y&lt;/code&gt;. It is sometimes referred to as a shotgun pattern or cone/triangle pattern. It’s common in time series data because your observations are not independent of each other, the value of one observation is dependent on what happens earlier in time. In other words, your response variable can be modelled based on the standard deviation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt; - What would you expect to see in a scatter plot for the assumption of homogeneity of variance to be met?&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;We want the variation across our grouped observations to be similar so the spread of observations along they axis (vertically) across the values of x (horizontally) should be similar.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can assess this for a linear model from the (standardised or non-standardised) residual plot vs fitted values. Here’s the residual plot using the above data. Can you see the unequal variance?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM4_files/figure-html/resid2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;https://jacintak.github.io/teaching/GLM/GLM4_files/figure-html/resid2-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One way of dealing with heteroscedasticity is to use weighted least squares regression where the parameters are fitted to a single observation based on its residual to correct for variation in the residuals (that scatter). But that is beyond the scope of this module.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
