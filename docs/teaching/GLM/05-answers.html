
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="answers" class="section level1">
<h1>Answers</h1>
<div id="chapter-2" class="section level2">
<h2>Chapter 2</h2>
<p><strong>Question 1</strong> - What types of variables are used in simple linear regression?</p>
<p>Both the response and predictor variable should be continuous. (Categorical predictor variables are also allowed as we will see in later lectures)</p>
<p><strong>Question 2</strong> - What is the method for parameterising linear models called?</p>
<p>Ordinary least squares regression. The aim is to minimise the sum of squared residuals.</p>
<p><strong>Question 3</strong> - What is random error in linear regression?</p>
<p>The variation in data that cannot be explained by the linear model. i.e. the difference between the predicted value of the model and the observations: the residuals. Denoted <span class="math inline">\(\varepsilon\)</span>.</p>
<p><strong>Question 4</strong> - What is the function in R to conduct a linear regression?</p>
<p>The function is <code>lm()</code>.</p>
<p><strong>Question 5</strong> - How do you find a predicted value of a response for a given predictor value using a parameterised linear model?</p>
<p>Substitute the value of the predictor in the parameterised linear model and solve it.</p>
<pre><code>if Y = 10 * X + 6 and X = 2:
   Y = 10 * 2 + 6 = 20 + 6
   Y = 26 </code></pre>
<p><strong>Question 6</strong> - What is the probability distribution function of linear regression?</p>
<p>The normal distribution. Also called Gaussian.</p>
<hr />
</div>
<div id="chapter-3" class="section level2">
<h2>Chapter 3</h2>
<p><strong>Question 1</strong> - What is the significance of fixed and random variables for drawing conclusions?</p>
<p>It changes the inferred conclusion. In fixed models, the identities of the groupings are the main level of inference whereas in random models we consider the identity of the group to be randomly selected from a wider pool of candidates. Thus we can generalise our findings beyond our selected groups in random models but are limited to the groups we selected in fixed models.</p>
<p>For example, if we counted the population of 5 towns in Ireland and analysed the data using a fixed model, then we could only state our conclusion for those 5 towns and cannot draw conclusions for the whole of Ireland.</p>
<p>With a random model, we consider those 5 towns to be representative of the whole of Ireland, we are not interested in those 5 towns in particular, thus we can generalise our conclusions to the whole of Ireland. E.g. those 5 towns were randomly selected from all the candidate towns to survey.</p>
<p><strong>Question 2</strong> - What is the function to conduct an Analysis of Variance in R?</p>
<p>Either <code>summary(aov())</code> or <code>anova(lm())</code> will do a Type I ANOVA. Type II and Type III models require additional packages.</p>
<p><strong>Question 3</strong> - How many groups were in the categorical predictor variable in this one-way analysis of variance? <span class="math inline">\(F_{5,24} = 14.23, P &lt; 0.001\)</span></p>
<p>6 groups. The degree of freedom for group is 5 and this is calculated from the number of groups - 1. So 5 + 1 = 6.</p>
<p><strong>Question 4</strong> - What is the difference between an additive and interactive linear model?</p>
<p>An additive model does not describe an interaction between the two predictor variables. An interactive model describes that the effect of one predictor variable on the response variable is dependent on the second predictor variable.</p>
<p><strong>Question 5</strong> - What is the function to conduct an interactive multiple regression model in R?</p>
<p><code>lm(Y ~ A * B, data)</code> where <code>A</code> &amp; <code>B</code> are the two predictor variables. <code>*</code> denotes the interaction.</p>
<p><strong>Question 6</strong> - A multiple regression of crab shell width (response) with crab body depth (continuous predictor) for two species (categorical predictor: B or O) and their interaction had the following coefficients:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">2.6942643</td>
</tr>
<tr class="even">
<td align="left">BD</td>
<td align="right">2.5449206</td>
</tr>
<tr class="odd">
<td align="left">spO</td>
<td align="right">-1.2531624</td>
</tr>
<tr class="even">
<td align="left">BD:spO</td>
<td align="right">-0.1756934</td>
</tr>
</tbody>
</table>
<p>What is the predicted shell length (mm) of a species O crab with a body depth of 15mm?</p>
<p>Answer: 36.97951 mm</p>
<p>Because B is alphabetically before O, the first parameter coefficients refer to species B.
The intercept for species O is <code>2.6942643-1.2531624 = 1.441102</code><br />
The slope for species O is <code>2.5449206-0.1756934 = 2.369227</code><br />
Thus the whole model for species O is:<br />
<code>shell length = 2.369227 * body depth + 1.441102</code></p>
<p>Using a value of 15 for body depth:</p>
<pre><code>  shell length = 2.369227 * 15 + 1.441102
               = 36.97951 mm</code></pre>
<hr />
</div>
<div id="chapter-4" class="section level2">
<h2>Chapter 4</h2>
<p><strong>Question 1</strong> - What is the error that tells us how much variation the line is (H1) explaining relative to our null hypothesis (H0)?</p>
<p>SSR - Sum of squares of the regression.</p>
<p><strong>Question 2</strong> - What is the error that tells us the total variation in our data?</p>
<p>SSY - Sum of squares of the error. Sometimes called SST - total sum of squares</p>
<p><strong>Question 3</strong> - Why can’t we use SSE as the denominator?</p>
<p>SSE depends on the total variation of Y. We could have more variation simply by having more data but the ratio values are the same. So using sum of squares does not tell us about how much variation is explained by our model in a way that is unbiased.</p>
<p><strong>Question 4</strong> - What would you expect to see if our observations followed a normal distribution?</p>
<p>Observations to fall along a straight line at approx a 45 degree angle</p>
<p><strong>Question 5</strong> - What is the main difference between the Q-Q plots of the continuous and discrete data?</p>
<p>You can see the discrete nature of the observations in the clustered groupings in the QQ plot - the staircase pattern</p>
<p><strong>Question 6</strong> - What would you expect to see in a bar plot showing means and standard deviation for the assumption of homogeneity of variance to be met?</p>
<p>We want the variation across our grouped observations to be similar so the error bars of standard deviation should also be similar.</p>
<p><strong>Question 7</strong> - What would you expect to see in a scatter plot for the assumption of homogeneity of variance to be met?</p>
<p>We want the variation across our grouped observations to be similar so the spread of observations along they axis (vertically) across the values of x (horizontally) should be similar.</p>
<hr />
</div>
<div id="chapter-5" class="section level2">
<h2>Chapter 5</h2>
<p><strong>Question 1</strong> - Why did I add 1 to the variable Oystercatcher in the poisson regression?</p>
<p>1 is a dummy variable to remove 0s from the counts - or <code>log(0)</code> will mess up the estimation of parameters</p>
<p><strong>Question 2</strong> - What is the link function for a poisson regression?</p>
<p>(natural) log link</p>
<p><strong>Question 4</strong> - What is the expected number of Oystercatchers at site k?</p>
<p>474 oystercatchers</p>
<p>Remember in linear regression (and by extension GLMs) the Intercept estimate is the estimated coefficient for the first site (site a) and the rest are the difference between site a and the respective site. So for site k you need to add the estimated coefficients together.</p>
<p>Coefficient for k: <code>2.5649 + 3.5984 = 6.1633</code><br />
But remember this is log counts so you need to transform it back to regular counts:<br />
<code>exp(6.1633) = 474.993</code>
Then remember we added a dummy variable so you need to subtract that from our estimate:<br />
<code>474.993 - 1 = 473.993</code> which rounded to the nearest whole number (as counts are discrete variables) is 474!</p>
<p><strong>Question 4</strong> - What is the function to conduct an Poisson regression in R?</p>
<p><code>glm(Y ~ X, data, family = poisson)</code></p>
<p><strong>Question 5</strong> - Is there a relationship between Melanoma tumour thickness (mm) and whether a patient survives?</p>
<p>Yes. The thicker the tumour, the greater the odds of death. The P value of the slope of the binomial regression is significantly different from 0. P &lt; 0.001.</p>
<p><strong>Question 6</strong> - What does the coefficient estimate of <code>-0.24853</code> for the variable <code>thickness</code> represent?</p>
<p>The estimate represents the log odds of survival as a function of tumour thickness. In other words, we expect the log odds of survival to decrease by 0.25 for a 1 mm increase in tumour thickness.</p>
<p>We can transform log odds to odds by taking the exponential:<br />
<code>exp(-0.24853) = 0.7799465</code>. <code>1 - 0.7799465 = 0.22</code><br />
So for every 1 mm increase in tumour thickness we expect the odds of survival to decrease by 22 %</p>
<p><strong>Question 7</strong> - What is the probability of survival with a tumour 5 mm thick?</p>
<p>The logit formula is <code>logit = p/(1-p)</code> and the regression equation is <code>logit(p) = 1.61134 - 0.24853 * thickness</code>
so <code>logit(p) =  1.61134 - 0.24853 * 5 = 0.36869</code></p>
<p>and to turn logit p into probability (p):</p>
<pre><code>p = exp(logit(p))/(1 + exp(logit(p)))
  = exp(0.36869)/(1 + exp(0.36869))
  = 0.5911424</code></pre>
<p>The probability of surviving is 59 %</p>
</div>
</div>
